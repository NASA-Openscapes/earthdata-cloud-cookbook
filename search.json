[
  {
    "href": "discovery/Navigate_CMR_STAC.html#introduction-to-the-cmr-stac-api",
    "title": "",
    "section": "2.1 Introduction to the CMR-STAC API ",
    "text": "What is STAC?\n\nSTAC is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data. ### Four STAC Specifications: 1. STAC API\n2. STAC Catalog\n3. STAC Collection\n4. STAC Item\n#### In the section below, we will walk through an example of each specification. For additional information, check out: https://stacspec.org/.\n\n\n\n1. STAC API: Endpoint that enables the querying of STAC items.\n\nBelow, set the CMR-STAC API Endpoint to a variable, and use the requests package to send a GET request to the endpoint, and set the response to a variable.\n\nstac = 'https://cmr.earthdata.nasa.gov/stac/' # CMR-STAC API Endpoint\nstac_response = r.get(stac).json()            # Call the STAC API endpoint\nfor s in stac_response: print(s)\n\nid\ntitle\nstac_version\ntype\ndescription\nlinks\n\n\n\nprint(f\"You are now using the {stac_response['id']} API (STAC Version: {stac_response['stac_version']}). {stac_response['description']}\")\nprint(f\"There are {len(stac_response['links'])} STAC catalogs available in CMR.\")\n\nYou are now using the stac API (STAC Version: 1.0.0). This is the landing page for CMR-STAC. Each provider link contains a STAC endpoint.\nThere are 46 STAC catalogs available in CMR.\n\n\n\n\nYou will notice above that the CMR-STAC API contains many different endpoints–not just from NASA LP DAAC, but also contains endpoints for other NASA ESDIS DAACs.\n\n\n\n2. STAC Catalog: Contains a JSON file of links that organize all of the collections available.\n\nBelow, search for LP DAAC Catalogs, and print the information contained in the Catalog that we will be using today, LPCLOUD.\n\nstac_lp = [s for s in stac_response['links'] if 'LP' in s['title']]  # Search for only LP-specific catalogs\n\n# LPCLOUD is the STAC catalog we will be using and exploring today\nlp_cloud = r.get([s for s in stac_lp if s['title'] == 'LPCLOUD'][0]['href']).json()\nfor l in lp_cloud: print(f\"{l}: {lp_cloud[l]}\")\n\nid: LPCLOUD\ntitle: LPCLOUD\ndescription: Root catalog for LPCLOUD\ntype: Catalog\nstac_version: 1.0.0\nlinks: [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Provider catalog', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac/', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'collections', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections', 'title': 'Provider Collections', 'type': 'application/json'}, {'rel': 'search', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search', 'title': 'Provider Item Search', 'type': 'application/geo+json', 'method': 'GET'}, {'rel': 'search', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search', 'title': 'Provider Item Search', 'type': 'application/geo+json', 'method': 'POST'}, {'rel': 'conformance', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/conformance', 'title': 'Conformance Classes', 'type': 'application/geo+json'}, {'rel': 'service-desc', 'href': 'https://api.stacspec.org/v1.0.0-beta.1/openapi.yaml', 'title': 'OpenAPI Doc', 'type': 'application/vnd.oai.openapi+json;version=3.0'}, {'rel': 'service-doc', 'href': 'https://api.stacspec.org/v1.0.0-beta.1/index.html', 'title': 'HTML documentation', 'type': 'text/html'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM.v003', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5', 'type': 'application/json'}]\nconformsTo: ['https://api.stacspec.org/v1.0.0-beta.1/core', 'https://api.stacspec.org/v1.0.0-beta.1/item-search', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#fields', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#query', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#sort', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#context', 'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/core', 'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/oas30', 'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/geojson']\n\n\n\n\nBelow, print the links contained in the LP CLOUD STAC Catalog:\n\nlp_links = lp_cloud['links']\nfor l in lp_links: \n    try: \n        print(f\"{l['href']} is the {l['title']}\")\n    except:\n        print(f\"{l['href']}\")       \n\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD is the Provider catalog\nhttps://cmr.earthdata.nasa.gov/stac/ is the Root catalog\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections is the Provider Collections\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/search is the Provider Item Search\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/search is the Provider Item Search\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/conformance is the Conformance Classes\nhttps://api.stacspec.org/v1.0.0-beta.1/openapi.yaml is the OpenAPI Doc\nhttps://api.stacspec.org/v1.0.0-beta.1/index.html is the HTML documentation\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM.v003\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5\n\n\n\n\n\n3. STAC Collection: Extension of STAC Catalog containing additional information that describe the STAC Items in that Collection.\n\nBelow, get a response from the LPCLOUD Collection and print the information included in the response.\n\nlp_collections = [l['href'] for l in lp_links if l['rel'] == 'collections'][0]  # Set collections endpoint to variable\ncollections_response = r.get(f\"{lp_collections}\").json()                        # Call collections endpoint\nprint(f\"This collection contains {collections_response['description']} ({len(collections_response['collections'])} available)\")\n\nThis collection contains All collections provided by LPCLOUD (3 available)\n\n\n\n\nAs of March 3, 2021, there are three collections available, and more will be added in the future.\n\n\nPrint out one of the collections:\n\ncollections = collections_response['collections']\ncollections[1]\n\n{‘id’: ‘HLSL30.v1.5,’ ‘stac_version’: ‘1.0.0,’ ‘license’: ‘not-provided,’ ‘title’: ‘HLS Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30 m V1.5,’ ‘type’: ‘Collection,’ ‘description’: ‘PROVISIONAL - The Harmonized Landsat and Sentinel-2 (HLS) project provides consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe’s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2–3 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment. HLSL30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat 8 OLI data products. The HLSS30 (https://doi.org/10.5067/HLS/HLSS30.015) and HLSL30 products are gridded to the same resolution and Military Grid Reference System (MGRS) (https://hls.gsfc.nasa.gov/products-description/tiling-system/) tiling system, and thus are “stackable” for time series analysis.HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate file. There are 10 bands included in the HLSL30 product along with one quality assessment (QA) band and four angle bands. For a more detailed description of the individual bands provided in the HLSL30 product, please see the User Guide (https://lpdaac.usgs.gov/documents/878/HLS_User_Guide_V15_provisional.pdf).’ ‘links’: [{‘rel’: ‘self,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5,’ ‘title’: ‘Info about this collection,’ ‘type’: ‘application/json’}, {‘rel’: ‘root,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac,’ ‘title’: ‘Root catalog,’ ‘type’: ‘application/json’}, {‘rel’: ‘parent,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD,’ ‘title’: ‘Parent catalog,’ ‘type’: ‘application/json’}, {‘rel’: ‘items,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5/items,’ ‘title’: ‘Granules in this collection,’ ‘type’: ‘application/json’}, {‘rel’: ‘about,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/search/concepts/C1711972753-LPCLOUD.html,’ ‘title’: ‘HTML metadata for collection,’ ‘type’: ‘text/html’}, {‘rel’: ‘via,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/search/concepts/C1711972753-LPCLOUD.json,’ ‘title’: ‘CMR JSON metadata for collection,’ ‘type’: ‘application/json’}], ‘extent’: {‘crs’: ‘http://www.opengis.net/def/crs/OGC/1.3/CRS84,’ ‘spatial’: {‘bbox’: [[-180, -90, 180, 90]]}, ‘trs’: ‘http://www.opengis.net/def/uom/ISO-8601/0/Gregorian,’ ‘temporal’: {‘interval’: [[‘2013-04-11T00:00:00.000Z,’ None]]}}}\n\n\n\n\nIn CMR, id is used to query by a specific product, so be sure to save the ID for the HLS S30 and L30 V1.5 products below:\n\n# Search available collections for HLS and print them out\nhls_collections = [c for c in collections if 'HLS' in c['title']]\nfor h in hls_collections: print(f\"{h['title']} has an ID (shortname) of: {h['id']}\")\n\nHLS Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30 m V1.5 has an ID (shortname) of: HLSL30.v1.5\nHLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30 m V1.5 has an ID (shortname) of: HLSS30.v1.5\n\n\n\nNote that the “id” shortname is in the format: productshortname.vVVV (where VVV = product version)\n\n\n\nExplore the attributes contained in the HLSS30 Collection.\n\ns30 = [h for h in hls_collections if h['id'] == 'HLSS30.v1.5'][0]  # Grab HLSS30 collection\nfor s in s30['extent']: print(f\"{s}: {s30['extent'][s]}\")          # Check out the extent of this collection\n\ncrs: http://www.opengis.net/def/crs/OGC/1.3/CRS84\nspatial: {'bbox': [[-180, -90, 180, 90]]}\ntrs: http://www.opengis.net/def/uom/ISO-8601/0/Gregorian\ntemporal: {'interval': [['2014-04-03T00:00:00.000Z', None]]}\n\n\n\n\nSo here we can see that the extent is global, and can also see the temporal range–where “None” means on-going or to present.\n\nprint(f\"HLS S30 Start Date is: {s30['extent']['temporal']['interval'][0][0]}\")\ns30_id = s30['id']\n\nHLS S30 Start Date is: 2014-04-03T00:00:00.000Z\n\n\n\n\nNext, explore the attributes of the HLSL30 collection.\n\nl30 = [h for h in hls_collections if h['id'] == 'HLSL30.v1.5'][0]     # Grab HLSL30 collection\nfor l in l30['extent']: print(f\"{l}: {l30['extent'][l]}\")             # Check out the extent of this collection\nprint(f\"HLS L30 Start Date is: {l30['extent']['temporal']['interval'][0][0]}\")\nl30_id = l30['id']\n\ncrs: http://www.opengis.net/def/crs/OGC/1.3/CRS84\nspatial: {'bbox': [[-180, -90, 180, 90]]}\ntrs: http://www.opengis.net/def/uom/ISO-8601/0/Gregorian\ntemporal: {'interval': [['2013-04-11T00:00:00.000Z', None]]}\nHLS L30 Start Date is: 2013-04-11T00:00:00.000Z\n\n\n\n\nAbove, notice that the L30 product has a different start date than the S30 product.\n\n\n\n4. STAC Item: Represents data and metadata assets that are spatiotemporally coincident\n\nBelow, query the HLSS30 collection for items and return the first item in the collection.\n\n# Below, go through all links in the collection and return the link containing the items endpoint\ns30_items = [s['href'] for s in s30['links'] if s['rel'] == 'items'][0]  # Set items endpoint to variable\ns30_items\n\n‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5/items’\n\n\n\ns30_items_response = r.get(f\"{s30_items}\").json()                        # Call items endpoint\ns30_item = s30_items_response['features'][0]                             # select first item (10 items returned by default)\ns30_item\n\n{‘type’: ‘Feature,’ ‘id’: ‘G1969487860-LPCLOUD,’ ‘stac_version’: ‘1.0.0,’ ‘stac_extensions’: [‘https://stac-extensions.github.io/eo/v1.0.0/schema.json’], ‘collection’: ‘HLSS30.v1.5,’ ‘geometry’: {‘type’: ‘Polygon,’ ‘coordinates’: [[[-119.1488671, 33.3327671], [-118.9832795, 33.3355226], [-118.6783731, 34.3301598], [-119.1737801, 34.3223655], [-119.1488671, 33.3327671]]]}, ‘bbox’: [-119.17378, 33.332767, -118.678373, 34.33016], ‘links’: [{‘rel’: ‘self,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5/items/G1969487860-LPCLOUD’}, {‘rel’: ‘parent,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5’}, {‘rel’: ‘collection,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5’}, {‘rel’: ‘root,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/’}, {‘rel’: ‘provider,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/stac/LPCLOUD’}, {‘rel’: ‘via,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/search/concepts/G1969487860-LPCLOUD.json’}, {‘rel’: ‘via,’ ‘href’: ‘https://cmr.earthdata.nasa.gov/search/concepts/G1969487860-LPCLOUD.umm_json’}], ‘properties’: {‘datetime’: ‘2015-08-26T18:54:35.450Z,’ ‘start_datetime’: ‘2015-08-26T18:54:35.450Z,’ ‘end_datetime’: ‘2015-08-26T18:54:35.450Z,’ ‘eo:cloud_cover’: 6}, ‘assets’: {‘VZA’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.VZA.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.VZA.tif’}, ‘VAA’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.VAA.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.VAA.tif’}, ‘SAA’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.SAA.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.SAA.tif’}, ‘B11’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B11.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B11.tif’}, ‘B02’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B02.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B02.tif’}, ‘B09’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B09.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B09.tif’}, ‘B12’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B12.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B12.tif’}, ‘B03’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B03.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B03.tif’}, ‘B01’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B01.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B01.tif’}, ‘B07’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B07.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B07.tif’}, ‘SZA’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.SZA.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.SZA.tif’}, ‘B05’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B05.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B05.tif’}, ‘B06’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B06.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B06.tif’}, ‘Fmask’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.Fmask.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.Fmask.tif’}, ‘B10’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B10.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B10.tif’}, ‘B08’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B08.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B08.tif’}, ‘B8A’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B8A.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B8A.tif’}, ‘B04’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.B04.tif,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B04.tif’}, ‘browse’: {‘title’: ‘Download HLS.S30.T11SLT.2015238T185436.v1.5.jpg,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-public/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.jpg,’ ‘type’: ‘image/jpeg’}, ‘metadata’: {‘href’: ‘https://cmr.earthdata.nasa.gov/search/concepts/G1969487860-LPCLOUD.xml,’ ‘type’: ‘application/xml’}}}\n\n\n\n\nSTAC metadata provides valuable information on the item, including a unique ID, when it was acquired, the location of the observation, and a cloud cover assessment.\n\n# Print metadata attributes from this observation\nprint(f\"The ID for this item is: {s30_item['id']}\")\nprint(f\"It was acquired on: {s30_item['properties']['datetime']}\")\nprint(f\"over: {s30_item['bbox']} (Lower Left, Upper Right corner coordinates)\")\nprint(f\"It contains {len(s30_item['assets'])} assets\")\nprint(f\"and is {s30_item['properties']['eo:cloud_cover']}% cloudy.\")\n\nThe ID for this item is: G1969487860-LPCLOUD\nIt was acquired on: 2015-08-26T18:54:35.450Z\nover: [-119.17378, 33.332767, -118.678373, 34.33016] (Lower Left, Upper Right corner coordinates)\nIt contains 20 assets\nand is 6% cloudy.\n\n\n\n\nBelow, print out the ten items and the percent cloud cover–we will use this to decide which item to visualize in the next section.\n\nfor i, s in enumerate(s30_items_response['features']):\n    print(f\"Item at index {i} is {s['properties']['eo:cloud_cover']}% cloudy.\")\n\nItem at index 0 is 6% cloudy.\nItem at index 1 is 4% cloudy.\nItem at index 2 is 0% cloudy.\nItem at index 3 is 64% cloudy.\nItem at index 4 is 0% cloudy.\nItem at index 5 is 39% cloudy.\nItem at index 6 is 74% cloudy.\nItem at index 7 is 100% cloudy.\nItem at index 8 is 30% cloudy.\nItem at index 9 is 67% cloudy.\n\n\n\n\nUsing the information printed above, set the item_index below to whichever observation is the least cloudy above.\n\nitem_index = 9  # Indexing starts at 0 in Python, so here select the eighth item in the list at index 7\n\n\ns30_item = s30_items_response['features'][item_index]  # Grab the next item in the list\n\nprint(f\"The ID for this item is: {s30_item['id']}\")\nprint(f\"It was acquired on: {s30_item['properties']['datetime']}\")\nprint(f\"over: {s30_item['bbox']} (Lower Left, Upper Right corner coordinates)\")\nprint(f\"It contains {len(s30_item['assets'])} assets\")\nprint(f\"and is {s30_item['properties']['eo:cloud_cover']}% cloudy.\")\n\nThe ID for this item is: G2010287698-LPCLOUD\nIt was acquired on: 2016-11-06T08:21:39.880Z\nover: [24.875464, -26.295042, 25.108568, -25.427554] (Lower Left, Upper Right corner coordinates)\nIt contains 20 assets\nand is 67% cloudy.\n\n\n\n\nBelow, print out the names of all of the assets included in this item.\n\nprint(\"The following assets are available for download:\")\nfor a in s30_item['assets']: print(a)\n\nThe following assets are available for download:\nSZA\nB01\nVAA\nSAA\nB10\nB8A\nB05\nB09\nFmask\nB02\nB12\nB11\nB03\nB06\nB04\nB08\nVZA\nB07\nbrowse\nmetadata\n\n\n\n\nNotice that each HLS item includes a browse image. Read the browse file into memory and visualize the HLS acquisition.\n\ns30_item['assets']['browse']\n\n{‘title’: ‘Download HLS.S30.T35JKM.2016311T080122.v1.5.jpg,’ ‘href’: ‘https://lpdaac.earthdata.nasa.gov/lp-prod-public/HLSS30.015/HLS.S30.T35JKM.2016311T080122.v1.5.jpg,’ ‘type’: ‘image/jpeg’}\n\n\n\n\nUse the skimage package to load the browse image into memory and matplotlib to quickly visualize it.\n\nimage = io.imread(s30_item['assets']['browse']['href'])  # Load jpg browse image into memory\n\n# Basic plot of the image\nplt.figure(figsize=(10,10))              \nplt.imshow(image)\nplt.show()\n\n\n\n\n\n\nCongrats! You have visualized your first Cloud-Native HLS asset!"
  },
  {
    "href": "discovery/cmr-virtual-directories.html",
    "title": "CMR Virtual Directories",
    "section": "",
    "text": "Details from here:\nhttps://cmr.earthdata.nasa.gov/search/site/collections/directory/eosdis"
  },
  {
    "href": "discovery/index.html",
    "title": "NASA Cloud Data Discovery",
    "section": "",
    "text": "Some background here about discovery.\nSome great text about CMR and CMR-STAC, among other things"
  },
  {
    "href": "discovery/earthdata-search-ui.html",
    "title": "Earthdata Search UI",
    "section": "",
    "text": "Maybe a video tutorial?"
  },
  {
    "href": "index.html#the-new-cloud-paradigm",
    "title": "Welcome",
    "section": "The new cloud paradigm",
    "text": "NASA Distributed Active Archive Centers (DAACs) are in the process of moving their data holdings to the cloud (orange in diagram). In the new paradigm, the data storage (green in diagram) and DAAC-provided tools and services built on top of the data are co-located in the Earthdata Cloud (hosted in AWS cloud).\n\n\n\nIllustration by Catalina Oaida, PO.DAAC\n\n\nAs this data migration occurs, DAACs will have more information about how users can access data. For example, the Cloud Data page at PO.DAAC offers access to resources to help guide data users in discovering, accessing, and utilizing cloud data. During this transition, some data will continue to be available from the traditional on premise archive, while some data will also be available from and within the Earthdata Cloud."
  },
  {
    "href": "index.html#about",
    "title": "Welcome",
    "section": "About",
    "text": "This Earthdata Cloud Cookbook is being developed by the NASA-Openscapes team. Learn more at <https://nasa-openscapes.github.io> and on the About page."
  },
  {
    "href": "access/cof-via-harmony.html",
    "title": "COF via Earthdata Harmony",
    "section": "",
    "text": "In COF (zarr) via Earthdata Harmony API (services) (scripted, in cloud)"
  },
  {
    "href": "access/download-to-local.html",
    "title": "Download to local",
    "section": "",
    "text": "Download to local machine"
  },
  {
    "href": "access/index.html",
    "title": "NASA Cloud Data Access",
    "section": "",
    "text": "Some background here about access."
  },
  {
    "href": "access/data-access-example.html#jupyter-notebook-that-demonstrates-how-to-access-data",
    "title": "Data access demo",
    "section": "Jupyter Notebook that demonstrates how to access data",
    "text": "See outline for more details. This file can be replaced by a real .ipynb example."
  },
  {
    "href": "access/earthdata-search.html",
    "title": "Earthdata Search",
    "section": "",
    "text": "Earthdata Search (UI)"
  },
  {
    "href": "access/opendap-cloud.html",
    "title": "OPeNDAP in the Cloud",
    "section": "",
    "text": "OPeNDAP in the Cloud (OPeNDAP Hyrax)"
  },
  {
    "href": "access/podaac-ecco-data_access-cloud_direct_access_s3.html#getting-started",
    "title": "",
    "section": "Getting Started",
    "text": "In this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.large instance (2 cpus; 8GB memory).\n\n\nPython 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\ncartopy\n\n\nimport s3fs\nimport requests\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeat\nfrom json import dumps\nfrom io import StringIO\nfrom os.path import dirname, join\nfrom IPython.display import HTML\n\nplt.rcParams.update({'font.size': 14})\n\nMake a folder to write some outputs, if needed:\n\n!mkdir -p outputs/\n\n\n\n\nInputs\nConfigure one input: the ShortName of the desired dataset from ECCO V4r4. In this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data.\n\nShortName = \"ECCO_L4_SSH_05DEG_MONTHLY_V4R4\"\n\n\n\nEarthdata Login\nYou should have a .netrc file set up like:\nmachine urs.earthdata.nasa.gov login <username> password <password>\n\n\nDirect access from S3\nSet up an s3fs session for authneticated access to ECCO netCDF files in s3:\n\ndef begin_s3_direct_access(url: str=\"https://archive.podaac.earthdata.nasa.gov/s3credentials\"):\n    response = requests.get(url).json()\n    return s3fs.S3FileSystem(key=response['accessKeyId'],\n                             secret=response['secretAccessKey'],\n                             token=response['sessionToken'],\n                             client_kwargs={'region_name':'us-west-2'})\n\nfs = begin_s3_direct_access()\n\ntype(fs)\n\ns3fs.core.S3FileSystem"
  },
  {
    "href": "access/podaac-ecco-data_access-cloud_direct_access_s3.html#datasets",
    "title": "",
    "section": "Datasets",
    "text": "sea surface height (0.5-degree gridded, monthly)\nECCO_L4_SSH_05DEG_MONTHLY_V4R4\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid.\n\nssh_Files = fs.glob(join(\"podaac-ops-cumulus-protected/\", ShortName, \"*2015*.nc\"))\n\nlen(ssh_Files)\n\n12\n\n\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nssh_Dataset = xr.open_mfdataset(\n    paths=[fs.open(f) for f in ssh_Files],\n    combine='by_coords',\n    mask_and_scale=True,\n    decode_cf=True,\n    chunks={'latitude': 60,   # These were chosen arbitrarily. You must specify \n            'longitude': 120, # chunking that is suitable to the data and target\n            'time': 100}      # analysis.\n)\n\nssh = ssh_Dataset.SSH\n\nprint(ssh)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\n\n\nPlot the gridded sea surface height time series\nBut only the timesteps beginning in 2015:\n\nssh_after_201x = ssh[ssh['time.year']>=2015,:,:]\n\nprint(ssh_after_201x)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\nPlot the grid for the first time step using a Robinson projection. Define a helper function for consistency throughout the notebook:\n\ndef make_figure(proj):\n    fig = plt.figure(figsize=(16,6))\n    ax = fig.add_subplot(1, 1, 1, projection=proj)\n    ax.add_feature(cfeat.LAND)\n    ax.add_feature(cfeat.OCEAN)\n    ax.add_feature(cfeat.COASTLINE)\n    ax.add_feature(cfeat.BORDERS, linestyle='dotted')\n    return fig, ax\n\nfig, ax = make_figure(proj=ccrs.Robinson())\n\nssh_after_201x.isel(time=0).plot(ax=ax, transform=ccrs.PlateCarree(), cmap='Spectral_r')\n\n<matplotlib.collections.QuadMesh at 0x7fae2533d730>\n\n\n\n\n\nNow plot the whole time series (post-2010) in an animation and write it to an mp4 file called ecco_monthly_ssh_grid_2015_to_x.mp4:\n\ndef get_animation(var, cmap: str=\"Spectral_r\"):\n    \"\"\"Get time series animation for input xarray dataset\"\"\"\n\n    def draw_map(i: int, add_colorbar: bool):\n        data = var[i]\n        m = data.plot(ax=ax, \n                      transform=ccrs.PlateCarree(),\n                      add_colorbar=add_colorbar,\n                      vmin=var.valid_min, \n                      vmax=var.valid_max,\n                      cmap=cmap)\n        plt.title(str(data.time.values)[:7])\n        return m\n\n    def init():\n        return draw_map(0, add_colorbar=True)\n    \n    def animate(i):\n        return draw_map(i, add_colorbar=False)\n\n    return init, animate\n\nNow make the animation using the function:\n\nfig, ax = make_figure(proj=ccrs.Robinson())\n\ninit, animate = get_animation(ssh_after_201x)\n\nani = animation.FuncAnimation(fig=fig, \n                              func=animate, \n                              frames=ssh_after_201x.time.size, \n                              init_func=init, \n                              interval=0.2, \n                              blit=False, \n                              repeat=False)\n\n# Now save the animation to an MP4 file:\nani.save('outputs/ecco_monthly_ssh_grid_2015_to_x.mp4', writer=animation.FFMpegWriter(fps=8))\n\nplt.close(fig)\n\nRender the animation in the ipynb:\n\n#HTML(ani.to_html5_video())\n\n\n\ntflux (0.5-degree gridded, daily)\nNow we will do something similar to access daily, gridded (0.5-degree) ocean and sea-ice surface heat fluxes (10.5067/ECG5D-HEA44). Read more about the dataset and the rest of the ECCO V4r4 product suite on the PO.DAAC Web Portal.\nUse a “glob” pattern when listing the S3 bucket contents such that only netCDFs from January 2015 are represented in the resulting list of paths.\n\ntflux_Files = fs.glob(join(\"podaac-ops-cumulus-protected/\", \"ECCO_L4_HEAT_FLUX_05DEG_DAILY_V4R4\", \"*2015-01*.nc\"))\n\nlen(tflux_Files)\n\n31\n\n\nNow open them all as one xarray dataset just like before. Open and pass the 365 netCDF files to the xarray.open_mfdataset constructor so that we can operate on them as a single aggregated dataset.\n\ntflux_Dataset = xr.open_mfdataset(\n    paths=[fs.open(f) for f in tflux_Files],\n    combine='by_coords',\n    mask_and_scale=True,\n    decode_cf=True,\n    chunks={'latitude': 60,   # These were chosen arbitrarily. You must specify \n            'longitude': 120, # chunking that is suitable to the data and target\n            'time': 100}      # analysis.\n)\n\ntflux = tflux_Dataset.TFLUX\n\nprint(tflux)\n\n<xarray.DataArray 'TFLUX' (time: 31, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(31, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-01T12:00:00 ... 2015-01-31T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    direction:              >0 increases potential temperature (THETA)\n    long_name:              Rate of change of ocean heat content per m2 accou...\n    units:                  W m-2\n    comment:                The rate of change of ocean heat content due to h...\n    valid_min:              [-1713.51220703]\n    valid_max:              [870.31304932]\n\n\nSelect a region over the Gulf of Mexico and spatially subset it from the larger dataset by slicing on the latitude and longitude axes.\n\ntflux_gom = tflux.sel(latitude=slice(15, 40), \n                      longitude=slice(-105, -70))\n\nprint(tflux_gom.shape)\n\n(31, 50, 70)\n\n\n\ntflux_gom.isel(time=0).plot()\n\n<matplotlib.collections.QuadMesh at 0x7fae1689a550>\n\n\n\n\n\nPlot the Jan 2015 surface heat flux as a gridded time series animation over the GOM study region.\n\nfig, ax = make_figure(proj=ccrs.Mercator())\n\nax.coastlines()\nax.set_extent([tflux_gom.longitude.min(), \n               tflux_gom.longitude.max(), \n               tflux_gom.latitude.min(), \n               tflux_gom.latitude.max()])\n\ninit, animate = get_animation(tflux_gom, cmap=\"RdBu\")\n\n# Plot a time series animation write it to an mp4 file:\nani = animation.FuncAnimation(fig=fig, \n                              func=animate, \n                              frames=tflux_gom.time.size, \n                              init_func=init, \n                              interval=0.2, \n                              blit=False, \n                              repeat=False)\n\nani.save('outputs/ecco_daily_tflux_gom_2015.mp4', writer=animation.FFMpegWriter(fps=8))\n\nplt.close(fig)"
  },
  {
    "href": "access/direct-in-region.html",
    "title": "Direct in-region",
    "section": "",
    "text": "(scripted, not cloud)\ntext here"
  },
  {
    "href": "contributing/index.html#intro",
    "title": "Contributing",
    "section": "Intro",
    "text": "This Section is about how our Openscapes Cohort collaborates to create this book, with an eye towards how others could collaborate with us in the future.\nTODO: Fork/borrow from the awesome work at The Turing Way: https://the-turing-way.netlify.app/community-handbook/community-handbook.html"
  },
  {
    "href": "contributing/index.html#quarto",
    "title": "Contributing",
    "section": "Quarto",
    "text": "We’re making the EarthData Cloud Cookbook with Quarto: quarto.org. Quarto makes collaborating to create technical documentation streamlined because we work in plain text documents that can have executable code (Python, R) and are rendered using Jupyter and Knitr engines.\nWhat is Quarto? Quarto builds from what RStudio learned from RMarkdown but enables different engines (Jupyter and knitr). It is both a Command Line Tool and R package. .qmd is a new filetype like .Rmd — meaning it’s a text file but coupled with an engine can execute code and be rendered as html, pdf, word, and beyond — but for other languages like Python. Quarto can convert .ipynb files to and from .md and .qmd easily so you can develop and publish with collaborators that have different workflows. Once the book is “served” locally, .md files auto-update as you edit, and files with executable code can be rendered individually, and the behavior of different code chunks can be controlled and cached.\n(Note: with Quarto, e-books and websites are very similarly structured, with e-books being set up for numbered chapters and references and websites set up for higher number of pages and organization. We can talk about our book as a book even as we explore whether book/website better suits our needs. This is assigned in _quarto.yml, as we’ll explore later)."
  },
  {
    "href": "contributing/setup.html#overview",
    "title": "Initial Setup",
    "section": "Overview",
    "text": "This is the setup required to contribute to our Cookbook. You can always refer to quarto.org for the most up-to-date and more detailed information.\nSetup for our Cookbook includes:\n\nInstalling the very latest version of Quarto\nCloning our Cookbook repo from GitHub\nBuilding our Cookbook\n\nInstructions here is to run Quarto from the command line; see quarto.org for equivalents in R.\n\n\n\n\n\n\nNote\n\n\n\nComing up we’ll also outline other ways to contribute and review the Cookbook through GitHub on the browser."
  },
  {
    "href": "contributing/setup.html#install-quarto",
    "title": "Initial Setup",
    "section": "Install Quarto",
    "text": "First, download the very latest version of the Quarto command line interface (CLI). To do this, go to:\nhttps://github.com/quarto-dev/quarto-cli/releases/latest\nClick on the file type to download for your operating system:\n\nLinux: amd64.deb\nMac: macos.pkg\nWindows: win.msi\n\nAfter downloading, follow the installation prompts on your computer like you do for other software.\nCheck to make sure Quarto installed properly:\nquarto check install \n\n\n\n\n\n\nAdditional checks\n\n\n\n\n\nYou can also run:\n\nquarto check knitr to locate R, verify we have the rmarkdown package, and do a basic render\nquarto check jupyter to locate Python, verify we have Jupyter, and do a basic render\nquarto check to run all of these checks together\n\n\n\n\nIf you need to install a version more recent than the latest release, see the documentation on installing the Development Version."
  },
  {
    "href": "contributing/setup.html#clone-from-github",
    "title": "Initial Setup",
    "section": "Clone from GitHub",
    "text": "Now clone our book and set it as your current directory.\ngit clone https://github.com/NASA-Openscapes/earthdata-cloud-cookbook \ncd earthdata-cloud-cookbook\nIf you need to set up GitHub see instructions [TODO]"
  },
  {
    "href": "contributing/setup.html#build-cookbook",
    "title": "Initial Setup",
    "section": "Build Cookbook!",
    "text": "You should now be able to serve and render the documents within the earthdata-cloud-cookbook directory.\nquarto serve\nThis will open the Cookbook as a new tab in your browser. Now you’re all set to contribute to the Cookbook! Read about how in the next chapter."
  },
  {
    "href": "contributing/workflow.html#workflow-for-contributing-to-our-cookbook",
    "title": "Workflow",
    "section": "Workflow for contributing to our Cookbook",
    "text": "Your workflow can be from wherever you are most comfortable. You can develop chapters working in a text editor, integrated development environment (IDE), or notebook interface. And you can serve Quarto from the Command Line or R. Quarto will combine files of different types ( .md , .ipynb, .Rmd, and .qmd) to make the Cookbook. This workflow can streamline collaboration for scientific & technical writing across programming languages.\nBy default, rendering the Cookbook will only act on markdown text and will not execute any code. This way, rendering the whole Cookbook will not become overly cumbersome as it grows, and there is not one single virtual environment with all the libraries required. Instead our workflow will be that as you develop a single chapter (or section), you control when you render, and can create a requirements.txt file for that chapter (or section). This will also make it much easier to port lessons that work standalone and are ready for a Cloud instance or a workshop."
  },
  {
    "href": "contributing/workflow.html#quickstart-reference",
    "title": "Workflow",
    "section": "Quickstart reference",
    "text": "Commands for returning to the Cookbook after lunch/months; each is described in more detail below.\n## check which branches exist, where you are, and pull recent from main branch\ngit branch\ngit checkout main\ngit pull\n\n## create and switch to new branch\ngit checkout -b branch-name \n\n## develop content\n## write prose in markdown, code in R and Python\n\n## commit changes\ngit add --all \ngit commit -m \"my commit message here\" \n\n## push changes\ngit push -u origin branch-name  # connect your branch to github.com and push\n\n## create a pull request\n## from GitHub.com, create a pull request and once it is merged, delete your branch\n\n## delete branch\ngit checkout main         # switch to the main branch\ngit pull                  # pull merged updates from github.com\ngit branch -d branch-name # delete old local  branch"
  },
  {
    "href": "contributing/workflow.html#github-workflow",
    "title": "Workflow",
    "section": "GitHub Workflow",
    "text": "First let’s talk about the GitHub part of the workflow.\nWe will work in branches so as to not overwrite each other’s work, and let GitHub do what it does best.\nThe main branch will be the current approved version of the book. The main branch is what displays at https://nasa-openscapes.github.io/earthdata-cloud-cookbook.\nA nice clean workflow with branches is to consider them temporary. You pull the most recent from main, you create a branch locally, you make your edits, you commit regularly, then you push back to github.com, create a pull request for it to be merged into main, and once approved, you delete the branch on github.com and also locally. That’s the workflow we’ll walk through here. A great resource on GitHub setup and collaboration is Happy Git with R, which includes fantastic background philosophy as well as bash commands for setup, workflows, and collaboration.\nThe following assumes you’re all setup from the previous chapter.\n\nBranch setup\nFirst off, check what branch you’re on and pull the most recent edits from the main branch. If you need to switch branches, use git checkout.\ngit branch          # returns all local branches\ngit checkout main   # switch branch to main\ngit pull            # pull most recent from the main branch\nIf you are already on the main branch, git will tell you, and that’s fine.\n(If you have any residual branches from before, you’ll likely want to start off by deleting them — assuming they were temporary and have been merged into github.com. You can delete a branch with git branch -d branch-name).\nNext, create a new branch, then switch to that branch to work in. Below is a one-step approach for the two-step process of git branch branch-name then git checkout branch-name (read more).\ngit checkout -b branch-name  # create and switch to new branch\n\n\nDevelop content\nTime to edit and develop content, and run your Quarto Workflow – see specific instructions below. While you’re developing, you’ll want to frequently commit your changes.\n\n\nCommit changes\nYou’ll commit your work regularly as you go, likely using the following, which commits all files you’ve affected within the Cookbook project:\ngit add --all \ngit commit -m \"my commit message here\" \nFrom R Packages:\n\nA commit takes a snapshot of your code at a specified point in time. Using a Git commit is like using anchors and other protection when climbing. If you’re crossing a dangerous rock face you want to make sure you’ve used protection to catch you if you fall. Commits play a similar role: if you make a mistake, you can’t fall past the previous commit.\n\nHere are some of Hadley Wickham’s suggested best practices\n\n\nPush changes\nWhen you’re ready to push changes you’ve made in your branch, you’ll first need to connect it to github.com by pushing it “upstream” to the “origin repository” (-u below is short for --set-upstream):\ngit push -u origin branch-name  # connect your branch to github.com and push\n\n\nPull Request\nNow you’ve synced your work to github.com. It is currently online, in a separate branch from the main branch. Go to https://github.com/nasa-openscapes/earthdata-cloud-cookbook, find your branch, and do a pull request.\nTODO: Let’s discuss our review process:\n\nTag someone to review, (including you if it’s a quick fix?)\nTimeline\nMerging\n\nWhen the pull request is merged, delete the branch on github.com. GitHub will prompt you with a button at the end of the merge.\n\n\nDelete Branch\nOnce your pull request is merged and you’ve deleted the branch from github.com, then come back to your local setup and delete the branch locally:\ngit checkout main         # switch to the main branch\ngit pull                  # pull merged updates from github.com\ngit branch -d branch-name # delete old local  branch"
  },
  {
    "href": "contributing/workflow.html#quarto-workflow",
    "title": "Workflow",
    "section": "Quarto Workflow",
    "text": "Now the fun part! Our overall workflow will be to serve the book at the beginning, develop/edit chapters as simple text files (.md/.qmd/.Rmd) or executable notebooks (.ipynb) that will all render into the book.\nQuarto lets us easily convert between file types, so depending on how you prefer to work and how you’d like to interact with different audiences, we can go between formats as we wish. For example, we can converting an existing .ipynb to .qmd to collaborate during development, and then convert back to .ipynb files for our workshops. See quarto convert help for details.\nAs you work, you’ll follow our GitHub workflow above, committing regularly. And you can optionally use quarto render to rebuild the whole Cookbook before pushing to github.com.\nThe following is to run Quarto from the command line; see quarto.org to see equivalents in R.\n\nQuarto serve\nThe thing to do first is to “serve” the Cookbook so that we can see what it looks like as we develop the chapters (it’s called “serve” because it’s really a website that looks like a book).\nRun the following from your branch in your earthdata-cloud-cookbook directory from the command line:\nquarto serve\nAnd after it’s is served, you can click from the console (or paste the url into your browser) to see the development version of the Cookbook.\n\n\n\n\n\n\nThis command line instance is now being used to serve Quarto\n\n\n\nYou can open another instance to continue working from the command line, including running other shell commands and rendering (see next). Launching your command line shell of choice will open a new instance.\n\n\n\n\nDevelop Cookbook Content\nYou can develop Cookbook chapters as text files (.md/.qmd/.Rmd) in the text editor of your choice. You can also develop chapters as .ipynb from JupyterLab, Anaconda, etc (see more about JupyterLab with Quarto).\n\nRStudio IDE & Visual Editor\nYou can also use the RStudio IDE. It can be used as a simple text editor, but it can also interactively execute code in .qmd and .Rmd files — which are plain text files. This will streamline testing and collaboration as we develop content.\nThe RStudio IDE Visual Editor makes this experience feel like a cross between an interactive notebook and a Google Doc:\n\n\n\nThe RStudio IDE Visual Editor with an interactive .qmd file\n\n\nAbove shows the Visual Editor in the top left pane with an interactive .qmd file. Learn more about the RStudio Visual Editor.\nAnother benefit of the RStudio IDE is that it has a docked command line (Terminal, bottom left pane), file navigation (bottom right pane) and GitHub interface (top right pane). The IDE helps keep things organized as you work, and provides a visual way to review your git commits and do common commands (see this RStudio-GitHub walk through from R for Excel Users). Note too that the image shows the second instance of the Terminal command line; the first is being used to serve Quarto.\n\n\n\nQuarto render\nAs you develop book chapters and edit files, any .md files will automatically refresh in the browser (so long as quarto serve is running)!\nTo refresh files with executable code, you’ll need to render them individually. You can do the following to render .ipynb/.qmd/.Rmd files so that they show up refreshed in the served Cookbook.\nquarto render my-document.ipynb      ## render a notebook\nquarto render my-work.qmd            ## render a Quarto file\nquarto render my-contribution.Rmd    ## render a RMarkdown file\nFrom the RStudio IDE, you can also press the Render button to render .qmd and .Rmd files.\nAnd you can also render the whole book:\nquarto render\nLearn more about rendering with Quarto. From J.J. Allaire:\n\nThe reason Quarto doesn’t render .Rmd and .qmd on save is that render could (potentially) be very long running and that cost shouldn’t be imposed on you whenever you save. Here we are talking about the age old debate of whether computational markdown should be rendered on save when running a development server. Quarto currently doesn’t do this to give the user a choice between an expensive render and a cheap save."
  },
  {
    "href": "contributing/workflow.html#virtual-environments",
    "title": "Workflow",
    "section": "Virtual Environments",
    "text": "If you are working on a chapter that loads any Python or R packages, to make your work reproducible you’ll need to create and then update the environments.txt file. Do this use the pip freeze command:\npip freeze > requirements.txt\nThis will overwrite/update the requirements.txt file. Depending on where you are working, you might also want to create a new subfolder to store the requirements.txt. See the next section on Cookbook Structure.\nYou you will then commit and push along with your other edits back to github.com.\nTODO: info about conda…"
  },
  {
    "href": "contributing/workflow.html#cookbook-structure",
    "title": "Workflow",
    "section": "Cookbook Structure",
    "text": "Each chapter in our Cookbook is a separate file (.md/ .ipynb/.qmd/.Rmd). These are stored in our files directory, organized by sub-directory.\nThe Cookbook structure (i.e. the order of sections and chapters) is determined in the _quarto.yml file in the root directory. We can shuffle chapter order by editing the _quarto.yml file, and and add new chapters by adding to the _quarto.yml and creating a new file in the appropriate sub-directory that is indicated in _quarto.yml.\n\n\n\nComparing `_quarto.yml` file to served project in the browser\n\n\nPlease experiment, add new chapters and sections; we can shuffle chapter order and subsections as we continue to develop the Cookbook, nothing is set in stone."
  },
  {
    "href": "contributing/workflow.html#cookbook-practices",
    "title": "Workflow",
    "section": "Cookbook Practices",
    "text": "These are shared practices that we have for co-developing the Cookbook. This will be developed further as we go!\n\nExecuting notebooks\nAs you develop files with executable code ( .qmd, .Rmd, and .ipynb), you can decide if you don’t want the notebook to execute. By adding YAML as a raw text cell at the top of an .ipynb file, you can control whether it is executed or not. Adding execute: false to the YAML at the top of the file basically means that Quarto never runs the code, but the user of course still can interactively in Jupyter.\nUsing .qmd there are also ways to control execution cell-by-cell via # | syntax within a code chunk; see https://quarto.org/docs/computations/execution-options.html"
  },
  {
    "href": "contributing/workflow.html#troubleshooting",
    "title": "Workflow",
    "section": "Troubleshooting",
    "text": "Error: AddrInUse\nERROR: AddrInUse: Address already in use (os error 48)\nThis error is because you had more than one instance of quarto serve going in your session. So close other command line instances that are running and try again. (If you use the R package and do quarto_serve() it will automatically make sure you only ever have 1 instance.)\n\n\nLeave/exit a virtual environment\nIn your Command Line Shell, if you want to leave your virtual environment, the command is:\ndeactivate\nThe way you tell that you are in a virtual environment: it’s named in parentheses at the beginning of your prompt:\n(.venv) (base) JLos-Macbook-Pro:earthdata-cloud-cookbook lowndes$ deactivate\n(base) JLos-Macbook-Pro:earthdata-cloud-cookbook lowndes$"
  },
  {
    "href": "appendix/index.html",
    "title": "Appendix",
    "section": "",
    "text": "More cool stuff."
  },
  {
    "href": "appendix/about.html",
    "title": "About",
    "section": "",
    "text": "About this project and who we are"
  },
  {
    "href": "appendix/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "All the NASA (and more) acronyms, uncovered\nCOG: Cloud-optimized geo-tiff\nCTL: Command Line Tool"
  },
  {
    "href": "examples/r-example.html",
    "title": "RMarkdown example .Rmd",
    "section": "",
    "text": "This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code.\nTry executing this chunk by clicking the Run button within the chunk or by placing your cursor inside it and pressing Cmd+Shift+Enter.\n\nplot(cars)\n\n\n\n\nAdd a new chunk by clicking the Insert Chunk button on the toolbar or by pressing Cmd+Option+I.\nWhen you save the notebook, an HTML file containing the code and output will be saved alongside it (click the Preview button or press Cmd+Shift+K to preview the HTML file).\nThe preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike Knit, Preview does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed."
  },
  {
    "href": "examples/index.html",
    "title": "End-to-end Examples",
    "section": "",
    "text": "End-to-end workflow examples, linked to pieces from examples above and vice-versa"
  },
  {
    "href": "examples/notebook-example.html#this-is-otherwise-just-testing",
    "title": "Notebook example",
    "section": "This is otherwise just testing ",
    "text": "Import the required packages and set the input/working directory to run this Jupyter Notebook locally.\n\nimport requests as r\nfrom skimage import io\nimport matplotlib.pyplot as plt\n\n\nprint(1+1)\nprint(\"woo NASA data!\")\n3+3\n\n2\nwoo NASA data!\n\n\n6"
  },
  {
    "href": "examples/python-example.html",
    "title": "Python example with Quarto",
    "section": "",
    "text": "This is a .qmd file. I write prose here, using keyboard shortcuts or clicking above to make text bold and other formatting.\nAs I develop I’ll interactively run the following, but can set up caching so that it doesn’t execute the code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\ntext boxes like this one are in markdown"
  },
  {
    "href": "transformations/L2-subsetter.html",
    "title": "L2 Subsetter",
    "section": "",
    "text": "Intro to the following notebooks:\n\nBasic harmony subsetting notebook in podaac git\nL2 data cube - more of an end-user example using the service"
  },
  {
    "href": "transformations/index.html",
    "title": "NASA Cloud Data Transformations",
    "section": "",
    "text": "text"
  },
  {
    "href": "transformations/harmonypy.html",
    "title": "Harmony Py Library",
    "section": "",
    "text": "Amy - should we include discussion/resource on using the HarmonyPy library?\ntest test"
  },
  {
    "href": "transformations/netcdf-to-zarr.html",
    "title": "netCDF to Zarr",
    "section": "",
    "text": "Intro to the following notebooks:\n\nHarmony dev team basic notebooks (in UAT, needs updating; including dataset it’s working on)\nECCO example"
  },
  {
    "href": "transformations/earthdata-search.html",
    "title": "Earthdata Search UI",
    "section": "",
    "text": "text"
  },
  {
    "href": "transformations/harmony-api.html",
    "title": "Harmony API",
    "section": "",
    "text": "text"
  },
  {
    "href": "transformations/opendap-cloud.html",
    "title": "OPeNDAP in the Cloud",
    "section": "",
    "text": "OPeNDAP in the Cloud (OPeNDAP Hydrax)"
  },
  {
    "href": "transformations/HarmonyPy_intro_tutorial.html#harmonypy-introduction",
    "title": "",
    "section": "HarmonyPy Introduction",
    "text": "This notebook demonstrates several basic examples highlighting how to query and access customized data outputs from NASA Earthdata Harmony. See https://harmony-py.readthedocs.io/en/latest/ for detailed documentation on HarmonyPy.\n\nImport packages\nFirst we import packages needed to request and visualize our data, as well as the harmony-py library itself. Make sure to install harmony-py and its dependencies into your current Python environment prior to executing the notebook:\n$ pip install -U harmony-py \n\nYou can install this and any other necessary libraries directly into your Jupyter notebook Python kernel as well:\n!{sys.executable} -m pip install -U harmony-py\n\n# !{sys.executable} -m pip install -U harmony-py #install harmony-py into Python kernel\nimport sys; sys.path.append('..')\nimport datetime as dt\nfrom IPython.display import display, JSON\nimport rasterio\nimport rasterio.plot\n\nfrom harmony import BBox, Client, Collection, Request\nfrom harmony.config import Environment\n\n\n\nQuick start\nYou can request and download data using harmony-py in just a few easy lines. Although more advanced subsetting and transformation options may be supported on your data product of interest, this example below demonstrates a basic spatial bounding box and temporal range request, using the direct download method:\nharmony_client = Client(auth=('EDL_username', 'EDL_password'))\nrequest = Request(\n    collection=Collection(id=dataset_short_name),\n    spatial=BBox(w, s, e, n),\n    temporal={\n        'start': dt.datetime(yyyy, mm, dd),\n        'stop': dt.datetime(yyyy, mm, dd)\n    }\n)\njob_id = harmony_client.submit(request)\nresults = harmony_client.download_all(job_id, directory='/tmp', overwrite=True)\nThe guidance below offers more detailed examples highlighting many of the helpful features provided by the Harmony Py library, including direct s3 access from Earthdata Cloud-hosted data if running in the AWS us-west-2 region.\n\n\nCreate Harmony Client object\nFirst, you will need to create your Harmony Client, which is what you will interact with to submit and inspect a data request to Harmony, as well as to retrieve your results.\nWhen creating the Client, you need to provide your Earthdata Login credentials, which are required to access data from NASA EOSDIS. There are three options for providing your Earthdata Login username and password:\n\nProvide your username and password directly when creating the client:\n\nharmony_client = Client(auth=('captainmarvel', 'marve10u5'))\n\nSet your credentials using environment variables:\n\nYou can either export these directly:\n$ export EDL_USERNAME='captainmarvel'\n$ export EDL_PASSWORD='marve10u5'\nOr by storing them in a .env file, which operates in a similar fashion to .netrc. You will need to store the file in your current working directory and it must be named .env with the following format:\nEDL_USERNAME=myusername\nEDL_PASSWORD=mypass\n\nUse a .netrc file:\n\nCreate a .netrc file in your home directory, using the example below:\nmachine urs.earthdata.nasa.gov\nlogin captainmarvel\npassword marve10u5\n\nharmony_client = Client() # assumes .netrc usage, option 3 above\n\n\n\nCreate Harmony Request\nThe following are common request parameters:\n\ncollection: Required parameter. This is the NASA EOSDIS collection, or data product. There are two options for inputting a collection of interest:\n\nProvide a concept ID, which is an ID provided in the Common Metadata Repository (CMR) metadata\nData product short name (e.g. SENTINEL-1_INTERFEROGRAMS).\n\nspatial: Bounding box spatial constraints on the data. The Harmony Bbox class accepts spatial coordinates as decimal degrees in w, s, e, n order, where longitude = -180, 180 and latitude = -90, 90.\n\nshape: Perform a shapefile subsetting request on a supported collection by passing the path to a GeoJSON file (*.json or .geojson), an ESRI Shapefile (.zip or .shz), or a kml file (.kml) as the “shape” parameter\ntemporal: Date/time constraints on the data. The example below demonstrates temporal start and end ranges using the python datetime library.\n\nOther advanced parameters that may be of interest. Note that many reformatting or advanced projection options may not be available for your requested dataset. See the documentation for details on how to construct these parameters.\n\ncrs: Reproject the output coverage to the given CRS. Recognizes CRS types that can be inferred by gdal, including EPSG codes, Proj4 strings, and OGC URLs (http://www.opengis.net/def/crs/…)\ninterpolation: specify the interpolation method used during reprojection and scaling\nscale_extent: scale the resulting coverage either among one axis to a given extent\nscale_size: scale the resulting coverage either among one axis to a given size\ngranule_id: The CMR Granule ID for the granule (file) which should be retrieved\nwidth: number of columns to return in the output coverage\nheight: number of rows to return in the output coverage\nformat: the output mime type to return\nmax_results: limits the number of input files processed in the request\n\nThe example utilized in this tutorial demonstrates a shapefile subset of the Big Island of Hawaii on February 24, 2020. A bounding box subset over the Mauna Kea and Mauna Loa volcanoes is also commented out below to show a similar subsetting option. The SENTINEL-1_INTERFEROGRAMS dataset, distributed by the ASF DAAC, is a prototype Level 2 NISAR-Format product. See https://asf.alaska.edu/data-sets/derived-data-sets/sentinel-1-interferograms/ for more information.\nThis request specifies a subset of the unwrappedPhase variable, in TIFF format, with a maximum file result capped at 2 for demonstration purposes.\n\nNote that a Sentinel-3 End-User License Agreement (EULA) is required to access these data.\n\n\nPlease go to https://grfn.asf.alaska.edu/door/download/S1-GUNW-D-R-021-tops-20201029_20191029-033636-28753N_27426N-PP-2dde-v2_0_3.nc to initiate a file download, which will first prompt you to accept the required EULA if you have not already done so. If you do not accept this EULA, you will receive an error when submitting your Harmony request.\n\nshapefile_path = 'Big_Island_0005.zip' \n\nrequest = Request(\n    collection=Collection(id='SENTINEL-1_INTERFEROGRAMS'),\n    #spatial=BBox(-155.75, 19.26, -155.3, 19.94), # bounding box example that can be used as an alternative to shapefile input\n    shape=shapefile_path,\n    temporal={\n        'start': dt.datetime(2020, 2, 24),\n        'stop': dt.datetime(2020, 2, 25),\n    },\n    variables=['science/grids/data/unwrappedPhase'],\n    format='image/tiff',\n    max_results=2,\n)\n\n\n\n\nCheck Request validity\nBefore submitting a Harmony Request, you can test your request to see if it’s valid and how to fix it if not. In particular, request.is_valid will check to ensure that your spatial BBox bounds and temporal ranges are entered correctly.\n\nprint(f\"Request valid? {request.is_valid()}\")\nfor m in request.error_messages():\n    print(\" * \" + m)\n\n\n\nSubmit Request\nOnce you have created your request, you can now submit it to Harmony using the Harmony Client object. A job id is returned, which is a unique identifier that represents your submitted request.\n\njob_id = harmony_client.submit(request)\njob_id\n\n\n\nCheck Request status\nYou can check on the progress of a processing job with status(). This method blocks while communicating with the server but returns quickly. The JSON helper function provides a nicer formatting option for viewing the status message.\n\nJSON(harmony_client.status(job_id))\n\nDepending on the size of the request, you may want to wait until the request has completed processing before the remainder of your code is executed. The wait_for_processing() method will block subsequent lines of code while optinally showing a progress bar.\n\nharmony_client.wait_for_processing(job_id, show_progress=True)\n\n\n\nView Harmony job response and output URLs\nOnce your data request has finished processing, you can view details on the job that was submitted to Harmony, including the API call to Harmony, and informational messages on your request if available.\nresult_json() calls wait_for_processing() and returns the complete job in JSON format once processing is complete. You may optionally show the progress bar using the same show_progress=True statement in the previous line.\n\ndata = harmony_client.result_json(job_id)\nJSON(data)\n\n\n\nDownload and visualize Harmony output files\nThe files returned from your request are returned as HTTPS URLs in all cases other than when requesting Zarr as an output format(s3 URLs are returned instead to support accessing and interacting with those outputs in the cloud). For all Harmony requests, direct s3 URLs are also provided within a STAC catalog provided in the job response as we’ll explore in more detail below.\n\nRetrieve list of output URLs\nThe result_urls() method calls wait_for_processing() and returns a list of the processed data URLs once processing is complete. You may optionally show the progress bar as shown below.\n\nurls = harmony_client.result_urls(job_id, show_progress=True)\nurls\n\n\n\nDownload files\nThe next code block utilizes download_all() to download all data output file URLs. This is a non-blocking step during the download itself, but this line will block subsequent code while waiting for the job to finish processing. You can optionally specify a directory and specify whether to overwrite existing files as shown below.\nYou may call result() on future objects (those that are awaiting processing) to realize them. A call to result() blocks until that particular future object finishes downloading. Other future objects will download in the background, in parallel. When downloading is complete, the future objects will return the file path string of the file that was just downloaded. This file path can then be fed into other libraries that may read the data files and perform other operations.\n\nresults = harmony_client.download_all(job_id, directory='/tmp', overwrite=True)\nfile_names = [f.result() for f in results]\nfile_names\n\nWith download(), this will download only the URL specified, in case you would like more control over individual files.\n\nfile_name = harmony_client.download(urls[0], overwrite=True).result()\nfile_name\n\n\n\n\nVisualize Downloaded Outputs\nThe output image files can be visualized using the Rasterio library.\n\nfor r in results:\n    rasterio.plot.show(rasterio.open(r.result()))\n\n\n\nExplore output STAC catalog and retrieve results from s3\nA STAC catalog is returned in each Harmony request result. The stac items include not only the s3 locations for each output file, but also the spatial and temporal metadata representing each subsetted output. stac_catalog_url will return the URL of the catalog, and can also accept an optional progress status if desired.\n\nstac_catalog_url = harmony_client.stac_catalog_url(job_id)\nstac_catalog_url\n\n\nUsing PySTAC:\nFollowing the directions for PySTAC (https://pystac.readthedocs.io/en/latest/quickstart.html), we can hook our harmony-py client into STAC_IO.\n\nfrom urllib.parse import urlparse\nimport requests\nfrom pystac import STAC_IO\n\ndef requests_read_method(uri):\n    parsed = urlparse(uri)\n    if parsed.hostname.startswith('harmony.'):\n        return harmony_client.read_text(uri)\n    else:\n        return STAC_IO.default_read_text_method(uri)\n\nSTAC_IO.read_text_method = requests_read_method\n\nView the timestamp and s3 locations of each STAC item:\n\nfrom pystac import Catalog\n\ncat = Catalog.from_file(stac_catalog_url)\n\nprint(cat.title)\ns3_links = []\nfor item in cat.get_all_items():\n    print(item.datetime, [asset.href for asset in item.assets.values()])\n    s3_links.append([asset.href for asset in item.assets.values()])\n\n\n\nUsing intake-stac:\nView each item value returned:\n\nimport intake\ncat = intake.open_stac_catalog(stac_catalog_url)\ndisplay(list(cat))\n\nAnd the metadata contents of each item:\n\nentries = []\nfor id, entry in cat.search('type').items():\n    display(entry)\n    entries.append(entry)\n\n\n\n\nCloud in-place access\nNote that the remainder of this tutorial will only succeed when running this notebook within the AWS us-west-2 region.\nHarmony data outputs can be accessed within the cloud using the s3 URLs and AWS credentials provided in the Harmony job response. Below are examples using both intake-stac or boto3 to access the data in the cloud.\n\nAWS credential retrieval\nUsing aws_credentials you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents.\n\ncreds = harmony_client.aws_credentials()\ncreds\n\n\n\nUsing boto3\n\n#\n# NOTE: Execution of this cell will only succeed within the AWS us-west-2 region. \n#\n\nimport boto3\n\ns3 = boto3.client('s3', **creds)\nfor i in range(len(s3_links)):\n    uri = s3_links[i][0]\n    bucket = uri.split('/')[2]\n    obj = '/'.join(uri.split('/')[3:])\n    fn = uri.split('/')[-1]\n    with open(fn, 'wb') as f:\n        s3.download_fileobj(bucket, obj, f)\n\n\n\nUsing intake-stac\nOnce again, you can use intake-stac to directly access each output from Harmony in AWS. Viewing the file structure and plotting the image can be done in a few simple lines when working with the data in-region:\n\n#\n# NOTE: Execution of this cell will only succeed within the AWS us-west-2 region. \n#\n\nfor i in range(len(list(cat))):\n    da = cat[list(cat)[i]][entries[i].describe()['name']].to_dask()\n    display(da)\n    da.plot()"
  },
  {
    "href": "get-started/earthdata-login.html",
    "title": "EarthData Login",
    "section": "",
    "text": "To access NASA data, you need EarthData Login…\n\nPrevious notes/ideas\nTODO: develop as prose to set up for the following .ipynb examples\nTo access NASA data you have to authenticate.\nSolutions that work - these are detailed in separate chapters as Jupyter notebooks (.ipynb).\n\nTo access NASA data one must setup an Earthdata Login profile. This involves (prose here)\n\nSee/link to Christine’s post & conversation on the Jupyter discourse forum\n\nCreate a netrc file\n\nSubmit EDL credentials within a script\n\nSome talk about the redirects…"
  },
  {
    "href": "get-started/index.html#prerequisites",
    "title": "Getting Started",
    "section": "Prerequisites",
    "text": "For programmatic access to NASA data on the cloud, we recommend familiarity with the following:\n\nIntroduction to Open Data Science with R\nIntroduction to Geospatial Raster and Vector Data with Python\nProject Pythia Foundations Book"
  },
  {
    "href": "get-started/index.html#cloud-optimized-data-formats",
    "title": "Getting Started",
    "section": "Cloud Optimized Data Formats",
    "text": "Some great info here about Cloud Optimized Data Formats.\n\nCloud-Performant NetCDF4/HDF5 Reading with the Zarr Library"
  },
  {
    "href": "get-started/index.html#sw3-buckets-etc",
    "title": "Getting Started",
    "section": "SW3 Buckets, etc",
    "text": "NASA AWS Cloud Primer"
  },
  {
    "href": "get-started/lpdaac-netrc.html#this-is-otherwise-just-testing",
    "title": "LP DAAC Authentication Example",
    "section": "This is otherwise just testing ",
    "text": "Import the required packages and set the input/working directory to run this Jupyter Notebook locally.\n\nimport requests as r\nfrom skimage import io\nimport matplotlib.pyplot as plt\n\n\nprint(1+1)\nprint(\"woo NASA data!\")\n\n2\nwoo NASA data!"
  },
  {
    "href": "get-started/how-to-use.html",
    "title": "How to use this book",
    "section": "",
    "text": "Draft prose here by Andy Barrett.\nIncludes both building blocks and end-to-end workflow examples that put together some of those building blocks. User can ‘choose their own adventure’ and build their own workflows based on their need."
  },
  {
    "href": "get-started/podaac-ecco-netrc.html",
    "title": "PO DAAC Authentication Example",
    "section": "",
    "text": "Note this is currently copied from https://github.com/podaac/ECCO/blob/main/Data_Access/local_bulk_download_and_open_in_py3.ipynb as a starting point and will be refined\n\n\n\n\nConfigure your .netrc file\nGood idea to back up your existing netrc file, if you have one. And while youre at it check for these entries because they might exist in there already:\n\n%cp ~/.netrc ~/bak.netrc\n\n%cat ~/.netrc | grep '.earthdata.nasa.gov' | cut -f-5 -d\" \"\n\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\n\n\n\nAdd entries to your netrc for these two earthdata.nasa.gov sub domains, at a minimum:\nmachine urs.earthdata.nasa.gov login jmcnelis password ***\nmachine opendap.earthdata.nasa.gov login jmcnelis password ***\nand replace jmcnelis and *** with your Earthdata Login username and password, respectively…\n\nReplace jmcnelis and *** with your Earthdata username and password, and then run the cell to append these two lines to your netrc file, if one exists. Otherwise write them to a new one. (all set up by -a)\n\n%%file -a ~/.netrc\nmachine urs.earthdata.nasa.gov login jmcnelis password ***\nmachine opendap.earthdata.nasa.gov login jmcnelis password ***\n\nAppending to /Users/lowndes/.netrc\n\n\nDump the netrc again sans passwords to confirm that it worked:\n\n!cat ~/.netrc | grep '.earthdata.nasa.gov' | cut -f-5 -d\" \"\n\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\n\n\nFinally, you need to make sure to limit access to the netrc file because it stores your plain text password. Simple on MacOS and Linux:\n\n!chmod 0600 ~/.netrc"
  },
  {
    "href": "get-started/authentication.html",
    "title": "Authentication",
    "section": "",
    "text": "There are multiple ways to authenticate. Options are:…\nThe following are example notebooks (.ipynb)…"
  },
  {
    "href": "get-started/api-primer.html",
    "title": "API Primer",
    "section": "",
    "text": "“Primer” about Interacting with NASA Earthdata APIs"
  },
  {
    "href": "processing/processing.html",
    "title": "Processing",
    "section": "",
    "text": "Processing details here"
  }
]