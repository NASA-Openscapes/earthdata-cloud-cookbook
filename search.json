[
  {
    "objectID": "access/cof-via-harmony.html",
    "href": "access/cof-via-harmony.html",
    "title": "COF via Earthdata Harmony",
    "section": "",
    "text": "In COF (zarr) via Earthdata Harmony API (services) (scripted, in cloud)"
  },
  {
    "objectID": "access/download-to-local.html",
    "href": "access/download-to-local.html",
    "title": "Download to local",
    "section": "",
    "text": "Download to local machine"
  },
  {
    "objectID": "access/podaac-ecco-data_access-cloud_direct_access_s3.html#getting-started",
    "href": "access/podaac-ecco-data_access-cloud_direct_access_s3.html#getting-started",
    "title": "",
    "section": "Getting Started",
    "text": "In this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\nWe will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.large instance (2 cpus; 8GB memory).\n\n\nPython 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\ncartopy\n\nimport s3fs\nimport requests\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeat\nfrom json import dumps\nfrom io import StringIO\nfrom os.path import dirname, join\nfrom IPython.display import HTML\n\nplt.rcParams.update({'font.size': 14})\nMake a folder to write some outputs, if needed:\n!mkdir -p outputs/\n\n\n\nInputs\nConfigure one input: the ShortName of the desired dataset from ECCO V4r4. In this case it’s the following string that unique identifies the collection of monthly, 0.5-degree sea surface height data.\nShortName = \"ECCO_L4_SSH_05DEG_MONTHLY_V4R4\"\n\n\nEarthdata Login\nYou should have a .netrc file set up like:\nmachine urs.earthdata.nasa.gov login <username> password <password>\n\n\nDirect access from S3\nSet up an s3fs session for authneticated access to ECCO netCDF files in s3:\n\ndef begin_s3_direct_access(url: str=\"https://archive.podaac.earthdata.nasa.gov/s3credentials\"):\n    response = requests.get(url).json()\n    return s3fs.S3FileSystem(key=response['accessKeyId'],\n                             secret=response['secretAccessKey'],\n                             token=response['sessionToken'],\n                             client_kwargs={'region_name':'us-west-2'})\n\nfs = begin_s3_direct_access()\n\ntype(fs)\n\ns3fs.core.S3FileSystem"
  },
  {
    "objectID": "access/podaac-ecco-data_access-cloud_direct_access_s3.html#datasets",
    "href": "access/podaac-ecco-data_access-cloud_direct_access_s3.html#datasets",
    "title": "",
    "section": "Datasets",
    "text": "sea surface height (0.5-degree gridded, monthly)\nECCO_L4_SSH_05DEG_MONTHLY_V4R4\nGet a list of netCDF files located at the S3 path corresponding to the ECCO V4r4 monthly sea surface height dataset on the 0.5-degree latitude/longitude grid.\n\nssh_Files = fs.glob(join(\"podaac-ops-cumulus-protected/\", ShortName, \"*2015*.nc\"))\n\nlen(ssh_Files)\n\n12\n\n\nOpen with the netCDF files using the s3fs package, then load them all at once into a concatenated xarray dataset.\n\nssh_Dataset = xr.open_mfdataset(\n    paths=[fs.open(f) for f in ssh_Files],\n    combine='by_coords',\n    mask_and_scale=True,\n    decode_cf=True,\n    chunks={'latitude': 60,   # These were chosen arbitrarily. You must specify \n            'longitude': 120, # chunking that is suitable to the data and target\n            'time': 100}      # analysis.\n)\n\nssh = ssh_Dataset.SSH\n\nprint(ssh)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\n\n\nPlot the gridded sea surface height time series\nBut only the timesteps beginning in 2015:\n\nssh_after_201x = ssh[ssh['time.year']>=2015,:,:]\n\nprint(ssh_after_201x)\n\n<xarray.DataArray 'SSH' (time: 12, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(12, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-16T12:00:00 ... 2015-12-16T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    long_name:              Dynamic sea surface height anomaly\n    standard_name:          sea_surface_height_above_geoid\n    units:                  m\n    comment:                Dynamic sea surface height anomaly above the geoi...\n    valid_min:              [-1.88057721]\n    valid_max:              [1.42077196]\n\n\nPlot the grid for the first time step using a Robinson projection. Define a helper function for consistency throughout the notebook:\n\ndef make_figure(proj):\n    fig = plt.figure(figsize=(16,6))\n    ax = fig.add_subplot(1, 1, 1, projection=proj)\n    ax.add_feature(cfeat.LAND)\n    ax.add_feature(cfeat.OCEAN)\n    ax.add_feature(cfeat.COASTLINE)\n    ax.add_feature(cfeat.BORDERS, linestyle='dotted')\n    return fig, ax\n\nfig, ax = make_figure(proj=ccrs.Robinson())\n\nssh_after_201x.isel(time=0).plot(ax=ax, transform=ccrs.PlateCarree(), cmap='Spectral_r')\n\n<matplotlib.collections.QuadMesh at 0x7fae2533d730>\n\n\n\n\n\nNow plot the whole time series (post-2010) in an animation and write it to an mp4 file called ecco_monthly_ssh_grid_2015_to_x.mp4:\ndef get_animation(var, cmap: str=\"Spectral_r\"):\n    \"\"\"Get time series animation for input xarray dataset\"\"\"\n\n    def draw_map(i: int, add_colorbar: bool):\n        data = var[i]\n        m = data.plot(ax=ax, \n                      transform=ccrs.PlateCarree(),\n                      add_colorbar=add_colorbar,\n                      vmin=var.valid_min, \n                      vmax=var.valid_max,\n                      cmap=cmap)\n        plt.title(str(data.time.values)[:7])\n        return m\n\n    def init():\n        return draw_map(0, add_colorbar=True)\n    \n    def animate(i):\n        return draw_map(i, add_colorbar=False)\n\n    return init, animate\nNow make the animation using the function:\nfig, ax = make_figure(proj=ccrs.Robinson())\n\ninit, animate = get_animation(ssh_after_201x)\n\nani = animation.FuncAnimation(fig=fig, \n                              func=animate, \n                              frames=ssh_after_201x.time.size, \n                              init_func=init, \n                              interval=0.2, \n                              blit=False, \n                              repeat=False)\n\n# Now save the animation to an MP4 file:\nani.save('outputs/ecco_monthly_ssh_grid_2015_to_x.mp4', writer=animation.FFMpegWriter(fps=8))\n\nplt.close(fig)\nRender the animation in the ipynb:\n#HTML(ani.to_html5_video())\n\n\ntflux (0.5-degree gridded, daily)\nNow we will do something similar to access daily, gridded (0.5-degree) ocean and sea-ice surface heat fluxes (10.5067/ECG5D-HEA44). Read more about the dataset and the rest of the ECCO V4r4 product suite on the PO.DAAC Web Portal.\nUse a “glob” pattern when listing the S3 bucket contents such that only netCDFs from January 2015 are represented in the resulting list of paths.\n\ntflux_Files = fs.glob(join(\"podaac-ops-cumulus-protected/\", \"ECCO_L4_HEAT_FLUX_05DEG_DAILY_V4R4\", \"*2015-01*.nc\"))\n\nlen(tflux_Files)\n\n31\n\n\nNow open them all as one xarray dataset just like before. Open and pass the 365 netCDF files to the xarray.open_mfdataset constructor so that we can operate on them as a single aggregated dataset.\n\ntflux_Dataset = xr.open_mfdataset(\n    paths=[fs.open(f) for f in tflux_Files],\n    combine='by_coords',\n    mask_and_scale=True,\n    decode_cf=True,\n    chunks={'latitude': 60,   # These were chosen arbitrarily. You must specify \n            'longitude': 120, # chunking that is suitable to the data and target\n            'time': 100}      # analysis.\n)\n\ntflux = tflux_Dataset.TFLUX\n\nprint(tflux)\n\n<xarray.DataArray 'TFLUX' (time: 31, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(31, 360, 720), dtype=float32, chunksize=(1, 60, 120), chunktype=numpy.ndarray>\nCoordinates:\n  * time       (time) datetime64[ns] 2015-01-01T12:00:00 ... 2015-01-31T12:00:00\n  * latitude   (latitude) float32 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float32 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\nAttributes:\n    coverage_content_type:  modelResult\n    direction:              >0 increases potential temperature (THETA)\n    long_name:              Rate of change of ocean heat content per m2 accou...\n    units:                  W m-2\n    comment:                The rate of change of ocean heat content due to h...\n    valid_min:              [-1713.51220703]\n    valid_max:              [870.31304932]\n\n\nSelect a region over the Gulf of Mexico and spatially subset it from the larger dataset by slicing on the latitude and longitude axes.\n\ntflux_gom = tflux.sel(latitude=slice(15, 40), \n                      longitude=slice(-105, -70))\n\nprint(tflux_gom.shape)\n\n(31, 50, 70)\n\n\n\ntflux_gom.isel(time=0).plot()\n\n<matplotlib.collections.QuadMesh at 0x7fae1689a550>\n\n\n\n\n\nPlot the Jan 2015 surface heat flux as a gridded time series animation over the GOM study region.\nfig, ax = make_figure(proj=ccrs.Mercator())\n\nax.coastlines()\nax.set_extent([tflux_gom.longitude.min(), \n               tflux_gom.longitude.max(), \n               tflux_gom.latitude.min(), \n               tflux_gom.latitude.max()])\n\ninit, animate = get_animation(tflux_gom, cmap=\"RdBu\")\n\n# Plot a time series animation write it to an mp4 file:\nani = animation.FuncAnimation(fig=fig, \n                              func=animate, \n                              frames=tflux_gom.time.size, \n                              init_func=init, \n                              interval=0.2, \n                              blit=False, \n                              repeat=False)\n\nani.save('outputs/ecco_daily_tflux_gom_2015.mp4', writer=animation.FFMpegWriter(fps=8))\n\nplt.close(fig)"
  },
  {
    "objectID": "access/index.html",
    "href": "access/index.html",
    "title": "NASA Cloud Data Access",
    "section": "",
    "text": "Some background here about access."
  },
  {
    "objectID": "access/earthdata-search.html",
    "href": "access/earthdata-search.html",
    "title": "Earthdata Search",
    "section": "",
    "text": "Earthdata Search (UI)"
  },
  {
    "objectID": "access/opendap-cloud.html",
    "href": "access/opendap-cloud.html",
    "title": "OPeNDAP in the Cloud",
    "section": "",
    "text": "OPeNDAP in the Cloud (OPeNDAP Hyrax)"
  },
  {
    "objectID": "access/direct-in-region.html",
    "href": "access/direct-in-region.html",
    "title": "Direct in-region",
    "section": "",
    "text": "(scripted, not cloud)\ntext here"
  },
  {
    "objectID": "access/data-access-example.html#jupyter-notebook-that-demonstrates-how-to-access-data",
    "href": "access/data-access-example.html#jupyter-notebook-that-demonstrates-how-to-access-data",
    "title": "Data access demo",
    "section": "Jupyter Notebook that demonstrates how to access data",
    "text": "See outline for more details. This file can be replaced by a real .ipynb example."
  },
  {
    "objectID": "examples/index.html",
    "href": "examples/index.html",
    "title": "Example tutorials",
    "section": "",
    "text": "Upcoming, there will be end-to-end workflow examples, linked to pieces from examples above and vice-versa.\nFor the moment, the following notebooks illustrate how we’re able to include .ipynb, .rmd, and .qmd files and execute python and R code in our tutorials."
  },
  {
    "objectID": "examples/NSIDC/ICESat2-CMR-OnPrem-vs-Cloud.html#data-used",
    "href": "examples/NSIDC/ICESat2-CMR-OnPrem-vs-Cloud.html#data-used",
    "title": "",
    "section": "Data used:",
    "text": "ICESat-2 ATL03: This data set contains height above the WGS 84 ellipsoid (ITRF2014 reference frame), latitude, longitude, and time for all photons."
  },
  {
    "objectID": "examples/NSIDC/ICESat2-CMR-OnPrem-vs-Cloud.html#requirements",
    "href": "examples/NSIDC/ICESat2-CMR-OnPrem-vs-Cloud.html#requirements",
    "title": "",
    "section": "Requirements",
    "text": "NASA Eartdata Login (EDL) credentials\npython libraries:\n\naws-cli\nxarray\nfs-s3fs\nzarr\ncmr\n\npython-cmr (optional fallback) NSIDC fork"
  },
  {
    "objectID": "examples/NSIDC/ICESat2-CMR-OnPrem-vs-Cloud.html#querying-cmr-for-nsidc-data",
    "href": "examples/NSIDC/ICESat2-CMR-OnPrem-vs-Cloud.html#querying-cmr-for-nsidc-data",
    "title": "",
    "section": "Querying CMR for NSIDC data",
    "text": "Most collections at NSIDC have not being migrated to the cloud and can be found using CMR with no authentication at all. Here is a simple example for altimeter data (ATL03) coming from the ICESat-2 mission. First we’ll search the regular collection and then we’ll do the same using the cloud collection.\nNote: This notebook uses a low level CMR endpoint, this won’t be not the only workflow for data discovery.\n\nfrom cmr.search import collection as cmr_collection\nfrom cmr.search import granule \nfrom cmr.auth import token\n\nimport textwrap\n# NON_AWS collections are hosted at the NSIDC DAAC data center\n# AWS_CLOUD collections are hosted at AWS S3 us-west-2\nNSIDC_PROVIDERS = {\n    'NSIDC_HOSTED': 'NSIDC_ECS', \n    'AWS_HOSTED':'NSIDC_CPRD'\n}\n\n# First let's search for some collections hosted at NSIDC using a keyword\ncollections = cmr_collection.search({'keyword':'ice',\n                                     'provider': NSIDC_PROVIDERS['NSIDC_HOSTED']})\n\n# Let's print some information about the first 3 collection that match our provider\nfor collection in collections[0:3]:\n    wrapped_abstract = '\\n'.join(textwrap.wrap(f\"Abstract: {collection['umm']['Abstract']}\", 80)) + '\\n'\n    print(f\"concept-id: {collection['meta']['concept-id']}\\n\" +\n          f\"Title: {collection['umm']['EntryTitle']}\\n\" +\n          wrapped_abstract)\n\nconcept-id: C1997321091-NSIDC_ECS\nTitle: ATLAS/ICESat-2 L2A Global Geolocated Photon Data V004\nAbstract: This data set (ATL03) contains height above the WGS 84 ellipsoid\n(ITRF2014 reference frame), latitude, longitude, and time for all photons\ndownlinked by the Advanced Topographic Laser Altimeter System (ATLAS) instrument\non board the Ice, Cloud and land Elevation Satellite-2 (ICESat-2) observatory.\nThe ATL03 product was designed to be a single source for all photon data and\nancillary information needed by higher-level ATLAS/ICESat-2 products. As such,\nit also includes spacecraft and instrument parameters and ancillary data not\nexplicitly required for ATL03.\n\nconcept-id: C1705401930-NSIDC_ECS\nTitle: ATLAS/ICESat-2 L2A Global Geolocated Photon Data V003\nAbstract: This data set (ATL03) contains height above the WGS 84 ellipsoid\n(ITRF2014 reference frame), latitude, longitude, and time for all photons\ndownlinked by the Advanced Topographic Laser Altimeter System (ATLAS) instrument\non board the Ice, Cloud and land Elevation Satellite-2 (ICESat-2) observatory.\nThe ATL03 product was designed to be a single source for all photon data and\nancillary information needed by higher-level ATLAS/ICESat-2 products. As such,\nit also includes spacecraft and instrument parameters and ancillary data not\nexplicitly required for ATL03.\n\nconcept-id: C2003771331-NSIDC_ECS\nTitle: ATLAS/ICESat-2 L3A Land Ice Height V004\nAbstract: This data set (ATL06) provides geolocated, land-ice surface heights\n(above the WGS 84 ellipsoid, ITRF2014 reference frame), plus ancillary\nparameters that can be used to interpret and assess the quality of the height\nestimates. The data were acquired by the Advanced Topographic Laser Altimeter\nSystem (ATLAS) instrument on board the Ice, Cloud and land Elevation Satellite-2\n(ICESat-2) observatory.\n\n\n\n\n# Now let's do the same with short names, a more specific way of finding data.\n\n#First let's search for some collections hosted at NSIDC\ncollections = cmr_collection.search({'short_name':'ATL03',\n                                     'provider': NSIDC_PROVIDERS['NSIDC_HOSTED']})\n\n# Note how we get back the same collection twice, that's because we have 2 versions available.\nfor collection in collections[0:3]:\n    wrapped_abstract = '\\n'.join(textwrap.wrap(f\"Abstract: {collection['umm']['Abstract']}\", 80)) + '\\n'\n    print(f\"concept-id: {collection['meta']['concept-id']}\\n\" +\n          f\"Title: {collection['umm']['EntryTitle']}\\n\" +\n          wrapped_abstract)\n\nconcept-id: C1997321091-NSIDC_ECS\nTitle: ATLAS/ICESat-2 L2A Global Geolocated Photon Data V004\nAbstract: This data set (ATL03) contains height above the WGS 84 ellipsoid\n(ITRF2014 reference frame), latitude, longitude, and time for all photons\ndownlinked by the Advanced Topographic Laser Altimeter System (ATLAS) instrument\non board the Ice, Cloud and land Elevation Satellite-2 (ICESat-2) observatory.\nThe ATL03 product was designed to be a single source for all photon data and\nancillary information needed by higher-level ATLAS/ICESat-2 products. As such,\nit also includes spacecraft and instrument parameters and ancillary data not\nexplicitly required for ATL03.\n\nconcept-id: C1705401930-NSIDC_ECS\nTitle: ATLAS/ICESat-2 L2A Global Geolocated Photon Data V003\nAbstract: This data set (ATL03) contains height above the WGS 84 ellipsoid\n(ITRF2014 reference frame), latitude, longitude, and time for all photons\ndownlinked by the Advanced Topographic Laser Altimeter System (ATLAS) instrument\non board the Ice, Cloud and land Elevation Satellite-2 (ICESat-2) observatory.\nThe ATL03 product was designed to be a single source for all photon data and\nancillary information needed by higher-level ATLAS/ICESat-2 products. As such,\nit also includes spacecraft and instrument parameters and ancillary data not\nexplicitly required for ATL03.\n\n\n\n\n# now that we have the concept-ids we can look for data granules in that collection and pass spatiotemporal parameters.\nfrom cmr_serializer import QueryResult\n\n# a bbox over Juneau Icefield \n# bbox = min Longitude , min Latitude , max Longitude , max Latitude \nquery = {'concept-id': 'C1997321091-NSIDC_ECS',\n         'bounding_box': '-135.1977,58.3325,-133.3410,58.9839'}\n\n# Querying for ATL03 v3 using its concept-id and a bounding box\nresults = granule.search(query, limit=1000)\n# This is a wrapper with convenient methods to work with CMR query results.\ngranules = QueryResult(results).items()\n\nprint(f\"Total granules found: {len(results)} \\n\")\nfor g in granules[0:3]:\n    display(g)\n\nTotal granules found: 201 \n\n\n\n\n        \n          Id: ATL03_20181014001049_02350102_004_01.h5\n          Collection: {'EntryTitle': 'ATLAS/ICESat-2 L2A Global Geolocated Photon Data V004'}\n          Spatial coverage: {'HorizontalSpatialDomain': {'Orbit': {'AscendingCrossing': -127.0482205607256, 'StartLatitude': 27.0, 'StartDirection': 'A', 'EndLatitude': 59.5, 'EndDirection': 'A'}}}\n          Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2018-10-14T00:10:49.722Z', 'EndingDateTime': '2018-10-14T00:19:19.918Z'}}\n          Size(MB): 1764.5729866028 \n          Data: https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.004/2018.10.14/ATL03_20181014001049_02350102_004_01.h5\n          \n        \n        \n\n\n\n        \n          Id: ATL03_20181015124359_02580106_004_01.h5\n          Collection: {'EntryTitle': 'ATLAS/ICESat-2 L2A Global Geolocated Photon Data V004'}\n          Spatial coverage: {'HorizontalSpatialDomain': {'Orbit': {'AscendingCrossing': 49.70324528818096, 'StartLatitude': 59.5, 'StartDirection': 'D', 'EndLatitude': 27.0, 'EndDirection': 'D'}}}\n          Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2018-10-15T12:43:57.696Z', 'EndingDateTime': '2018-10-15T12:52:28.274Z'}}\n          Size(MB): 276.2403841019 \n          Data: https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.004/2018.10.15/ATL03_20181015124359_02580106_004_01.h5\n          \n        \n        \n\n\n\n        \n          Id: ATL03_20181018000228_02960102_004_01.h5\n          Collection: {'EntryTitle': 'ATLAS/ICESat-2 L2A Global Geolocated Photon Data V004'}\n          Spatial coverage: {'HorizontalSpatialDomain': {'Orbit': {'AscendingCrossing': -127.82682215638665, 'StartLatitude': 27.0, 'StartDirection': 'A', 'EndLatitude': 59.5, 'EndDirection': 'A'}}}\n          Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2018-10-18T00:02:28.717Z', 'EndingDateTime': '2018-10-18T00:10:58.903Z'}}\n          Size(MB): 877.0574979782 \n          Data: https://n5eil01u.ecs.nsidc.org/DP9/ATLAS/ATL03.004/2018.10.18/ATL03_20181018000228_02960102_004_01.h5\n          \n        \n        \n\n\n# We  can access the data links with the data_links()\nfor g in granules[0:10]:\n    print(g.data_links())"
  },
  {
    "objectID": "examples/NSIDC/ICESat2-CMR-OnPrem-vs-Cloud.html#cloud-collections",
    "href": "examples/NSIDC/ICESat2-CMR-OnPrem-vs-Cloud.html#cloud-collections",
    "title": "",
    "section": "Cloud Collections",
    "text": "Some NSIDC cloud collections are not yet public we need to authenticate ourselves with CMR first.\n\nimport getpass\nimport textwrap\n\nfrom cmr.search import collection as cmr_collection\nfrom cmr.search import granule \nfrom cmr.auth import token\n\nfrom cmr_auth import CMRAuth\n\n# NON_AWS collections are hosted at the NSIDC DAAC data center\n# AWS_CLOUD collections are hosted at AWS S3 us-west-2\nNSIDC_PROVIDERS = {\n    'NSIDC_HOSTED': 'NSIDC_ECS', \n    'AWS_HOSTED':'NSIDC_CPRD'\n}\n\n# Use your own EDL username\nUSER= 'betolink'\n\nprint('Enter your NASA Earthdata login password:')\npassword = getpass.getpass()\nCMR_auth = CMRAuth(USER, password)\n# Token to search private collections on CMR\ncmr_token = CMR_auth.get_token()\n\nEnter your NASA Earthdata login password:\n\n\n ········\n\n\n\n# Now let's start our aunthenticated queries on CMR\nquery = {'short_name':'ATL03',\n         'token': cmr_token,\n         'provider': NSIDC_PROVIDERS['AWS_HOSTED']}\n\ncollections = cmr_collection.search(query)\n\nfor collection in collections[0:3]:\n    wrapped_abstract = '\\n'.join(textwrap.wrap(f\"Abstract: {collection['umm']['Abstract']}\", 80)) + '\\n'\n    print(f\"concept-id: {collection['meta']['concept-id']}\\n\" +\n          f\"Title: {collection['umm']['EntryTitle']}\\n\" +\n          wrapped_abstract)\n\nconcept-id: C2027878642-NSIDC_CPRD\nTitle: ATLAS/ICESat-2 L2A Global Geolocated Photon Data V004\nAbstract: This data set (ATL03) contains height above the WGS 84 ellipsoid\n(ITRF2014 reference frame), latitude, longitude, and time for all photons\ndownlinked by the Advanced Topographic Laser Altimeter System (ATLAS) instrument\non board the Ice, Cloud and land Elevation Satellite-2 (ICESat-2) observatory.\nThe ATL03 product was designed to be a single source for all photon data and\nancillary information needed by higher-level ATLAS/ICESat-2 products. As such,\nit also includes spacecraft and instrument parameters and ancillary data not\nexplicitly required for ATL03.\n\n\n\n\n# now that we have the concept-id for our ATL03 in the cloud we do the same thing we did with ATL03 hosted at\nfrom cmr_serializer import QueryResult\n# NSIDC but using the cloud concept-id\n# Jeneau ice sheet\nquery = {'concept-id': 'C2027878642-NSIDC_CPRD',\n         'token': cmr_token,\n         'bounding_box': '-135.1977,58.3325,-133.3410,58.9839'}\n\n# Querying for ATL03 v3 using its concept-id and a bounding box\nresults = granule.search(query, limit=1000)\ngranules = QueryResult(results).items()\n\nprint(f\"Total granules found: {len(results)} \\n\")\n\n# Print the first 3 granules\nfor g in granules[0:3]:\n    display(g)\n    # You can use: print(g) for the regular text representation.\n\nTotal granules found: 135 \n\n\n\n\n        \n          Id: ATL03_20181014001049_02350102_004_01.h5\n          Collection: {'EntryTitle': 'ATLAS/ICESat-2 L2A Global Geolocated Photon Data V004'}\n          Spatial coverage: {'HorizontalSpatialDomain': {'Orbit': {'AscendingCrossing': -127.0482205607256, 'StartLatitude': 27.0, 'StartDirection': 'A', 'EndLatitude': 59.5, 'EndDirection': 'A'}}}\n          Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2018-10-14T00:10:49.722Z', 'EndingDateTime': '2018-10-14T00:19:19.918Z'}}\n          Size(MB): 1764.5729866027832 \n          Data: https://data.nsidc.earthdatacloud.nasa.gov/nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/10/14/ATL03_20181014001049_02350102_004_01.h5s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/10/14/ATL03_20181014001049_02350102_004_01.h5\n          \n        \n        \n\n\n\n        \n          Id: ATL03_20181015124359_02580106_004_01.h5\n          Collection: {'EntryTitle': 'ATLAS/ICESat-2 L2A Global Geolocated Photon Data V004'}\n          Spatial coverage: {'HorizontalSpatialDomain': {'Orbit': {'AscendingCrossing': 49.70324528818096, 'StartLatitude': 59.5, 'StartDirection': 'D', 'EndLatitude': 27.0, 'EndDirection': 'D'}}}\n          Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2018-10-15T12:43:57.696Z', 'EndingDateTime': '2018-10-15T12:52:28.274Z'}}\n          Size(MB): 276.2403841018677 \n          Data: https://data.nsidc.earthdatacloud.nasa.gov/nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/10/15/ATL03_20181015124359_02580106_004_01.h5s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/10/15/ATL03_20181015124359_02580106_004_01.h5\n          \n        \n        \n\n\n\n        \n          Id: ATL03_20181018000228_02960102_004_01.h5\n          Collection: {'EntryTitle': 'ATLAS/ICESat-2 L2A Global Geolocated Photon Data V004'}\n          Spatial coverage: {'HorizontalSpatialDomain': {'Orbit': {'AscendingCrossing': -127.82682215638665, 'StartLatitude': 27.0, 'StartDirection': 'A', 'EndLatitude': 59.5, 'EndDirection': 'A'}}}\n          Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2018-10-18T00:02:28.717Z', 'EndingDateTime': '2018-10-18T00:10:58.903Z'}}\n          Size(MB): 877.0574979782104 \n          Data: https://data.nsidc.earthdatacloud.nasa.gov/nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/10/18/ATL03_20181018000228_02960102_004_01.h5s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/10/18/ATL03_20181018000228_02960102_004_01.h5\n          \n        \n        \n\n\n\nNOTE: Not all the data granules for NSIDC datasets have been migrated to S3. This might result in different counts between the NSIDC hosted data collections and the ones in AWS S3\n\n# We can list the s3 links but \nfor g in granules[0:10]:\n    print(g.data_links(only_s3=True))\n\n['s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/10/14/ATL03_20181014001049_02350102_004_01.h5']\n['s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/10/15/ATL03_20181015124359_02580106_004_01.h5']\n['s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/10/18/ATL03_20181018000228_02960102_004_01.h5']\n['s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/11/05/ATL03_20181105113651_05780106_004_01.h5']\n['s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/11/07/ATL03_20181107225525_06160102_004_01.h5']\n['s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/11/09/ATL03_20181109112837_06390106_004_01.h5']\n['s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/11/11/ATL03_20181111224708_06770102_004_01.h5']\n['s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/11/15/ATL03_20181115223845_07380102_004_01.h5']\n['s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/12/04/ATL03_20181204101243_10200106_004_01.h5']\n['s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/12/06/ATL03_20181206213114_10580102_004_01.h5']\n\n\nWe note that our RelatedLinks array now contain links to AWS S3, these are the direct URIs for our data granules in the AWS us-west-2 region."
  },
  {
    "objectID": "examples/NSIDC/ICESat2-CMR-OnPrem-vs-Cloud.html#data-access-using-aws-s3",
    "href": "examples/NSIDC/ICESat2-CMR-OnPrem-vs-Cloud.html#data-access-using-aws-s3",
    "title": "",
    "section": "Data Access using AWS S3",
    "text": "IMPORTANT: This section will only work if this notebook is running on the AWS us-west-2 zone\n\nThere is more than one way of accessing data on AWS S3, either downloading it to your local machine using the official client library or using a python library.\nPerformance tip: using the HTTPS URLs will decrease the access performance since these links have to internally be processed by AWS’s content delivery system (CloudFront). To get a better performance we should access the S3:// URLs with BOTO3 or a high level S3 enabled library (i.e. S3FS)\nRelated links: * HDF in the Cloud challenges and solutions for scientific data * Cloud Storage (Amazon S3) HDF5 Connector\n# READ only temporary credentials\nimport s3fs\nimport h5py\n\n# This credentials only last 1 hour.\ns3_cred = CMR_auth.get_s3_credentials()\n\n\ns3_fs = s3fs.S3FileSystem(key=s3_cred['accessKeyId'],\n                          secret=s3_cred['secretAccessKey'],\n                          token=s3_cred['sessionToken'])\n\n# Now you could grab S3 links to your cloud instance (EC2, Hub etc) using:\n# s3_fs.get('s3://SOME_LOCATION/ATL03_20181015124359_02580106_004_01.h5', 'test.h5')\n\nWe now have the propper credentials and file mapper to access the data within AWS us-west-2.\nwith s3_fs.open('s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/10/15/ATL03_20181015124359_02580106_004_01.h5', 'rb') as s3f:\n    with h5py.File(s3f, 'r') as f:\n        print([key for key in f.keys()])\n\n\nUsing xarray to open files on S3\nATL data is complex so xarray doesn’t know how to extract the important bits out of it.\nimport xarray\n\nwith s3_fs.open('s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/10/15/ATL03_20181015124359_02580106_004_01.h5', 'rb') as s3f:\n    ds= xarray.open_dataset(s3f)\n    for varname in ds:\n        print(varname)\nds\n\n\n“Downloading” files on S3 using the official aws-cli library\nThe quotes on downloading are because ideally you’ll be working on an EC2 (virtual machine for short) instance on the us-west-2 region."
  },
  {
    "objectID": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#the-big-picture",
    "href": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#the-big-picture",
    "title": "",
    "section": "The big picture:",
    "text": "There is nothing wrong with downloading data to our local machine but that can get complicated or even impossible if a dataset is too large. For this reason NSIDC along with other NASA data centers started to collocate or migrate their dataset holdings to the cloud.\n\nSteps\n\nAuthenticate with the NASA Earthdata Login API (EDL).\nSearch granules/collections using a CMR client that supports authentication\nParse CMR responses and get AWS S3 URLs\nAccess the data granules using temporary AWS credentials given by the NSIDC cloud credentials endpoint\n\n\n\nData used:\n\nICESat-2 ATL03: This data set contains height above the WGS 84 ellipsoid (ITRF2014 reference frame), latitude, longitude, and time for all photons.\n\n\n\nRequirements\n\nNASA Eartdata Login (EDL) credentials\npython libraries:\n\nh5py\nmatplotlib\nxarray\ns3fs\npython-cmr\n\ncmr helpers: included in this notebook"
  },
  {
    "objectID": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#querying-cmr-for-nsidc-data-in-the-cloud",
    "href": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#querying-cmr-for-nsidc-data-in-the-cloud",
    "title": "",
    "section": "Querying CMR for NSIDC data in the cloud",
    "text": "Most collections at NSIDC have not being migrated to the cloud and can be found using CMR with no authentication at all. Here is a simple example for altimeter data (ATL03) coming from the ICESat-2 mission. First we’ll search the regular collection and then we’ll do the same using the cloud collection.\nNote: This notebook uses CMR to search and locate the data granules, this is not the only workflow for data access and discovery.\n\nHarmonyPy: Uses Harmony the NASA API to search, subset and transform the data in the cloud.\ncmr-stac: A “static” metadata catalog than can be read by Intake oand other client libraries to optimize the access of files in the cloud."
  },
  {
    "objectID": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#cloud-collections",
    "href": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#cloud-collections",
    "title": "",
    "section": "Cloud Collections",
    "text": "Some NSIDC cloud collections are not yet, which means that temporarily you’ll have to request access emailing nsidc@nsidc.org so your Eartdata login is in the authorized list for early users.\nimport getpass\nimport textwrap\n\nfrom cmr.search import collection as cmr_collection\nfrom cmr.search import granule \nfrom cmr.auth import token\n\nfrom cmr_auth import CMRAuth\n\n# NON_AWS collections are hosted at the NSIDC DAAC data center\n# AWS_CLOUD collections are hosted at AWS S3 us-west-2\nNSIDC_PROVIDERS = {\n    'NSIDC_HOSTED': 'NSIDC_ECS', \n    'AWS_HOSTED':'NSIDC_CPRD'\n}\n\n# Use your own EDL username\nUSER = 'betolink'\n\nprint('Enter your NASA Earthdata login password:')\npassword = getpass.getpass()\n# This helper class will handle credentials with CMR\nCMR_auth = CMRAuth(USER, password)\n# Token to search preliminary collections on CMR\ncmr_token = CMR_auth.get_token()\n# The query object uses a simple python dictionary\nquery = {'short_name':'ATL03',\n         'token': cmr_token,\n         'provider': NSIDC_PROVIDERS['AWS_HOSTED']}\n\ncollections = cmr_collection.search(query)\n\nfor collection in collections[0:3]:\n    wrapped_abstract = '\\n'.join(textwrap.wrap(f\"Abstract: {collection['umm']['Abstract']}\", 80)) + '\\n'\n    print(f\"concept-id: {collection['meta']['concept-id']}\\n\" +\n          f\"Title: {collection['umm']['EntryTitle']}\\n\" +\n          wrapped_abstract)"
  },
  {
    "objectID": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#searching-for-data-granules-in-the-cloud-with-cmr",
    "href": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#searching-for-data-granules-in-the-cloud-with-cmr",
    "title": "",
    "section": "Searching for data granules in the cloud with CMR",
    "text": "CMR uses different collection id’s for datasets in the cloud.\n# now that we have the concept-id for our ATL03 in the cloud we do the same thing we did with ATL03 hosted at\nfrom cmr_serializer import QueryResult\n# NSIDC but using the cloud concept-id\n# Jeneau ice sheet\nquery = {'concept-id': 'C2027878642-NSIDC_CPRD',\n         'token': cmr_token,\n         'bounding_box': '-135.1977,58.3325,-133.3410,58.9839'}\n\n# Querying for ATL03 v3 using its concept-id and a bounding box\nresults = granule.search(query, limit=1000)\ngranules = QueryResult(results).items()\n\nprint(f\"Total granules found: {len(results)} \\n\")\n\n# Print the first 3 granules\nfor g in granules[0:3]:\n    display(g)\n    # You can use: print(g) for the regular text representation.\n# We can list the s3 links but \nfor g in granules:\n    for link in g.data_links():\n        print(link)\nWe note that our RelatedLinks array now contain links to AWS S3, these are the direct URIs for our data granules in the AWS us-west-2 region."
  },
  {
    "objectID": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#data-access-using-aws-s3",
    "href": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#data-access-using-aws-s3",
    "title": "",
    "section": "Data Access using AWS S3",
    "text": "IMPORTANT: This section will only work if this notebook is running on the AWS us-west-2 zone\n\nThere is more than one way of accessing data on AWS S3, either downloading it to your local machine using the official client library or using a python library.\nPerformance tip: using the HTTPS URLs will decrease the access performance since these links have to internally be processed by AWS’s content delivery system (CloudFront). To get a better performance we should access the S3:// URLs with BOTO3 or a high level S3 enabled library (i.e. S3FS)\nRelated links: * HDF in the Cloud challenges and solutions for scientific data * Cloud Storage (Amazon S3) HDF5 Connector\nimport s3fs\nimport h5py\n\nimport xarray as xr\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n# READ only temporary credentials\n# This credentials only last 1 hour.\ns3_cred = CMR_auth.get_s3_credentials()\n\ns3_fs = s3fs.S3FileSystem(key=s3_cred['accessKeyId'],\n                          secret=s3_cred['secretAccessKey'],\n                          token=s3_cred['sessionToken'])\n\n# Now you could grab S3 links to your cloud instance (EC2, Hub etc) using:\n# s3_fs.get('s3://SOME_LOCATION/ATL03_20181015124359_02580106_004_01.h5', 'test.h5')\nNow that we have the propper credentials in our file mapper, we can access the data within AWS us-west-2.\nIf we are not running this notebook in us-west-2 will get an access denied error"
  },
  {
    "objectID": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#using-xarray-to-open-files-on-s3",
    "href": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#using-xarray-to-open-files-on-s3",
    "title": "",
    "section": "Using xarray to open files on S3",
    "text": "ATL data is complex so xarray doesn’t know how to extract the important bits out of it.\nwith s3_fs.open('s3://nsidc-cumulus-prod-protected/ATLAS/ATL03/004/2018/10/15/ATL03_20181015124359_02580106_004_01.h5', 'rb') as s3f:\n    with h5py.File(s3f, 'r') as f:\n        print([key for key in f.keys()])\n        gt1l = xr.Dataset({'height': (['x'], f['gt1l']['heights']['h_ph'][:]),\n                       'latitude': (['x'], f['gt1l']['heights']['lat_ph'][:]), \n                       'longitude': (['x'], f['gt1l']['heights']['lon_ph'][:]),\n                       'dist_ph': (['x'], f['gt1l']['heights']['dist_ph_along'][:])})\ngt1l"
  },
  {
    "objectID": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#plotting-the-data",
    "href": "examples/NSIDC/ICESat2-CMR-AWS-S3.html#plotting-the-data",
    "title": "",
    "section": "Plotting the data",
    "text": "gt1l.height.plot()\n%matplotlib widget\n\nfig, ax = plt.subplots(figsize=(14, 4))\n\ngt1l.height.plot(ax=ax, ls='', marker='o', ms=1)\ngt1l.height.rolling(x=1000, min_periods=500, center=True).mean().plot(ax=ax, c='k', lw=2)\nax.set_xlabel('Along track distance (m)', fontsize=12);\nax.set_ylabel('Photon Height (m)', fontsize=12)\nax.set_title('ICESat-2 ATL03', fontsize=14)\nax.tick_params(axis='both', which='major', labelsize=12)\n\nsubax = fig.add_axes([0.69,0.50,0.3,0.3], projection=ccrs.NorthPolarStereo())\nsubax.set_aspect('equal')\nsubax.set_extent([-180., 180., 30, 90.], ccrs.PlateCarree())\nsubax.add_feature(cfeature.LAND)\nsubax.plot(gt1l.longitude, gt1l.latitude, transform=ccrs.PlateCarree(), lw=1);\n\nfig.savefig('test.png')\n\nplt.show()"
  },
  {
    "objectID": "get-started/earthdata-login.html#background",
    "href": "get-started/earthdata-login.html#background",
    "title": "EarthData Login",
    "section": "Background",
    "text": "An Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need EarthData Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\nTo avoid being prompted for credentials every time you run and also allow clients such as curl to log in, you can create and/or add the following to a “netrc” file (pronounced “Net RC”). On a Mac, this is a file called .netrc, and on Windows it is _netrc. (There are no extensions on either file).\nBelow are ways to create a “netrc” file in your home directory."
  },
  {
    "objectID": "get-started/earthdata-login.html#r-for-windows-or-macos",
    "href": "get-started/earthdata-login.html#r-for-windows-or-macos",
    "title": "EarthData Login",
    "section": "R for Windows or macOS",
    "text": "Create a “netrc” file by running the following code in your R console (or after saving to an R script).\n\n\nShow the code\n# Required packages for this script\npackages = c('sys', 'getPass')\n\n# Identify missing (not installed) packages\nnew.packages = packages[!(packages %in% installed.packages()[,\"Package\"])]\n\n# Install missing packages\nif(length(new.packages)) install.packages(new.packages, repos='http://cran.rstudio.com/')\n\n# Load packages into R\nlibrary(sys)\nlibrary(getPass)\n\n# Determine OS and associated netrc file \nnetrc_type <- if(.Platform$OS.type == \"windows\") \"_netrc\" else \".netrc\"    # Windows OS uses _netrc file\n\n# Specify path to user profile \nup <- file.path(Sys.getenv(\"USERPROFILE\"))                            # Retrieve user directory (for netrc file)\n\n# Below, HOME and Userprofile directories are set.  \n\nif (up == \"\") {\n    up <- Sys.getenv(\"HOME\") \n    Sys.setenv(\"userprofile\" = up)\n    if (up == \"\") {\n        cat('USERPROFILE/HOME directories need to be set up. Please type sys.setenv(\"HOME\" = \"YOURDIRECTORY\") or  sys.setenv(\"USERPROFILE\" = \"YOURDIRECTORY\") in your console and type your USERPROFILE/HOME direcory instead of \"YOURDIRECTORY\". Next, run the code chunk again.')\n    }\n} else {Sys.setenv(\"HOME\" = up)}        \n\nnetrc_path <- file.path(up, netrc_type, fsep = .Platform$file.sep)    # Path to netrc file\n\n# Create a netrc file if one does not exist already\nif (file.exists(netrc_path) == FALSE || grepl(\"urs.earthdata.nasa.gov\", readLines(netrc_path)) == FALSE) {\n    netrc_conn <- file(netrc_path)\n    \n    # User will be prompted for NASA Earthdata Login Username and Password below\n    writeLines(c(\"machine urs.earthdata.nasa.gov\",\n                 sprintf(\"login %s\", getPass(msg = \"Enter NASA Earthdata Login Username \\n (An account can be Created at urs.earthdata.nasa.gov):\")),\n                 sprintf(\"password %s\", getPass(msg = \"Enter NASA Earthdata Login Password:\"))), netrc_conn)\n    close(netrc_conn)\n}"
  },
  {
    "objectID": "get-started/earthdata-login.html#python-for-windows-or-macos",
    "href": "get-started/earthdata-login.html#python-for-windows-or-macos",
    "title": "EarthData Login",
    "section": "Python for Windows or macOS",
    "text": "Create a “netrc” file by running the following code in your python console (or after saving to an .py script).\nLP DAAC Approach:\n\n\nShow the code\n# Load necessary packages into Python\nfrom netrc import netrc\nfrom subprocess import Popen\nfrom getpass import getpass\nimport os\n\n# -----------------------------------AUTHENTICATION CONFIGURATION-------------------------------- #\nurs = 'urs.earthdata.nasa.gov'    # Earthdata URL to call for authentication\nprompts = ['Enter NASA Earthdata Login Username \\n(or create an account at urs.earthdata.nasa.gov): ',\n           'Enter NASA Earthdata Login Password: ']\n\n# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\ntry:\n    netrcDir = os.path.expanduser(\"~/.netrc\")\n    netrc(netrcDir).authenticators(urs)[0]\n\n# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\nexcept FileNotFoundError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('touch {0}.netrc | chmod og-rw {0}.netrc | echo machine {1} >> {0}.netrc'.format(homeDir + os.sep, urs), shell=True)\n    Popen('echo login {} >> {}.netrc'.format(getpass(prompt=prompts[0]), homeDir + os.sep), shell=True)\n    Popen('echo password {} >> {}.netrc'.format(getpass(prompt=prompts[1]), homeDir + os.sep), shell=True)\n\n# Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\nexcept TypeError:\n    homeDir = os.path.expanduser(\"~\")\n    Popen('echo machine {1} >> {0}.netrc'.format(homeDir + os.sep, urs), shell=True)\n    Popen('echo login {} >> {}.netrc'.format(getpass(prompt=prompts[0]), homeDir + os.sep), shell=True)\n    Popen('echo password {} >> {}.netrc'.format(getpass(prompt=prompts[1]), homeDir + os.sep), shell=True)\n\n\nPO.DAAC Approach:\nJulie note to Catalina/Jack: I wasn't sure if this would be best here or in the search-by-shapefile.qmd: \n\nThe setup_earthdata_login_auth function will allow Python scripts to log into any Earthdata Login application programmatically. To avoid being prompted for credentials every time you run and also allow clients such as curl to log in, you can add the following to a .netrc (_netrc on Windows) file in your home directory:\n\nmachine urs.earthdata.nasa.gov\n    login <your username>\n    password <your password>\nMake sure that this file is only readable by the current user or you will receive an error stating \"netrc access too permissive.\"\n\n$ chmod 0600 ~/.netrc\n\n\nShow the code\nfrom netrc import netrc\nfrom platform import system\nfrom getpass import getpass\nfrom http.cookiejar import CookieJar\nfrom os.path import join, expanduser\n\nTOKEN_DATA = (\"<token>\"\n              \"<username>%s</username>\"\n              \"<password>%s</password>\"\n              \"<client_id>PODAAC CMR Client</client_id>\"\n              \"<user_ip_address>%s</user_ip_address>\"\n              \"</token>\")\n\n\ndef setup_cmr_token_auth(endpoint: str='cmr.earthdata.nasa.gov'):\n    ip = requests.get(\"https://ipinfo.io/ip\").text.strip()\n    return requests.post(\n        url=\"https://%s/legacy-services/rest/tokens\" % endpoint,\n        data=TOKEN_DATA % (input(\"Username: \"), getpass(\"Password: \"), ip),\n        headers={'Content-Type': 'application/xml', 'Accept': 'application/json'}\n    ).json()['token']['id']\n\n\ndef setup_earthdata_login_auth(endpoint: str='urs.earthdata.nasa.gov'):\n    netrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n    try:\n        username, _, password = netrc(file=join(expanduser('~'), netrc_name)).authenticators(endpoint)\n    except (FileNotFoundError, TypeError):\n        print('Please provide your Earthdata Login credentials for access.')\n        print('Your info will only be passed to %s and will not be exposed in Jupyter.' % (endpoint))\n        username = input('Username: ')\n        password = getpass('Password: ')\n    manager = request.HTTPPasswordMgrWithDefaultRealm()\n    manager.add_password(None, endpoint, username, password)\n    auth = request.HTTPBasicAuthHandler(manager)\n    jar = CookieJar()\n    processor = request.HTTPCookieProcessor(jar)\n    opener = request.build_opener(auth, processor)\n    request.install_opener(opener)\n\n\n# Get your authentication token for searching restricted records in the CMR:\n_token = setup_cmr_token_auth(endpoint=\"cmr.earthdata.nasa.gov\")\n\n# Start authenticated session with URS to allow restricted data downloads:\nsetup_earthdata_login_auth(endpoint=\"urs.earthdata.nasa.gov\")"
  },
  {
    "objectID": "get-started/earthdata-login.html#common-questions",
    "href": "get-started/earthdata-login.html#common-questions",
    "title": "EarthData Login",
    "section": "Common questions",
    "text": "How do I know if I already have a netrc file?\nYour netrc file will likely be in your root directory. It is a hidden file that you will not be able to see from your Finder (Mac) or Windows Explorer (Windows), so you’ll have to do this from the Command Line. Navigate to your root directory and list all:\n\nOn a Mac:\ncd ~\nls -la\nIf you see a .netrc file, view what’s inside (perhaps with nano), and if you’d like to delete the current version to start afresh, type rm .netrc."
  },
  {
    "objectID": "get-started/lpdaac-netrc.html#this-is-otherwise-just-testing",
    "href": "get-started/lpdaac-netrc.html#this-is-otherwise-just-testing",
    "title": "LP DAAC Authentication Example",
    "section": "This is otherwise just testing ",
    "text": "Import the required packages and set the input/working directory to run this Jupyter Notebook locally.\nimport requests as r\nfrom skimage import io\nimport matplotlib.pyplot as plt\n\nprint(1+1)\nprint(\"woo NASA data!\")\n\n2\nwoo NASA data!"
  },
  {
    "objectID": "get-started/podaac-ecco-netrc.html",
    "href": "get-started/podaac-ecco-netrc.html",
    "title": "PO DAAC Authentication Example",
    "section": "",
    "text": "Note this is currently copied from https://github.com/podaac/ECCO/blob/main/Data_Access/local_bulk_download_and_open_in_py3.ipynb as a starting point and will be refined\n\n\n\n\nConfigure your .netrc file\nGood idea to back up your existing netrc file, if you have one. And while youre at it check for these entries because they might exist in there already:\n\n%cp ~/.netrc ~/bak.netrc\n\n%cat ~/.netrc | grep '.earthdata.nasa.gov' | cut -f-5 -d\" \"\n\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\n\n\n\nAdd entries to your netrc for these two earthdata.nasa.gov sub domains, at a minimum:\nmachine urs.earthdata.nasa.gov login jmcnelis password ***\nmachine opendap.earthdata.nasa.gov login jmcnelis password ***\nand replace jmcnelis and *** with your Earthdata Login username and password, respectively…\n\nReplace jmcnelis and *** with your Earthdata username and password, and then run the cell to append these two lines to your netrc file, if one exists. Otherwise write them to a new one. (all set up by -a)\n\n%%file -a ~/.netrc\nmachine urs.earthdata.nasa.gov login jmcnelis password ***\nmachine opendap.earthdata.nasa.gov login jmcnelis password ***\n\nAppending to /Users/lowndes/.netrc\n\n\nDump the netrc again sans passwords to confirm that it worked:\n\n!cat ~/.netrc | grep '.earthdata.nasa.gov' | cut -f-5 -d\" \"\n\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\nmachine urs.earthdata.nasa.gov login jmcnelis password\nmachine opendap.earthdata.nasa.gov login jmcnelis password\n\n\nFinally, you need to make sure to limit access to the netrc file because it stores your plain text password. Simple on MacOS and Linux:\n!chmod 0600 ~/.netrc"
  },
  {
    "objectID": "get-started/index.html#prerequisites-and-related-resources",
    "href": "get-started/index.html#prerequisites-and-related-resources",
    "title": "Getting Started",
    "section": "Prerequisites and related resources",
    "text": "For programmatic access to NASA data on the cloud, we recommend familiarity with the following:\nBasic Programming in R and Python:\n\nIntroduction to Open Data Science with R - Lowndes & Horst, RStudio Conference 2020\nIntroduction to Geospatial Raster and Vector Data with Python - Carpentries Incubator\n\nUsing both the Scientific Python Ecosystem and Cloud Computing together:\n\nProject Pythia Foundations Book\nJupyter meets Earth"
  },
  {
    "objectID": "get-started/index.html#cloud-optimized-data-formats",
    "href": "get-started/index.html#cloud-optimized-data-formats",
    "title": "Getting Started",
    "section": "Cloud Optimized Data Formats",
    "text": "Some great info here about Cloud Optimized Data Formats.\n\nCloud-Performant NetCDF4/HDF5 Reading with the Zarr Library"
  },
  {
    "objectID": "get-started/index.html#sw3-buckets-etc",
    "href": "get-started/index.html#sw3-buckets-etc",
    "title": "Getting Started",
    "section": "SW3 Buckets, etc",
    "text": "NASA AWS Cloud Primer"
  },
  {
    "objectID": "get-started/how-to-use.html",
    "href": "get-started/how-to-use.html",
    "title": "How to use this book",
    "section": "",
    "text": "Draft prose here by Andy Barrett.\nIncludes both building blocks and end-to-end workflow examples that put together some of those building blocks. User can ‘choose their own adventure’ and build their own workflows based on their need."
  },
  {
    "objectID": "get-started/authentication.html",
    "href": "get-started/authentication.html",
    "title": "Authentication",
    "section": "",
    "text": "There are multiple ways to authenticate. Options are:…\nThe following are example notebooks (.ipynb)…"
  },
  {
    "objectID": "get-started/api-primer.html",
    "href": "get-started/api-primer.html",
    "title": "API Primer",
    "section": "",
    "text": "The code-based chapters of this Cookbook utilize several NASA EOSDIS-supported Application Programming Interfaces, or APIs, to discover and access data programmatically.\n\n\n\n\n\n\nNote\n\n\n\nWhat is an API? You can think of an API as a middle man between an application or end-use and a data provider. In most cases the data provider is either the NASA Common Metadata Repository (CMR), which houses data information, or one or more of the NASA DAACs as the data distributors. These APIs are generally structured as a root URL, plus individual key-value-pairs separated by ‘&’.\n\n\nProgrammatic API access is beneficial for those who want to incorporate data access into a visualization and analysis workflow. This method is also reproducible and documented to ensure data provenance.\nThe table below describes each of the common NASA Earthdata APIs along with their functions and use case examples. Documentation for each API is linked in each row.\n\n\n\n\n\n\n\n\nAPI\nKey Functions\nUse Case Example\n\n\n\n\nCommon Metadata Repository (CMR)\nSearch for datasets and data files using several filtering options, as well as associated tools, services, and data variables.\nFor a given data set, I want to retrieve file information, including data access URLs, for all data occurring over my region and time period of interest.\n\n\nCMR-STAC:\n\nCMR STAC Catalog Documentation\nCMR STAC Catalog Endpoint\nCMR STAC Catalog Endpoint for Cloud-hosted Holdings Only\n\nSearch data information housed by the CMR as assets through the SpatioTemporal Asset Catalog specification (STAC), which provides a common language for interpreting geospatial information in order to standardize indexing and discovering data.\nIn addition to the CMR use case example above, I also want to use Python to directly load a dataset STAC asset into memory in order to view imagery and plot the data.\n\n\nGlobal Imagery Browse Service (GIBS)\nAccess full-resolution browse imagery using standards-based web services and formats.\nI want to access and browse near real-time satellite imagery in a programming language or GIS application.\n\n\nHarmony\nAccess and Customize data through subsetting, reformatting, and reprojection options.\nI want to receive subsetted data by time, area, and variables of interest in order to reduce data volume, as well as reformat to a cloud-optimized format to improve analysis performance in the cloud.\n\n\nOPeNDAP\nAccess and Customize data through subsetting and reformatting options.\nI want to access data files on-the-fly, subsetted by area or time dimensions."
  },
  {
    "objectID": "transformations/netcdf-to-zarr.html",
    "href": "transformations/netcdf-to-zarr.html",
    "title": "netCDF to Zarr",
    "section": "",
    "text": "Intro to the following notebooks:\n\nHarmony dev team basic notebooks (in UAT, needs updating; including dataset it’s working on)\nECCO example"
  },
  {
    "objectID": "transformations/harmony-subsetting.html#introduction",
    "href": "transformations/harmony-subsetting.html#introduction",
    "title": "Harmony Subsetting",
    "section": "Introduction",
    "text": "The Jupyter Notebook is an interactive computing environment that enables users to author notebook documents that include: - Live code - Interactive widgets - Plots - Narrative text - Equations - Images - Video\nThese documents provide a complete and self-contained record of a computation that can be converted to various formats and shared with others using email, Dropbox, version control systems (like git/GitHub) or nbviewer.jupyter.org.\n\nComponents\nThe Jupyter Notebook combines three components:\n\nThe notebook web application: An interactive web application for writing and running code interactively and authoring notebook documents.\nKernels: Separate processes started by the notebook web application that runs users’ code in a given language and returns output back to the notebook web application. The kernel also handles things like computations for interactive widgets, tab completion and introspection.\nNotebook documents: Self-contained documents that contain a representation of all content visible in the notebook web application, including inputs and outputs of the computations, narrative text, equations, images, and rich media representations of objects. Each notebook document has its own kernel."
  },
  {
    "objectID": "transformations/harmony-subsetting.html#notebook-web-application",
    "href": "transformations/harmony-subsetting.html#notebook-web-application",
    "title": "Harmony Subsetting",
    "section": "Notebook web application",
    "text": "The notebook web application enables users to:\n\nEdit code in the browser, with automatic syntax highlighting, indentation, and tab completion/introspection.\nRun code from the browser, with the results of computations attached to the code which generated them.\nSee the results of computations with rich media representations, such as HTML, LaTeX, PNG, SVG, PDF, etc.\nCreate and use interactive JavaScript widgets, which bind interactive user interface controls and visualizations to reactive kernel side computations.\nAuthor narrative text using the Markdown markup language.\nInclude mathematical equations using LaTeX syntax in Markdown, which are rendered in-browser by MathJax."
  },
  {
    "objectID": "transformations/harmony-subsetting.html#kernels",
    "href": "transformations/harmony-subsetting.html#kernels",
    "title": "Harmony Subsetting",
    "section": "Kernels",
    "text": "Through Jupyter’s kernel and messaging architecture, the Notebook allows code to be run in a range of different programming languages. For each notebook document that a user opens, the web application starts a kernel that runs the code for that notebook. Each kernel is capable of running code in a single programming language and there are kernels available in the following languages:\n\nPython(https://github.com/ipython/ipython)\nJulia (https://github.com/JuliaLang/IJulia.jl)\nR (https://github.com/IRkernel/IRkernel)\nRuby (https://github.com/minrk/iruby)\nHaskell (https://github.com/gibiansky/IHaskell)\nScala (https://github.com/Bridgewater/scala-notebook)\nnode.js (https://gist.github.com/Carreau/4279371)\nGo (https://github.com/takluyver/igo)\n\nThe default kernel runs Python code. The notebook provides a simple way for users to pick which of these kernels is used for a given notebook.\nEach of these kernels communicate with the notebook web application and web browser using a JSON over ZeroMQ/WebSockets message protocol that is described here. Most users don’t need to know about these details, but it helps to understand that “kernels run code.”"
  },
  {
    "objectID": "transformations/harmony-subsetting.html#notebook-documents",
    "href": "transformations/harmony-subsetting.html#notebook-documents",
    "title": "Harmony Subsetting",
    "section": "Notebook documents",
    "text": "Notebook documents contain the inputs and outputs of an interactive session as well as narrative text that accompanies the code but is not meant for execution. Rich output generated by running code, including HTML, images, video, and plots, is embeddeed in the notebook, which makes it a complete and self-contained record of a computation.\nWhen you run the notebook web application on your computer, notebook documents are just files on your local filesystem with a .ipynb extension. This allows you to use familiar workflows for organizing your notebooks into folders and sharing them with others.\nNotebooks consist of a linear sequence of cells. There are three basic cell types:\n\nCode cells: Input and output of live code that is run in the kernel\nMarkdown cells: Narrative text with embedded LaTeX equations\nRaw cells: Unformatted text that is included, without modification, when notebooks are converted to different formats using nbconvert\n\nInternally, notebook documents are JSON data with binary values base64 encoded. This allows them to be read and manipulated programmatically by any programming language. Because JSON is a text format, notebook documents are version control friendly.\nNotebooks can be exported to different static formats including HTML, reStructeredText, LaTeX, PDF, and slide shows (reveal.js) using Jupyter’s nbconvert utility.\nFurthermore, any notebook document available from a public URL or on GitHub can be shared via nbviewer. This service loads the notebook document from the URL and renders it as a static web page. The resulting web page may thus be shared with others without their needing to install the Jupyter Notebook.\n\n\n[1] TRUE"
  },
  {
    "objectID": "transformations/L2-subsetter.html",
    "href": "transformations/L2-subsetter.html",
    "title": "L2 Subsetter",
    "section": "",
    "text": "Intro to the following notebooks:\n\nBasic harmony subsetting notebook in podaac git\nL2 data cube - more of an end-user example using the service"
  },
  {
    "objectID": "transformations/index.html",
    "href": "transformations/index.html",
    "title": "NASA Cloud Data Transformations",
    "section": "",
    "text": "text"
  },
  {
    "objectID": "transformations/HarmonyPy_intro_tutorial.html#harmonypy-introduction",
    "href": "transformations/HarmonyPy_intro_tutorial.html#harmonypy-introduction",
    "title": "",
    "section": "HarmonyPy Introduction",
    "text": "This notebook demonstrates several basic examples highlighting how to query and access customized data outputs from NASA Earthdata Harmony. See https://harmony-py.readthedocs.io/en/latest/ for detailed documentation on HarmonyPy.\n\nImport packages\nFirst we import packages needed to request and visualize our data, as well as the harmony-py library itself. Make sure to install harmony-py and its dependencies into your current Python environment prior to executing the notebook:\n$ pip install -U harmony-py \n\nYou can install this and any other necessary libraries directly into your Jupyter notebook Python kernel as well:\n!{sys.executable} -m pip install -U harmony-py\n# !{sys.executable} -m pip install -U harmony-py #install harmony-py into Python kernel\nimport sys; sys.path.append('..')\nimport datetime as dt\nfrom IPython.display import display, JSON\nimport rasterio\nimport rasterio.plot\n\nfrom harmony import BBox, Client, Collection, Request\nfrom harmony.config import Environment\n\n\nQuick start\nYou can request and download data using harmony-py in just a few easy lines. Although more advanced subsetting and transformation options may be supported on your data product of interest, this example below demonstrates a basic spatial bounding box and temporal range request, using the direct download method:\nharmony_client = Client(auth=('EDL_username', 'EDL_password'))\nrequest = Request(\n    collection=Collection(id=dataset_short_name),\n    spatial=BBox(w, s, e, n),\n    temporal={\n        'start': dt.datetime(yyyy, mm, dd),\n        'stop': dt.datetime(yyyy, mm, dd)\n    }\n)\njob_id = harmony_client.submit(request)\nresults = harmony_client.download_all(job_id, directory='/tmp', overwrite=True)\nThe guidance below offers more detailed examples highlighting many of the helpful features provided by the Harmony Py library, including direct s3 access from Earthdata Cloud-hosted data if running in the AWS us-west-2 region.\n\n\nCreate Harmony Client object\nFirst, you will need to create your Harmony Client, which is what you will interact with to submit and inspect a data request to Harmony, as well as to retrieve your results.\nWhen creating the Client, you need to provide your Earthdata Login credentials, which are required to access data from NASA EOSDIS. There are three options for providing your Earthdata Login username and password:\n\nProvide your username and password directly when creating the client:\n\nharmony_client = Client(auth=('captainmarvel', 'marve10u5'))\n\nSet your credentials using environment variables:\n\nYou can either export these directly:\n$ export EDL_USERNAME='captainmarvel'\n$ export EDL_PASSWORD='marve10u5'\nOr by storing them in a .env file, which operates in a similar fashion to .netrc. You will need to store the file in your current working directory and it must be named .env with the following format:\nEDL_USERNAME=myusername\nEDL_PASSWORD=mypass\n\nUse a .netrc file:\n\nCreate a .netrc file in your home directory, using the example below:\nmachine urs.earthdata.nasa.gov\nlogin captainmarvel\npassword marve10u5\nharmony_client = Client() # assumes .netrc usage, option 3 above\n\n\nCreate Harmony Request\nThe following are common request parameters:\n\ncollection: Required parameter. This is the NASA EOSDIS collection, or data product. There are two options for inputting a collection of interest:\n\nProvide a concept ID, which is an ID provided in the Common Metadata Repository (CMR) metadata\nData product short name (e.g. SENTINEL-1_INTERFEROGRAMS).\n\nspatial: Bounding box spatial constraints on the data. The Harmony Bbox class accepts spatial coordinates as decimal degrees in w, s, e, n order, where longitude = -180, 180 and latitude = -90, 90.\n\nshape: Perform a shapefile subsetting request on a supported collection by passing the path to a GeoJSON file (*.json or .geojson), an ESRI Shapefile (.zip or .shz), or a kml file (.kml) as the “shape” parameter\ntemporal: Date/time constraints on the data. The example below demonstrates temporal start and end ranges using the python datetime library.\n\nOther advanced parameters that may be of interest. Note that many reformatting or advanced projection options may not be available for your requested dataset. See the documentation for details on how to construct these parameters.\n\ncrs: Reproject the output coverage to the given CRS. Recognizes CRS types that can be inferred by gdal, including EPSG codes, Proj4 strings, and OGC URLs (http://www.opengis.net/def/crs/…)\ninterpolation: specify the interpolation method used during reprojection and scaling\nscale_extent: scale the resulting coverage either among one axis to a given extent\nscale_size: scale the resulting coverage either among one axis to a given size\ngranule_id: The CMR Granule ID for the granule (file) which should be retrieved\nwidth: number of columns to return in the output coverage\nheight: number of rows to return in the output coverage\nformat: the output mime type to return\nmax_results: limits the number of input files processed in the request\n\nThe example utilized in this tutorial demonstrates a shapefile subset of the Big Island of Hawaii on February 24, 2020. A bounding box subset over the Mauna Kea and Mauna Loa volcanoes is also commented out below to show a similar subsetting option. The SENTINEL-1_INTERFEROGRAMS dataset, distributed by the ASF DAAC, is a prototype Level 2 NISAR-Format product. See https://asf.alaska.edu/data-sets/derived-data-sets/sentinel-1-interferograms/ for more information.\nThis request specifies a subset of the unwrappedPhase variable, in TIFF format, with a maximum file result capped at 2 for demonstration purposes.\n\nNote that a Sentinel-3 End-User License Agreement (EULA) is required to access these data.\n\n\nPlease go to https://grfn.asf.alaska.edu/door/download/S1-GUNW-D-R-021-tops-20201029_20191029-033636-28753N_27426N-PP-2dde-v2_0_3.nc to initiate a file download, which will first prompt you to accept the required EULA if you have not already done so. If you do not accept this EULA, you will receive an error when submitting your Harmony request.\nshapefile_path = 'Big_Island_0005.zip' \n\nrequest = Request(\n    collection=Collection(id='SENTINEL-1_INTERFEROGRAMS'),\n    #spatial=BBox(-155.75, 19.26, -155.3, 19.94), # bounding box example that can be used as an alternative to shapefile input\n    shape=shapefile_path,\n    temporal={\n        'start': dt.datetime(2020, 2, 24),\n        'stop': dt.datetime(2020, 2, 25),\n    },\n    variables=['science/grids/data/unwrappedPhase'],\n    format='image/tiff',\n    max_results=2,\n)\n\n\n\nCheck Request validity\nBefore submitting a Harmony Request, you can test your request to see if it’s valid and how to fix it if not. In particular, request.is_valid will check to ensure that your spatial BBox bounds and temporal ranges are entered correctly.\nprint(f\"Request valid? {request.is_valid()}\")\nfor m in request.error_messages():\n    print(\" * \" + m)\n\n\nSubmit Request\nOnce you have created your request, you can now submit it to Harmony using the Harmony Client object. A job id is returned, which is a unique identifier that represents your submitted request.\njob_id = harmony_client.submit(request)\njob_id\n\n\nCheck Request status\nYou can check on the progress of a processing job with status(). This method blocks while communicating with the server but returns quickly. The JSON helper function provides a nicer formatting option for viewing the status message.\nJSON(harmony_client.status(job_id))\nDepending on the size of the request, you may want to wait until the request has completed processing before the remainder of your code is executed. The wait_for_processing() method will block subsequent lines of code while optinally showing a progress bar.\nharmony_client.wait_for_processing(job_id, show_progress=True)\n\n\nView Harmony job response and output URLs\nOnce your data request has finished processing, you can view details on the job that was submitted to Harmony, including the API call to Harmony, and informational messages on your request if available.\nresult_json() calls wait_for_processing() and returns the complete job in JSON format once processing is complete. You may optionally show the progress bar using the same show_progress=True statement in the previous line.\ndata = harmony_client.result_json(job_id)\nJSON(data)\n\n\nDownload and visualize Harmony output files\nThe files returned from your request are returned as HTTPS URLs in all cases other than when requesting Zarr as an output format(s3 URLs are returned instead to support accessing and interacting with those outputs in the cloud). For all Harmony requests, direct s3 URLs are also provided within a STAC catalog provided in the job response as we’ll explore in more detail below.\n\nRetrieve list of output URLs\nThe result_urls() method calls wait_for_processing() and returns a list of the processed data URLs once processing is complete. You may optionally show the progress bar as shown below.\nurls = harmony_client.result_urls(job_id, show_progress=True)\nurls\n\n\nDownload files\nThe next code block utilizes download_all() to download all data output file URLs. This is a non-blocking step during the download itself, but this line will block subsequent code while waiting for the job to finish processing. You can optionally specify a directory and specify whether to overwrite existing files as shown below.\nYou may call result() on future objects (those that are awaiting processing) to realize them. A call to result() blocks until that particular future object finishes downloading. Other future objects will download in the background, in parallel. When downloading is complete, the future objects will return the file path string of the file that was just downloaded. This file path can then be fed into other libraries that may read the data files and perform other operations.\nresults = harmony_client.download_all(job_id, directory='/tmp', overwrite=True)\nfile_names = [f.result() for f in results]\nfile_names\nWith download(), this will download only the URL specified, in case you would like more control over individual files.\nfile_name = harmony_client.download(urls[0], overwrite=True).result()\nfile_name\n\n\n\nVisualize Downloaded Outputs\nThe output image files can be visualized using the Rasterio library.\nfor r in results:\n    rasterio.plot.show(rasterio.open(r.result()))\n\n\nExplore output STAC catalog and retrieve results from s3\nA STAC catalog is returned in each Harmony request result. The stac items include not only the s3 locations for each output file, but also the spatial and temporal metadata representing each subsetted output. stac_catalog_url will return the URL of the catalog, and can also accept an optional progress status if desired.\nstac_catalog_url = harmony_client.stac_catalog_url(job_id)\nstac_catalog_url\n\nUsing PySTAC:\nFollowing the directions for PySTAC (https://pystac.readthedocs.io/en/latest/quickstart.html), we can hook our harmony-py client into STAC_IO.\nfrom urllib.parse import urlparse\nimport requests\nfrom pystac import STAC_IO\n\ndef requests_read_method(uri):\n    parsed = urlparse(uri)\n    if parsed.hostname.startswith('harmony.'):\n        return harmony_client.read_text(uri)\n    else:\n        return STAC_IO.default_read_text_method(uri)\n\nSTAC_IO.read_text_method = requests_read_method\nView the timestamp and s3 locations of each STAC item:\nfrom pystac import Catalog\n\ncat = Catalog.from_file(stac_catalog_url)\n\nprint(cat.title)\ns3_links = []\nfor item in cat.get_all_items():\n    print(item.datetime, [asset.href for asset in item.assets.values()])\n    s3_links.append([asset.href for asset in item.assets.values()])\n\n\nUsing intake-stac:\nView each item value returned:\nimport intake\ncat = intake.open_stac_catalog(stac_catalog_url)\ndisplay(list(cat))\nAnd the metadata contents of each item:\nentries = []\nfor id, entry in cat.search('type').items():\n    display(entry)\n    entries.append(entry)\n\n\n\nCloud in-place access\nNote that the remainder of this tutorial will only succeed when running this notebook within the AWS us-west-2 region.\nHarmony data outputs can be accessed within the cloud using the s3 URLs and AWS credentials provided in the Harmony job response. Below are examples using both intake-stac or boto3 to access the data in the cloud.\n\nAWS credential retrieval\nUsing aws_credentials you can retrieve the credentials needed to access the Harmony s3 staging bucket and its contents.\ncreds = harmony_client.aws_credentials()\ncreds\n\n\nUsing boto3\n#\n# NOTE: Execution of this cell will only succeed within the AWS us-west-2 region. \n#\n\nimport boto3\n\ns3 = boto3.client('s3', **creds)\nfor i in range(len(s3_links)):\n    uri = s3_links[i][0]\n    bucket = uri.split('/')[2]\n    obj = '/'.join(uri.split('/')[3:])\n    fn = uri.split('/')[-1]\n    with open(fn, 'wb') as f:\n        s3.download_fileobj(bucket, obj, f)\n\n\nUsing intake-stac\nOnce again, you can use intake-stac to directly access each output from Harmony in AWS. Viewing the file structure and plotting the image can be done in a few simple lines when working with the data in-region:\n#\n# NOTE: Execution of this cell will only succeed within the AWS us-west-2 region. \n#\n\nfor i in range(len(list(cat))):\n    da = cat[list(cat)[i]][entries[i].describe()['name']].to_dask()\n    display(da)\n    da.plot()"
  },
  {
    "objectID": "transformations/earthdata-search.html",
    "href": "transformations/earthdata-search.html",
    "title": "Earthdata Search UI",
    "section": "",
    "text": "text"
  },
  {
    "objectID": "transformations/opendap-cloud.html",
    "href": "transformations/opendap-cloud.html",
    "title": "OPeNDAP in the Cloud",
    "section": "",
    "text": "OPeNDAP in the Cloud (OPeNDAP Hydrax)"
  },
  {
    "objectID": "transformations/harmonypy.html",
    "href": "transformations/harmonypy.html",
    "title": "Harmony Py Library",
    "section": "",
    "text": "Amy - should we include discussion/resource on using the HarmonyPy library?\ntest test"
  },
  {
    "objectID": "transformations/harmony-api.html",
    "href": "transformations/harmony-api.html",
    "title": "Harmony API",
    "section": "",
    "text": "text"
  },
  {
    "objectID": "processing/processing.html",
    "href": "processing/processing.html",
    "title": "Processing",
    "section": "",
    "text": "Processing details here"
  },
  {
    "objectID": "external/zarr-eosdis-store.html",
    "href": "external/zarr-eosdis-store.html",
    "title": "",
    "section": "",
    "text": "Zarr Example\nimported on: 2021-11-09\n\nThis notebook is from NASA’s Zarr EOSDIS store notebook\n\n\nThe original source for this document is https://github.com/nasa/zarr-eosdis-store\n\n\n\nzarr-eosdis-store example\nInstall dependencies\nimport sys\n\n# zarr and zarr-eosdis-store, the main libraries being demoed\n!{sys.executable} -m pip install zarr zarr-eosdis-store\n\n# Notebook-specific libraries\n!{sys.executable} -m pip install matplotlib\nImportant: To run this, you must first create an Earthdata Login account (https://urs.earthdata.nasa.gov) and place your credentials in ~/.netrc e.g.:\n   machine urs.earthdata.nasa.gov login YOUR_USER password YOUR_PASSWORD\nNever share or commit your password / .netrc file!\nBasic usage. After these lines, we work with ds as though it were a normal Zarr dataset\nimport zarr\nfrom eosdis_store import EosdisStore\n\nurl = 'https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/MUR-JPL-L4-GLOB-v4.1/20210715090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc'\n\nds = zarr.open(EosdisStore(url))\nView the file’s variable structure\n\nprint(ds.tree())\n\n/\n ├── analysed_sst (1, 17999, 36000) int16\n ├── analysis_error (1, 17999, 36000) int16\n ├── dt_1km_data (1, 17999, 36000) int16\n ├── lat (17999,) float32\n ├── lon (36000,) float32\n ├── mask (1, 17999, 36000) int16\n ├── sea_ice_fraction (1, 17999, 36000) int16\n ├── sst_anomaly (1, 17999, 36000) int16\n └── time (1,) int32\n\n\nFetch the latitude and longitude arrays and determine start and end indices for our area of interest. In this case, we’re looking at the Great Lakes, which have a nice, recognizeable shape. Latitudes 41 to 49, longitudes -93 to 76.\nlats = ds['lat'][:]\nlons = ds['lon'][:]\nlat_range = slice(lats.searchsorted(41), lats.searchsorted(49))\nlon_range = slice(lons.searchsorted(-93), lons.searchsorted(-76))\nGet the analysed sea surface temperature variable over our area of interest and apply scale factor and offset from the file metadata. In a future release, scale factor and add offset will be automatically applied.\nvar = ds['analysed_sst']\nanalysed_sst = var[0, lat_range, lon_range] * var.attrs['scale_factor'] + var.attrs['add_offset']\nDraw a pretty picture\n\nfrom matplotlib import pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = [16, 8]\nplt.imshow(analysed_sst[::-1, :])\nNone\n\n\n\n\nIn a dozen lines of code and a few seconds, we have managed to fetch and visualize the 3.2 megabyte we needed from a 732 megabyte file using the original archive URL and no processing services"
  },
  {
    "objectID": "external/cof-zarr-reformat.html#getting-started",
    "href": "external/cof-zarr-reformat.html#getting-started",
    "title": "",
    "section": "Getting Started",
    "text": "We will access monthly ocean bottom pressure (OBP) data from ECCO V4r4 (10.5067/ECG5M-OBP44), which are provided as a monthly time series on a 0.5-degree latitude/longitude grid.\nThe data are archived in netCDF format. However, this notebook demonstration will request conversion to Zarr format for files covering the period between 2010 and 2018. Upon receiving our request, Harmony’s backend will convert the files and stage them in S3 for native access in AWS (us-west-2 region, specifically). We will access the new Zarr datasets as an aggregated dataset using xarray, and leverage the S3 native protocols for direct access to the data in an efficient manner.\n\n\nRequirements\n\nAWS\nThis notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\nThe notebook was developed and tested using a t2.large instance (2 cpus; 8GB memory).\n\n\nPython 3\nMost of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n\ns3fs\nrequests\npandas\nxarray\nmatplotlib\n\n\n\n\nRequirements\nimport matplotlib.pyplot as plt\nimport xarray as xr\nimport pandas as pd\nimport numpy as np\nimport requests\nimport json\nimport time\nimport s3fs\n\nShortName = \"ECCO_L4_OBP_05DEG_MONTHLY_V4R4\"\n\n\nStudy period\nSet some “master” inputs to define the time and place contexts for our case studies in the ipynb. This example will be requesting time subsets and receiving global data back from the Harmony API.\nstart_date = \"2010-01-01\"\nend_date   = \"2018-12-31\"\n\n\nData Access\nSome features in the Harmony API require us to identify the target dataset/collection by its concept-id (which uniquely idenfifies it among the other datasets in the Common Metadata Repository). Support for selection by the dataset ShortName will be added in a future release.\n\nCommon Metadata Repository (CMR)\nFor now, we will need to get the concept-id that corresponds to our dataset by accessing its metadata from the CMR. Read more about the CMR at: https://cmr.earthdata.nasa.gov/\nRequest the UMM Collection metadata (i.e. metadata about the dataset) from the CMR and select the concept-id as a new variable ccid.\n\nresponse = requests.get(\n    url='https://cmr.earthdata.nasa.gov/search/collections.umm_json', \n    params={'provider': \"POCLOUD\",\n            'ShortName': ShortName,\n            'page_size': 1}\n)\n\nummc = response.json()['items'][0]\n\nccid = ummc['meta']['concept-id']\n\nccid\n\n'C1990404791-POCLOUD'\n\n\n\n\nHarmony API\nAnd get the Harmony API endpoint and zarr parameter like we did for SMAP before:\n\nbase = f\"https://harmony.earthdata.nasa.gov/{ccid}\"\nhreq = f\"{base}/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset\"\nrurl = f\"{hreq}?format=application/x-zarr\"\n\nprint(rurl)\n\nhttps://harmony.earthdata.nasa.gov/C1990404791-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr\n\n\nECCO monthly collections have 312 granules in V4r4 (you can confirm with the granule listing from CMR Search API) so we can get the entire time series for 2010 to 2018 with one request to the Harmony API.\nFormat a string of query parameters to limit the processing to the desired time period. Then, append the string of time subset parameters to the variable rurl.\n\nsubs = '&'.join([f'subset=time(\"{start_date}T00:00:00.000Z\":\"{end_date}T23:59:59.999Z\")'])\n\nrurl = f\"{rurl}&{subs}\"\n\nprint(rurl)\n\nhttps://harmony.earthdata.nasa.gov/C1990404791-POCLOUD/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr&subset=time(\"2010-01-01T00:00:00.000Z\":\"2018-12-31T23:59:59.999Z\")\n\n\nSubmit the request and monitor the processing status in a while loop, breaking it on completion of the request job:\n\nresponse = requests.get(url=rurl).json()\n\n# Monitor status in a while loop. Wait 10 seconds for each check.\nwait = 10\nwhile True:\n    response = requests.get(url=response['links'][0]['href']).json()\n    if response['status']!='running':\n        break\n    print(f\"Job in progress ({response['progress']}%)\")\n    time.sleep(wait)\n\nprint(\"DONE!\")\n\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nJob in progress (0%)\nDONE!\n\n\nAccess the staged cloud datasets over native AWS interfaces\nCheck the message field in the response for clues about how to proceed:\n\nprint(response['message'])\n\nThe job has completed successfully. Contains results in AWS S3. Access from AWS us-west-2 with keys from https://harmony.earthdata.nasa.gov/cloud-access.sh\n\n\nThe third item in the list of links contains the shell script from the job status message printed above. Let’s download the same information in JSON format. It should be the fourth item; check to be sure:\n\nlen(response['links'])\n\n102\n\n\nSelect the url and download the json, then load to Python dictionary and print the keys:\n\nwith requests.get(response['links'][3]['href']) as r:\n    creds = r.json()\n\nprint(creds.keys())\n\ndict_keys(['AccessKeyId', 'SecretAccessKey', 'SessionToken', 'Expiration'])\n\n\nCheck the expiration timestamp for the temporary credentials:\n\ncreds['Expiration']\n\n'2021-06-11T02:36:29.000Z'\n\n\nOpen zarr datasets with s3fs and xarray\nGet the s3 output directory and list of zarr datasets from the list of links. The s3 directory should be the fifth item; the urls are from item six onward:\n\ns3_dir = response['links'][4]['href']\n\nprint(s3_dir)\n\ns3://harmony-prod-staging/public/harmony/netcdf-to-zarr/2295236b-8086-4543-9482-f524a9f2d0c3/\n\n\nNow select the URLs for the staged files and print the first one:\n\ns3_urls = [u['href'] for u in response['links'][5:]]\n\nprint(s3_urls[0])\n\ns3://harmony-prod-staging/public/harmony/netcdf-to-zarr/2295236b-8086-4543-9482-f524a9f2d0c3/OCEAN_BOTTOM_PRESSURE_mon_mean_2009-12_ECCO_V4r4_latlon_0p50deg.zarr\n\n\nUse the AWS s3fs package and your temporary aws_creds to open the zarr directory storage:\n\ns3 = s3fs.S3FileSystem(\n    key=creds['AccessKeyId'],\n    secret=creds['SecretAccessKey'],\n    token=creds['SessionToken'],\n    client_kwargs={'region_name':'us-west-2'},\n)\n\nlen(s3.ls(s3_dir))\n\n97\n\n\nPlot the first Ocean Bottom Pressure dataset\nCheck out the documentation for xarray’s open_zarr method at this link. Open the first dataset and plot the OBP variable:\n\nds0 = xr.open_zarr(s3.get_mapper(s3_urls[0]), decode_cf=True, mask_and_scale=True)\n\n# Mask the dataset where OBP is not within the bounds of the variable's valid min/max:\nds0_masked = ds0.where((ds0.OBP>=ds0.OBP.valid_min) & (ds0.OBP<=ds0.OBP.valid_max))\n\n# Plot the masked dataset\nds0_masked.OBP.isel(time=0).plot.imshow(size=10)\n\n<matplotlib.image.AxesImage at 0x7f28ed2ba4c0>\n\n\n\n\n\nLoad the zarr datasets into one large xarray dataset\nLoad all the datasets in a loop and concatenate them:\n\nzds = xr.concat([xr.open_zarr(s3.get_mapper(u)) for u in s3_urls], dim=\"time\")\n\nprint(zds)\n\n<xarray.Dataset>\nDimensions:         (latitude: 360, longitude: 720, nv: 2, time: 97)\nCoordinates:\n  * latitude        (latitude) float64 -89.75 -89.25 -88.75 ... 89.25 89.75\n    latitude_bnds   (latitude, nv) float64 -90.0 -89.5 -89.5 ... 89.5 89.5 90.0\n  * longitude       (longitude) float64 -179.8 -179.2 -178.8 ... 179.2 179.8\n    longitude_bnds  (longitude, nv) float64 -180.0 -179.5 -179.5 ... 179.5 180.0\n  * time            (time) datetime64[ns] 2009-12-16T12:00:00 ... 2017-12-16T...\n    time_bnds       (time, nv) datetime64[ns] dask.array<chunksize=(1, 2), meta=np.ndarray>\nDimensions without coordinates: nv\nData variables:\n    OBP             (time, latitude, longitude) float64 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\n    OBPGMAP         (time, latitude, longitude) float64 dask.array<chunksize=(1, 360, 720), meta=np.ndarray>\nAttributes: (12/57)\n    Conventions:                  CF-1.8, ACDD-1.3\n    acknowledgement:              This research was carried out by the Jet Pr...\n    author:                       Ian Fenty and Ou Wang\n    cdm_data_type:                Grid\n    comment:                      Fields provided on a regular lat-lon grid. ...\n    coordinates_comment:          Note: the global 'coordinates' attribute de...\n    ...                           ...\n    time_coverage_duration:       P1M\n    time_coverage_end:            2010-01-01T00:00:00\n    time_coverage_resolution:     P1M\n    time_coverage_start:          2009-12-01T00:00:00\n    title:                        ECCO Ocean Bottom Pressure - Monthly Mean 0...\n    uuid:                         297c8df0-4158-11eb-b208-0cc47a3f687b\n\n\nReference OBP and mask the dataset according to the valid minimum and maximum:\n\nobp = zds.OBP\n\nprint(obp)\n\n<xarray.DataArray 'OBP' (time: 97, latitude: 360, longitude: 720)>\ndask.array<concatenate, shape=(97, 360, 720), dtype=float64, chunksize=(1, 360, 720), chunktype=numpy.ndarray>\nCoordinates:\n  * latitude   (latitude) float64 -89.75 -89.25 -88.75 ... 88.75 89.25 89.75\n  * longitude  (longitude) float64 -179.8 -179.2 -178.8 ... 178.8 179.2 179.8\n  * time       (time) datetime64[ns] 2009-12-16T12:00:00 ... 2017-12-16T06:00:00\nAttributes:\n    comment:                OBP excludes the contribution from global mean at...\n    coverage_content_type:  modelResult\n    long_name:              Ocean bottom pressure given as equivalent water t...\n    units:                  m\n    valid_max:              72.07011413574219\n    valid_min:              -1.7899188995361328\n\n\nGet the valid min and max from the corresponding CF attributes:\n\nobp_vmin, obp_vmax = obp.valid_min, obp.valid_max\n\nobp_vmin, obp_vmax\n\n(-1.7899188995361328, 72.07011413574219)\n\n\nMask the dataset according to the OBP min and max and plot a series:\n\n# Mask dataset where not inside OBP variable valid min/max:\nzds_masked = zds.where((obp>=obp_vmin)&(obp<=obp_vmax))\n\n# Plot SSH again for the first 12 time slices:\nobpp = zds_masked.OBP.isel(time=slice(0, 6)).plot(\n    x=\"longitude\", \n    y=\"latitude\", \n    col=\"time\",\n    levels=8,\n    col_wrap=3, \n    add_colorbar=False,\n    figsize=(14, 8)\n)\n\n# Plot a colorbar on a secondary axis\nmappable = obpp.axes[0][0].collections[0]\ncax = plt.axes([0.05, -0.04, 0.95, 0.04])\ncbar1 = plt.colorbar(mappable, cax=cax, orientation='horizontal')"
  },
  {
    "objectID": "appendix/index.html",
    "href": "appendix/index.html",
    "title": "Appendix",
    "section": "",
    "text": "More cool stuff."
  },
  {
    "objectID": "discovery/earthdata-search-ui.html",
    "href": "discovery/earthdata-search-ui.html",
    "title": "Earthdata Search UI",
    "section": "",
    "text": "Maybe a video tutorial?"
  },
  {
    "objectID": "discovery/search-by-shapefile.html#introduction",
    "href": "discovery/search-by-shapefile.html#introduction",
    "title": "Search by shapefile",
    "section": "Introduction",
    "text": "This notebook walks through the discovery, download, and plotting of remote sensing ocean SST level 2 (L2) data over a geographic region of interest defined by the user (through the use of common GIS files such as .shp) during the 2020 hurricane season.\n\nExplore Gulf of Mexico SSTs during the 2020 hurricane season\nThe evolution of sea surface temperature (SST) anomalies in the Gulf of Mexico before, during, and after Tropical Cyclones can be explored using NASA remote sensing data, such as the MODIS Aqua SST dataset. An ocean response can often be seen in the wake of hurricane tracks, with cold wakes, or areas of cooler water, along or to the right of hurricane tracks in association with wind-induced water column mixing that brings cooler waters at depth to the surface. These cold signatures are generally patchy and spatially confined.\nAs NASA Earthdata transitions to the Earthdata Cloud, it will be common for some data to exist in a traditional on-premise storage system, accessed by direct download to a local environment, while other data will have migrated to the cloud system. This need to access and customize data from the Earthdata Cloud, and work with it locally is described in this notebook. This access workflow of cloud-based data and downloding to local environment for further analysis or use is presented in the Introduction (Part I) of the workshop, and described again here:\n\n\nNote: In this example we are using a shape file to search on ocean remote sensing data, although one can do this type of search on any Earthdata (DAAC) data. For example, of particular interest could be using shape files to search on terrestrial hydrology data (e.g watersheds), globally (depending on data availability of course).\nNote: Here we show a programmatic way of completing this type of workflow, but this can also be accomplished through the Earthdata Search user interface. For a video tutorial on how to do this, please see https://www.youtube.com/watch?v=d1BR8w3u0dI&list=PLDWiCz1Ka4kSbqkoeOcPXGAv0gp8OS1Ah&index=7, and disregard the comments about ‘UAT’ (test environment), as this capability has since become operational. Use https://search.earthdata.nasa.gov/ and follow the rest of the steps in the video tutorial. Search by HUC (hydrologic unit code) capability is also available, if you are working with data over the United States. A video tutorial on how to search by HUC in Earthdata Search is available here: https://www.youtube.com/watch?v=8TLJOFe7XPw&list=PLDWiCz1Ka4kSbqkoeOcPXGAv0gp8OS1Ah&index=6\n\nLearning Objectives:\n\nEarthdata Login Authentication (for download access from Earthdata data archive)\nSearch CMR for collection and granule IDs, using the collection shortname and provider\nDownload a file from the PO.DAAC (Earthdata) cloud archive to local computer and preview the data\nSearch a collection by user-provided shapefile (ESRI shp) and temporal range\nDonwload the first file (from the PO.DAAC cloud archive to local computer) and preview subset\nDownload all data (from the PO.DAAC cloud archive to local computer) based on shp and time search criteria"
  },
  {
    "objectID": "discovery/search-by-shapefile.html#first-import-needed-packages-and-libraries",
    "href": "discovery/search-by-shapefile.html#first-import-needed-packages-and-libraries",
    "title": "Search by shapefile",
    "section": "First, import needed packages and libraries",
    "text": "#| ExecuteTime: {end_time: '2020-12-04T06:29:48.865985Z', start_time: '2020-12-04T06:29:47.947204Z'}\n%matplotlib inline\n\nfrom os.path import join, expanduser, basename\nfrom urllib import request, parse\nimport requests\nimport shutil\nimport json\nfrom http.cookiejar import CookieJar\nfrom getpass import getpass\nfrom netrc import netrc\nimport pprint\nfrom platform import system\n\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs"
  },
  {
    "objectID": "discovery/search-by-shapefile.html#earthdata-login-authentication",
    "href": "discovery/search-by-shapefile.html#earthdata-login-authentication",
    "title": "Search by shapefile",
    "section": "Earthdata Login Authentication",
    "text": "You’ll need to create an Earthdata Login account to access data, as well as discover restricted data, from the NASA Earthdata system. Follow these setup instructions\n\n\nExplore data availability using the Common Metadata Repository\nThe Common Metadata Repository (CMR) is a robust metadata system that catalogs Earth Science data and associated service metadata records. CMR supports data search and discovery through an Application Programming Interface, or API, enabling reproducible data product and data file searches using a number of helpful variables, including geographic area, keyword, provider, and time.\nGeneral CMR API documentation: https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html"
  },
  {
    "objectID": "discovery/search-by-shapefile.html#identify-a-data-collection-of-interest",
    "href": "discovery/search-by-shapefile.html#identify-a-data-collection-of-interest",
    "title": "Search by shapefile",
    "section": "1. Identify a data collection of interest",
    "text": "Data sets are selected by data set IDs (e.g. MODIS_T-JPL-L2P-v2019.0). In the CMR API documentation, a data set id is referred to as a “short name”. These short names are located at the top of each NSIDC data set landing page in gray above the full title:\n\ngo to https://search.earthdata.nasa.gov/search and type POCLOUD in the search box (which is the PO.DAAC data provider for cloud-based Pathfinder datasets in a restricted operational cloud environment). Click on the (i) next to the data collection of interest; the shortname is the subheader name below the main full data collection name.\n\n\nShortName: \nIn this tutorial we will be using the MODIS-Terra SST L2 collection, with the shortname: MODIS_T-JPL-L2P-v2019.0. Data from our collection of interest can be obtained from the PO.DAAC (Earthdata) cloud archive.\n\nSearch by dataset shortname and provider\nTo search Earthdata from this notebook, we can use the following code with key words that describe our dataset (also knows as a collection). This snippet of Python code uses the requests module to get collection metadata from the CMR, with our dataset of interset shortname and provider as search criteria (parameters). In subsequet notebooks (demos), this functionality is hidden in a package called tutorial_helper_functions for ease of use. For now, this code snippet will help us understand how the request is made:\n#| ExecuteTime: {end_time: '2020-12-04T06:29:54.594592Z', start_time: '2020-12-04T06:29:54.153543Z'}\nmodis_coll = requests.get(\n    url=\"https://cmr.earthdata.nasa.gov/search/collections.umm_json\",    # CMR API url\n    params={'ShortName': \"MODIS_T-JPL-L2P-v2019.0\",      # dataset collection shortname\n            'provider': \"POCLOUD\",      # data provider\n            'token': _token},\n).json()\n\nmodis_coll['items'][0]['meta']   # print collection metadata\nBased on the metadata retrieved above, we now know the collection ID (concept-id) is C1940475563-POCLOUD\nTo retrieve the granule (file) metadata from the CMR:\n#| ExecuteTime: {end_time: '2020-12-04T06:29:55.049622Z', start_time: '2020-12-04T06:29:54.597071Z'}\nmodis_gran = requests.get(\n    url=\"https://cmr.earthdata.nasa.gov/search/granules.umm_json\", \n    params={'ShortName': \"MODIS_T-JPL-L2P-v2019.0\", \n            'provider': \"POCLOUD\",\n            'token': _token, },\n).json()\n\nmodis_gran['items'][0]['meta']\nIn this case, the metadata tells us that the the granule ID is G1967602341-POCLOUD, among other information."
  },
  {
    "objectID": "discovery/search-by-shapefile.html#download-a-granule-preview-data",
    "href": "discovery/search-by-shapefile.html#download-a-granule-preview-data",
    "title": "Search by shapefile",
    "section": "2. Download a granule & preview data",
    "text": "Access & preview the data - get a quick feel for what the data looks like\nHere is how to list the URLs needed to access the data:\n#| ExecuteTime: {end_time: '2020-12-04T06:29:55.055258Z', start_time: '2020-12-04T06:29:55.051358Z'}\nmodis_gran['items'][0]['umm']['RelatedUrls']\nTo access POCLOUD Earthdata Cloud data, you want URLs with ‘Type’: 'GET DATA' and host https://archive.podaac.earthdata.nasa.gov.\nLesson note: This URL points you to data that is archived in a cloud environment, namely, PO.DAAC’s ‘space’ in the AWS S3 storage environment.\n#| ExecuteTime: {end_time: '2020-12-04T06:29:55.060096Z', start_time: '2020-12-04T06:29:55.056913Z'}\nmodis_url = modis_gran['items'][0]['umm']['RelatedUrls'][1]['URL']\nmodis_url\n\n\nDownload a file and open to explore\nDownload the file and open it with xarray in memory. This will pull cloud-stored data onto local space. Note there is no charge for the user in doing so.\nLesson note: You are now downloading a file to your local space, from the PO.DAAC (Earthdata) archive that lives in the AWS cloud. Again, there is no charge for the user in doing so.\n#| ExecuteTime: {end_time: '2020-12-04T06:30:04.410453Z', start_time: '2020-12-04T06:29:55.061545Z'}\nwith request.urlopen(modis_url) as response, open('tutorial1_data_MODIS.nc', 'wb') as out_file:\n    print('Content Size:', response.headers['Content-length'])\n    shutil.copyfileobj(response, out_file)\n    print(\"Downloaded request to tutorial1_data_MODIS.nc\")\nYou can preview metadata of the downloaded file with ncdump, or open in memory wtih xarray.\n#| ExecuteTime: {end_time: '2020-12-04T06:30:04.421842Z', start_time: '2020-12-04T06:30:04.418347Z'}\n#!ncdump -h tutorial1_data_MODIS.nc\n#| ExecuteTime: {end_time: '2020-12-04T06:30:04.487207Z', start_time: '2020-12-04T06:30:04.426000Z'}\nimport xarray as xr\nds_MODIS = xr.open_dataset('tutorial1_data_MODIS.nc')\nprint(ds_MODIS)\nLet’s do a quick plot of the sea_surface_temperature variable:\n#| ExecuteTime: {end_time: '2020-12-04T06:30:06.085479Z', start_time: '2020-12-04T06:30:04.488930Z'}\nfrom pandas.plotting import register_matplotlib_converters\n\nds_MODIS.sea_surface_temperature.plot()\nWe can see the time this file represents, and the variable, but it’s hard to tell where on the map we are. Let’s try it another way:\n#| ExecuteTime: {end_time: '2020-12-04T06:30:48.432752Z', start_time: '2020-12-04T06:30:06.086741Z'}\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport numpy as np\n\nax = plt.axes(projection=ccrs.PlateCarree())\nax.coastlines()\n\nplt.scatter(ds_MODIS.lon, ds_MODIS.lat, lw=2, c=ds_MODIS.sea_surface_temperature)\nplt.colorbar()\n#plt.clim(-0.3, 0.3)\n\nplt.show()"
  },
  {
    "objectID": "discovery/search-by-shapefile.html#select-data-with-shp-file-using-the-cmr-api",
    "href": "discovery/search-by-shapefile.html#select-data-with-shp-file-using-the-cmr-api",
    "title": "Search by shapefile",
    "section": "3. Select data with shp file using the CMR API",
    "text": "Now that we’ve previewed the data and decided we’d like to request more, specific to our use case, we can pass the collection ID into the CMR API to search by geographic shape file. The service to subset by shp is also in development and should be available in 2021.\n\nAccess data selected by geographic shapefile\nWe will request data overlapping the Gulf of Mexico by uploading a shape file with that boundary. This shape file can be one that you created, shred by a collaborator, or any other user, as long as it follows shape file convention. In essence, this following service allows the user to bring their own shape file to do a data search. The returned response will provide a list of data files from the cloud-based PO.DAAC archive (Earthdata Cloud) that intersect this given shape file.\nThis requires the use of a multipart/form-data POST request. Supported shapefile formats include ESRI, GeoJSON, and KML. The associated mime-types are as follows:\n\n\n\nShapefile Format\nmime-type\n\n\n\n\nESRI\napplication/shapefile+zip\n\n\nGeoJSON\napplication/geo+json\n\n\nKML\napplication/vnd.google-earth.kml+xml\n\n\n\nESRI shapefiles must be uploaded as a single .zip file.\n\n\nExample with ESRI .shp file:\nWe will now search with an ESRI shapefile and a temporal bound to limit our space and time region of interest. Namely, we will search over the Gulf of Mexico during the period 1 Aug 2020 to 1 Nov 2020, since we are interested in exploring how SST responds during the latest Atlantic hurricane season. Our spatial boundary shape file to search with over the Gulf of Mexico looks like this (previewed in QGIS):\n\nLesson Note: You are once again searching on data that lives in the PO.DAAC (Earthdata) cloud archive, from your local computer, by running this next code snippet. You are searching spatially (via shape file boundaries) and temporally.\n(Side note: the token parameter below allows you to access this new PO.DAAC cloud operational archive space. This is possible because your Earthdata login was added to a list of early access users to this cloud environment, for this workshop.)\n#| ExecuteTime: {end_time: '2020-12-04T06:30:49.137241Z', start_time: '2020-12-04T06:30:48.434222Z'}\n#| scrolled: false\n# the URL of the CMR searvice\nurl = 'https://cmr.earthdata.nasa.gov/search/granules.umm_json'\n\n#The shapefile we want to use in our search\nshp_file = open('resources/gulf_shapefile.zip', 'rb')\n\n#need to declare the file and the type we are uploading\nfiles = {'shapefile':('gulf_shapefile.zip',shp_file, 'application/shapefile+zip')}\n\n#used to define parameters such as the concept-id and things like temporal searches\nparameters = {'collection_concept_id':'C1940475563-POCLOUD', 'token': _token, 'temporal':'2020-08-23T00:00:00Z,2020-08-29T00:00:00Z', 'page_size':33}\n\nresponse = requests.post(url, files=files, params=parameters)\nresults  = response.json()\n\nprint(json.dumps(results['items'][0], indent=2))"
  },
  {
    "objectID": "discovery/search-by-shapefile.html#preview-the-selected-data",
    "href": "discovery/search-by-shapefile.html#preview-the-selected-data",
    "title": "Search by shapefile",
    "section": "4. Preview the selected data",
    "text": "The POCLOUD in the response above signals that you are indeed searching on data from the PO.DAAC archive within the Earthdata Cloud.\nLet’s take a look at the url for the first data file this spatial and temporal search result returned:\nFor now, let’s plot the first files from our query, that we have downloaded to your local space.\n#| ExecuteTime: {end_time: '2020-12-04T06:30:49.142051Z', start_time: '2020-12-04T06:30:49.138854Z'}\nnew_modis_url = results['items'][0]['umm']['RelatedUrls'][1]['URL']\nnew_modis_url\n#| ExecuteTime: {end_time: '2020-12-04T06:30:52.119682Z', start_time: '2020-12-04T06:30:49.143442Z'}\nwith request.urlopen(new_modis_url) as response, open('tutorial1_data_MODIS_from_shp.nc', 'wb') as out_file:\n    print('Content Size:', response.headers['Content-length'])\n    shutil.copyfileobj(response, out_file)\n    print(\"Downloaded request to tutorial1_data_MODIS_from_shp.nc\")\n#| ExecuteTime: {end_time: '2020-12-04T06:30:52.172485Z', start_time: '2020-12-04T06:30:52.121636Z'}\nds_MODIS_shp = xr.open_dataset('tutorial1_data_MODIS_from_shp.nc')\nds_MODIS_shp\n#| ExecuteTime: {end_time: '2020-12-04T06:31:34.060892Z', start_time: '2020-12-04T06:30:52.174585Z'}\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport numpy as np\n\nax = plt.axes(projection=ccrs.PlateCarree())\nax.coastlines()\n\nplt.scatter(ds_MODIS_shp.lon, ds_MODIS_shp.lat, lw=2, c=ds_MODIS_shp.sea_surface_temperature)\nplt.colorbar()\n#plt.clim(-0.3, 0.3)\nThis file from 23 Aug 2020 intersects our shp boundary. In this case, overlap is not extensive, but it does overlap. We can confirm our search by doing the same query in the Earthdata Search portal mentioned at the beginning of the tutorial. https://search.earthdata.nasa.gov/search/granules?projectId=8195834861\nIndeed the granule returned matches the search boundary:"
  },
  {
    "objectID": "discovery/search-by-shapefile.html#download-after-spatial-and-temporal-selection",
    "href": "discovery/search-by-shapefile.html#download-after-spatial-and-temporal-selection",
    "title": "Search by shapefile",
    "section": "5. Download after spatial and temporal selection",
    "text": "Loop to download the MODIS SST files based on our search criteria (those that intersect the region of interest (defined by our shapefile here) and our period of interest.\n#| ExecuteTime: {end_time: '2020-12-04T06:31:34.072671Z', start_time: '2020-12-04T06:31:34.068053Z'}\nurls = []\nfor r in results['items']:\n    for u in r['umm']['RelatedUrls']:\n        if u['URL'].startswith(\"https://archive.podaac\") and u['Type']==\"GET DATA\":\n            urls.append(u['URL'])\nurls\n#| ExecuteTime: {end_time: '2020-12-04T06:33:08.813653Z', start_time: '2020-12-04T06:31:34.074108Z'}\nfor u in urls:\n    print(u)\n    with request.urlopen(u) as response, open(f'{basename(u)}', 'wb') as out_file:\n        print('Content Size:', response.headers['Content-length'])\n        shutil.copyfileobj(response, out_file)\n        print(f\"Downloaded request to {basename(u)}\")\nWe now have the 33 files from our search downloaded to our local environment (computer)."
  },
  {
    "objectID": "discovery/search-by-shapefile.html#tutorial-summary-discusssion",
    "href": "discovery/search-by-shapefile.html#tutorial-summary-discusssion",
    "title": "Search by shapefile",
    "section": "Tutorial Summary & Discusssion",
    "text": "We’ve searched for a dataset of interest that is archived in the PO.DAAC Earthdata Cloud, previewed it, selected certain data granules that matched a user-defined spatial area (by using .shp files) and temporal range, and downloaded the serach results to our local computer, from the Earthdata Cloud.\nFrom here, additional analysis can be done. In future guiding notebooks, handling of MODIS L2 SST data into analysis ready data (ARD) ‘data cubes’or ’time series stacks’ will also be made available, which further shows how to treat level 2 data to create level 3-like uniformly gridded L2 data to your exact specifications.\nAnd as mentioned at the beginning of the notebook, in this example we are using a shape file to search on ocean remote sensing data, although one can do this type of search on any Earthdata (DAAC) data. For example, of particular interest could be using shape files to search on terrestrial hydrology data (e.g watersheds), globally (depending on data availability of course), or other coastal regions with unique boundaries."
  },
  {
    "objectID": "discovery/index.html",
    "href": "discovery/index.html",
    "title": "NASA Cloud Data Discovery",
    "section": "",
    "text": "Some background here about discovery.\nSome great text about CMR and CMR-STAC, among other things"
  },
  {
    "objectID": "discovery/cmr-virtual-directories.html",
    "href": "discovery/cmr-virtual-directories.html",
    "title": "CMR Virtual Directories",
    "section": "",
    "text": "Details from here:\nhttps://cmr.earthdata.nasa.gov/search/site/collections/directory/eosdis"
  },
  {
    "objectID": "discovery/Navigate_CMR_STAC.html#introduction-to-the-cmr-stac-api",
    "href": "discovery/Navigate_CMR_STAC.html#introduction-to-the-cmr-stac-api",
    "title": "",
    "section": "2.1 Introduction to the CMR-STAC API ",
    "text": "What is STAC?\n\nSTAC is a specification that provides a common language for interpreting geospatial information in order to standardize indexing and discovering data. ### Four STAC Specifications: 1. STAC API\n2. STAC Catalog\n3. STAC Collection\n4. STAC Item\n#### In the section below, we will walk through an example of each specification. For additional information, check out: https://stacspec.org/.\n\n\n\n1. STAC API: Endpoint that enables the querying of STAC items.\n\nBelow, set the CMR-STAC API Endpoint to a variable, and use the requests package to send a GET request to the endpoint, and set the response to a variable.\n\nstac = 'https://cmr.earthdata.nasa.gov/stac/' # CMR-STAC API Endpoint\nstac_response = r.get(stac).json()            # Call the STAC API endpoint\nfor s in stac_response: print(s)\n\nid\ntitle\nstac_version\ntype\ndescription\nlinks\n\n\n\nprint(f\"You are now using the {stac_response['id']} API (STAC Version: {stac_response['stac_version']}). {stac_response['description']}\")\nprint(f\"There are {len(stac_response['links'])} STAC catalogs available in CMR.\")\n\nYou are now using the stac API (STAC Version: 1.0.0). This is the landing page for CMR-STAC. Each provider link contains a STAC endpoint.\nThere are 46 STAC catalogs available in CMR.\n\n\n\n\nYou will notice above that the CMR-STAC API contains many different endpoints–not just from NASA LP DAAC, but also contains endpoints for other NASA ESDIS DAACs.\n\n\n\n2. STAC Catalog: Contains a JSON file of links that organize all of the collections available.\n\nBelow, search for LP DAAC Catalogs, and print the information contained in the Catalog that we will be using today, LPCLOUD.\n\nstac_lp = [s for s in stac_response['links'] if 'LP' in s['title']]  # Search for only LP-specific catalogs\n\n# LPCLOUD is the STAC catalog we will be using and exploring today\nlp_cloud = r.get([s for s in stac_lp if s['title'] == 'LPCLOUD'][0]['href']).json()\nfor l in lp_cloud: print(f\"{l}: {lp_cloud[l]}\")\n\nid: LPCLOUD\ntitle: LPCLOUD\ndescription: Root catalog for LPCLOUD\ntype: Catalog\nstac_version: 1.0.0\nlinks: [{'rel': 'self', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD', 'title': 'Provider catalog', 'type': 'application/json'}, {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac/', 'title': 'Root catalog', 'type': 'application/json'}, {'rel': 'collections', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections', 'title': 'Provider Collections', 'type': 'application/json'}, {'rel': 'search', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search', 'title': 'Provider Item Search', 'type': 'application/geo+json', 'method': 'GET'}, {'rel': 'search', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search', 'title': 'Provider Item Search', 'type': 'application/geo+json', 'method': 'POST'}, {'rel': 'conformance', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/conformance', 'title': 'Conformance Classes', 'type': 'application/geo+json'}, {'rel': 'service-desc', 'href': 'https://api.stacspec.org/v1.0.0-beta.1/openapi.yaml', 'title': 'OpenAPI Doc', 'type': 'application/vnd.oai.openapi+json;version=3.0'}, {'rel': 'service-doc', 'href': 'https://api.stacspec.org/v1.0.0-beta.1/index.html', 'title': 'HTML documentation', 'type': 'text/html'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM.v003', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5', 'type': 'application/json'}, {'rel': 'child', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5', 'type': 'application/json'}]\nconformsTo: ['https://api.stacspec.org/v1.0.0-beta.1/core', 'https://api.stacspec.org/v1.0.0-beta.1/item-search', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#fields', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#query', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#sort', 'https://api.stacspec.org/v1.0.0-beta.1/item-search#context', 'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/core', 'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/oas30', 'http://www.opengis.net/spec/ogcapi-features-1/1.0/conf/geojson']\n\n\n\n\nBelow, print the links contained in the LP CLOUD STAC Catalog:\n\nlp_links = lp_cloud['links']\nfor l in lp_links: \n    try: \n        print(f\"{l['href']} is the {l['title']}\")\n    except:\n        print(f\"{l['href']}\")       \n\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD is the Provider catalog\nhttps://cmr.earthdata.nasa.gov/stac/ is the Root catalog\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections is the Provider Collections\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/search is the Provider Item Search\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/search is the Provider Item Search\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/conformance is the Conformance Classes\nhttps://api.stacspec.org/v1.0.0-beta.1/openapi.yaml is the OpenAPI Doc\nhttps://api.stacspec.org/v1.0.0-beta.1/index.html is the HTML documentation\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/ASTGTM.v003\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5\nhttps://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5\n\n\n\n\n\n3. STAC Collection: Extension of STAC Catalog containing additional information that describe the STAC Items in that Collection.\n\nBelow, get a response from the LPCLOUD Collection and print the information included in the response.\n\nlp_collections = [l['href'] for l in lp_links if l['rel'] == 'collections'][0]  # Set collections endpoint to variable\ncollections_response = r.get(f\"{lp_collections}\").json()                        # Call collections endpoint\nprint(f\"This collection contains {collections_response['description']} ({len(collections_response['collections'])} available)\")\n\nThis collection contains All collections provided by LPCLOUD (3 available)\n\n\n\n\nAs of March 3, 2021, there are three collections available, and more will be added in the future.\n\n\nPrint out one of the collections:\n\ncollections = collections_response['collections']\ncollections[1]\n\n{'id': 'HLSL30.v1.5',\n 'stac_version': '1.0.0',\n 'license': 'not-provided',\n 'title': 'HLS Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30 m V1.5',\n 'type': 'Collection',\n 'description': 'PROVISIONAL - The Harmonized Landsat and Sentinel-2 (HLS) project provides consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from the Operational Land Imager (OLI) aboard the joint NASA/USGS Landsat 8 satellite and the Multi-Spectral Instrument (MSI) aboard Europe’s Copernicus Sentinel-2A and Sentinel-2B satellites. The combined measurement enables global observations of the land every 2–3 days at 30-meter (m) spatial resolution. The HLS project uses a set of algorithms to obtain seamless products from OLI and MSI that include atmospheric correction, cloud and cloud-shadow masking, spatial co-registration and common gridding, illumination and view angle normalization, and spectral bandpass adjustment. \\r\\n\\r\\nThe HLSL30 product provides 30-m Nadir Bidirectional Reflectance Distribution Function (BRDF)-Adjusted Reflectance (NBAR) and is derived from Landsat 8 OLI data products. The  HLSS30 (https://doi.org/10.5067/HLS/HLSS30.015) and HLSL30 products are gridded to the same resolution and Military Grid Reference System (MGRS) (https://hls.gsfc.nasa.gov/products-description/tiling-system/) tiling system, and thus are “stackable” for time series analysis.\\r\\n\\r\\nThe HLSL30 product is provided in Cloud Optimized GeoTIFF (COG) format, and each band is distributed as a separate file. There are 10 bands included in the HLSL30 product along with one quality assessment (QA) band and four angle bands. For a more detailed description of the individual bands provided in the HLSL30 product, please see the User Guide (https://lpdaac.usgs.gov/documents/878/HLS_User_Guide_V15_provisional.pdf).\\r\\n\\r\\n',\n 'links': [{'rel': 'self',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5',\n   'title': 'Info about this collection',\n   'type': 'application/json'},\n  {'rel': 'root',\n   'href': 'https://cmr.earthdata.nasa.gov/stac',\n   'title': 'Root catalog',\n   'type': 'application/json'},\n  {'rel': 'parent',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD',\n   'title': 'Parent catalog',\n   'type': 'application/json'},\n  {'rel': 'items',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSL30.v1.5/items',\n   'title': 'Granules in this collection',\n   'type': 'application/json'},\n  {'rel': 'about',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C1711972753-LPCLOUD.html',\n   'title': 'HTML metadata for collection',\n   'type': 'text/html'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/C1711972753-LPCLOUD.json',\n   'title': 'CMR JSON metadata for collection',\n   'type': 'application/json'}],\n 'extent': {'crs': 'http://www.opengis.net/def/crs/OGC/1.3/CRS84',\n  'spatial': {'bbox': [[-180, -90, 180, 90]]},\n  'trs': 'http://www.opengis.net/def/uom/ISO-8601/0/Gregorian',\n  'temporal': {'interval': [['2013-04-11T00:00:00.000Z', None]]}}}\n\n\n\n\nIn CMR, id is used to query by a specific product, so be sure to save the ID for the HLS S30 and L30 V1.5 products below:\n\n# Search available collections for HLS and print them out\nhls_collections = [c for c in collections if 'HLS' in c['title']]\nfor h in hls_collections: print(f\"{h['title']} has an ID (shortname) of: {h['id']}\")\n\nHLS Operational Land Imager Surface Reflectance and TOA Brightness Daily Global 30 m V1.5 has an ID (shortname) of: HLSL30.v1.5\nHLS Sentinel-2 Multi-spectral Instrument Surface Reflectance Daily Global 30 m V1.5 has an ID (shortname) of: HLSS30.v1.5\n\n\n\nNote that the “id” shortname is in the format: productshortname.vVVV (where VVV = product version)\n\n\n\nExplore the attributes contained in the HLSS30 Collection.\n\ns30 = [h for h in hls_collections if h['id'] == 'HLSS30.v1.5'][0]  # Grab HLSS30 collection\nfor s in s30['extent']: print(f\"{s}: {s30['extent'][s]}\")          # Check out the extent of this collection\n\ncrs: http://www.opengis.net/def/crs/OGC/1.3/CRS84\nspatial: {'bbox': [[-180, -90, 180, 90]]}\ntrs: http://www.opengis.net/def/uom/ISO-8601/0/Gregorian\ntemporal: {'interval': [['2014-04-03T00:00:00.000Z', None]]}\n\n\n\n\nSo here we can see that the extent is global, and can also see the temporal range–where “None” means on-going or to present.\n\nprint(f\"HLS S30 Start Date is: {s30['extent']['temporal']['interval'][0][0]}\")\ns30_id = s30['id']\n\nHLS S30 Start Date is: 2014-04-03T00:00:00.000Z\n\n\n\n\nNext, explore the attributes of the HLSL30 collection.\n\nl30 = [h for h in hls_collections if h['id'] == 'HLSL30.v1.5'][0]     # Grab HLSL30 collection\nfor l in l30['extent']: print(f\"{l}: {l30['extent'][l]}\")             # Check out the extent of this collection\nprint(f\"HLS L30 Start Date is: {l30['extent']['temporal']['interval'][0][0]}\")\nl30_id = l30['id']\n\ncrs: http://www.opengis.net/def/crs/OGC/1.3/CRS84\nspatial: {'bbox': [[-180, -90, 180, 90]]}\ntrs: http://www.opengis.net/def/uom/ISO-8601/0/Gregorian\ntemporal: {'interval': [['2013-04-11T00:00:00.000Z', None]]}\nHLS L30 Start Date is: 2013-04-11T00:00:00.000Z\n\n\n\n\nAbove, notice that the L30 product has a different start date than the S30 product.\n\n\n\n4. STAC Item: Represents data and metadata assets that are spatiotemporally coincident\n\nBelow, query the HLSS30 collection for items and return the first item in the collection.\n\n# Below, go through all links in the collection and return the link containing the items endpoint\ns30_items = [s['href'] for s in s30['links'] if s['rel'] == 'items'][0]  # Set items endpoint to variable\ns30_items\n\n'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5/items'\n\n\n\ns30_items_response = r.get(f\"{s30_items}\").json()                        # Call items endpoint\ns30_item = s30_items_response['features'][0]                             # select first item (10 items returned by default)\ns30_item\n\n{'type': 'Feature',\n 'id': 'G1969487860-LPCLOUD',\n 'stac_version': '1.0.0',\n 'stac_extensions': ['https://stac-extensions.github.io/eo/v1.0.0/schema.json'],\n 'collection': 'HLSS30.v1.5',\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-119.1488671, 33.3327671],\n    [-118.9832795, 33.3355226],\n    [-118.6783731, 34.3301598],\n    [-119.1737801, 34.3223655],\n    [-119.1488671, 33.3327671]]]},\n 'bbox': [-119.17378, 33.332767, -118.678373, 34.33016],\n 'links': [{'rel': 'self',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5/items/G1969487860-LPCLOUD'},\n  {'rel': 'parent',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5'},\n  {'rel': 'collection',\n   'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/collections/HLSS30.v1.5'},\n  {'rel': 'root', 'href': 'https://cmr.earthdata.nasa.gov/stac/'},\n  {'rel': 'provider', 'href': 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G1969487860-LPCLOUD.json'},\n  {'rel': 'via',\n   'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G1969487860-LPCLOUD.umm_json'}],\n 'properties': {'datetime': '2015-08-26T18:54:35.450Z',\n  'start_datetime': '2015-08-26T18:54:35.450Z',\n  'end_datetime': '2015-08-26T18:54:35.450Z',\n  'eo:cloud_cover': 6},\n 'assets': {'VZA': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.VZA.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.VZA.tif'},\n  'VAA': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.VAA.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.VAA.tif'},\n  'SAA': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.SAA.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.SAA.tif'},\n  'B11': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B11.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B11.tif'},\n  'B02': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B02.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B02.tif'},\n  'B09': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B09.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B09.tif'},\n  'B12': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B12.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B12.tif'},\n  'B03': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B03.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B03.tif'},\n  'B01': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B01.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B01.tif'},\n  'B07': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B07.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B07.tif'},\n  'SZA': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.SZA.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.SZA.tif'},\n  'B05': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B05.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B05.tif'},\n  'B06': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B06.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B06.tif'},\n  'Fmask': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.Fmask.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.Fmask.tif'},\n  'B10': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B10.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B10.tif'},\n  'B08': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B08.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B08.tif'},\n  'B8A': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B8A.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B8A.tif'},\n  'B04': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.B04.tif',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.B04.tif'},\n  'browse': {'title': 'Download HLS.S30.T11SLT.2015238T185436.v1.5.jpg',\n   'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-public/HLSS30.015/HLS.S30.T11SLT.2015238T185436.v1.5.jpg',\n   'type': 'image/jpeg'},\n  'metadata': {'href': 'https://cmr.earthdata.nasa.gov/search/concepts/G1969487860-LPCLOUD.xml',\n   'type': 'application/xml'}}}\n\n\n\n\nSTAC metadata provides valuable information on the item, including a unique ID, when it was acquired, the location of the observation, and a cloud cover assessment.\n\n# Print metadata attributes from this observation\nprint(f\"The ID for this item is: {s30_item['id']}\")\nprint(f\"It was acquired on: {s30_item['properties']['datetime']}\")\nprint(f\"over: {s30_item['bbox']} (Lower Left, Upper Right corner coordinates)\")\nprint(f\"It contains {len(s30_item['assets'])} assets\")\nprint(f\"and is {s30_item['properties']['eo:cloud_cover']}% cloudy.\")\n\nThe ID for this item is: G1969487860-LPCLOUD\nIt was acquired on: 2015-08-26T18:54:35.450Z\nover: [-119.17378, 33.332767, -118.678373, 34.33016] (Lower Left, Upper Right corner coordinates)\nIt contains 20 assets\nand is 6% cloudy.\n\n\n\n\nBelow, print out the ten items and the percent cloud cover–we will use this to decide which item to visualize in the next section.\n\nfor i, s in enumerate(s30_items_response['features']):\n    print(f\"Item at index {i} is {s['properties']['eo:cloud_cover']}% cloudy.\")\n\nItem at index 0 is 6% cloudy.\nItem at index 1 is 4% cloudy.\nItem at index 2 is 0% cloudy.\nItem at index 3 is 64% cloudy.\nItem at index 4 is 0% cloudy.\nItem at index 5 is 39% cloudy.\nItem at index 6 is 74% cloudy.\nItem at index 7 is 100% cloudy.\nItem at index 8 is 30% cloudy.\nItem at index 9 is 67% cloudy.\n\n\n\n\nUsing the information printed above, set the item_index below to whichever observation is the least cloudy above.\nitem_index = 9  # Indexing starts at 0 in Python, so here select the eighth item in the list at index 7\n\ns30_item = s30_items_response['features'][item_index]  # Grab the next item in the list\n\nprint(f\"The ID for this item is: {s30_item['id']}\")\nprint(f\"It was acquired on: {s30_item['properties']['datetime']}\")\nprint(f\"over: {s30_item['bbox']} (Lower Left, Upper Right corner coordinates)\")\nprint(f\"It contains {len(s30_item['assets'])} assets\")\nprint(f\"and is {s30_item['properties']['eo:cloud_cover']}% cloudy.\")\n\nThe ID for this item is: G2010287698-LPCLOUD\nIt was acquired on: 2016-11-06T08:21:39.880Z\nover: [24.875464, -26.295042, 25.108568, -25.427554] (Lower Left, Upper Right corner coordinates)\nIt contains 20 assets\nand is 67% cloudy.\n\n\n\n\nBelow, print out the names of all of the assets included in this item.\n\nprint(\"The following assets are available for download:\")\nfor a in s30_item['assets']: print(a)\n\nThe following assets are available for download:\nSZA\nB01\nVAA\nSAA\nB10\nB8A\nB05\nB09\nFmask\nB02\nB12\nB11\nB03\nB06\nB04\nB08\nVZA\nB07\nbrowse\nmetadata\n\n\n\n\nNotice that each HLS item includes a browse image. Read the browse file into memory and visualize the HLS acquisition.\n\ns30_item['assets']['browse']\n\n{'title': 'Download HLS.S30.T35JKM.2016311T080122.v1.5.jpg',\n 'href': 'https://lpdaac.earthdata.nasa.gov/lp-prod-public/HLSS30.015/HLS.S30.T35JKM.2016311T080122.v1.5.jpg',\n 'type': 'image/jpeg'}\n\n\n\n\nUse the skimage package to load the browse image into memory and matplotlib to quickly visualize it.\n\nimage = io.imread(s30_item['assets']['browse']['href'])  # Load jpg browse image into memory\n\n# Basic plot of the image\nplt.figure(figsize=(10,10))              \nplt.imshow(image)\nplt.show()\n\n\n\n\n\n\nCongrats! You have visualized your first Cloud-Native HLS asset!"
  },
  {
    "objectID": "contributing/index.html#intro",
    "href": "contributing/index.html#intro",
    "title": "Contributing",
    "section": "Intro",
    "text": "This section describes how our NASA Openscapes team collaborates to create this Cookbook, with an eye towards how others could collaborate with us in the future.\nOur style of working is greatly influenced by:\n\nThe Turing Way Community Handbook\nThe Carpentries Curriculum Development Handbook\nThe Documentation System"
  },
  {
    "objectID": "contributing/index.html#quarto",
    "href": "contributing/index.html#quarto",
    "title": "Contributing",
    "section": "Quarto",
    "text": "We’re making the EarthData Cloud Cookbook with Quarto: quarto.org. Quarto makes collaborating to create technical documentation streamlined because we work in plain text documents that can have executable code (Python, R) and are rendered using Jupyter and Knitr engines.\nWhat is Quarto? Quarto builds from what RStudio learned from RMarkdown but enables different engines (Jupyter and knitr). It is both a Command Line Tool and R package. .qmd is a new filetype like .Rmd — meaning it’s a text file but when coupled with an engine that executes code it can be rendered as html, pdf, word, and beyond. Collaborators can develop text and notebooks in wherever they are most comfortable. Then Quarto builds them together as a book or website, even converting between file types like .ipynb, .md and .qmd it’s a streamlined was to develop and publish with collaborators that have different workflows. Once the book is “served” locally, .md files auto-update as you edit, and files with executable code can be rendered individually, and the behavior of different code chunks can be controlled and cached.\n(Note: with Quarto, e-books and websites are very similarly structured, with e-books being set up for numbered chapters and references and websites set up for higher number of pages and organization. We can talk about our book as a book even as we explore whether book/website better suits our needs. Our Cookbook is currently a website; this is assigned in _quarto.yml, as we’ll explore later)."
  },
  {
    "objectID": "contributing/workflow.html#workflow-for-contributing-to-our-cookbook",
    "href": "contributing/workflow.html#workflow-for-contributing-to-our-cookbook",
    "title": "Workflow",
    "section": "Workflow for contributing to our Cookbook",
    "text": "Your workflow can be from wherever you are most comfortable. You can develop chapters working in a text editor, integrated development environment (IDE), or notebook interface. And you can serve Quarto from the Command Line or R. Quarto will combine files of different types ( .md , .ipynb, .Rmd, and .qmd) to make the Cookbook. This workflow can streamline collaboration for scientific & technical writing across programming languages.\nBy default, rendering the Cookbook will only act on markdown text and will not execute any code. This way, rendering the whole Cookbook will not become overly cumbersome as it grows, and there is not one single virtual environment with all the libraries required. Instead, our workflow is that as you develop a single chapter (or section), you control when you render, and can create a requirements.txt file for that chapter (or section). This will also make it much easier to port lessons that work standalone and are ready for a Cloud instance or a workshop."
  },
  {
    "objectID": "contributing/workflow.html#quickstart-reference",
    "href": "contributing/workflow.html#quickstart-reference",
    "title": "Workflow",
    "section": "Quickstart reference",
    "text": "Daily setup: get the latest!\nQuarto (and the RStudio IDE if you’re using it) are under active development; currently it’s important to get the daily versions before you start working.\n\nQuarto: https://github.com/quarto-dev/quarto-cli/releases/latest\n(RStudio IDE: https://dailies.rstudio.com/)\nPull from GitHub\ngit checkout main\ngit pull\n\n\n\n\n\n\n\nSummary of GitHub and Quarto commands detailed below\n\n\n\n\n\n## check which branches exist, where you are, and pull recent from main branch\ngit branch\ngit checkout main\ngit pull\n\n## create and switch to new branch\ngit checkout -b branch-name\n\n## develop content: write prose in markdown, code in R and Python\n## remember to render any .ipynb, .rmd, or .qmd files before pushing\nquarto serve\nquarto render # can also render single file\n\n## commit changes\ngit add --all\ngit status\ngit commit -m \"my commit message here\"\n\n## push changes\ngit push -u origin branch-name  # connect your branch to github.com and push\n\n## create a pull request\n## from GitHub.com, create a pull request and once it is merged, delete your branch\n\n## delete branch\ngit checkout main         # switch to the main branch\ngit pull                  # pull merged updates from github.com\ngit branch -d branch-name # delete old local  branch"
  },
  {
    "objectID": "contributing/workflow.html#github-workflow",
    "href": "contributing/workflow.html#github-workflow",
    "title": "Workflow",
    "section": "GitHub Workflow",
    "text": "First let’s talk about the GitHub part of the workflow.\nWe will work in branches so as to not overwrite each other’s work, and let GitHub do what it does best.\nThe main branch will be the current approved version of the book. The main branch is what displays at https://nasa-openscapes.github.io/earthdata-cloud-cookbook.\nA nice clean workflow with branches is to consider them temporary. You pull the most recent from main, you create a branch locally, you make your edits, you commit regularly, you push regularly to github.com, and then you create a pull request for it to be merged into main, and when it’s approved the branch is deleted on github.com and you also delete it locally. That’s the workflow we’ll walk through here. A great resource on GitHub setup and collaboration is Happy Git with R, which includes fantastic background philosophy as well as bash commands for setup, workflows, and collaboration.\nThe following assumes you’re all setup from the previous chapter.\n\nBranch setup\nFirst off, check what branch you’re on and pull the most recent edits from the main branch. If you need to switch branches, use git checkout. *Note: a new alternative to git checkout is git switch (see this blog); when you updated git consider using it here instead too.\ngit branch          # returns all local branches\ngit checkout main   # switch branch to main\ngit pull            # pull most recent from the main branch\nIf you are already on the main branch, git will tell you, and that’s fine.\n(If you have any residual branches from before, you’ll likely want to start off by deleting them — assuming they were temporary and have been merged into github.com. You can delete a branch with git branch -d branch-name).\nNext, create a new branch, then switch to that branch to work in. Below is a one-step approach for the two-step process of git branch branch-name then git checkout branch-name (read more).\ngit checkout -b branch-name  # create and switch to new branch\n\n\nDevelop content\nTime to edit and develop content, and run your Quarto Workflow – see specific instructions below. While you’re developing, you’ll want to frequently commit and push your changes.\n\n\nCommit changes\nYou’ll commit your work regularly as you go, likely using the following, which commits all files you’ve affected within the Cookbook project:\ngit add --all\ngit commit -m \"my commit message here\"\nFrom R Packages by Hadley Wickham:\n\nA commit takes a snapshot of your code at a specified point in time. Using a Git commit is like using anchors and other protection when climbing. If you’re crossing a dangerous rock face you want to make sure you’ve used protection to catch you if you fall. Commits play a similar role: if you make a mistake, you can’t fall past the previous commit.\n\nHere are more of Hadley’s suggested best practices.\n\n\n\n\n\n\nYou must re-render all files that aren’t plain .mds to view your edits\n\n\n\nIf you added or made changes to any text or code (including changing the dataset upon which existing code relies) within .ipynb, .qmd, or .Rmd files, you must re-render those files individually before pushing. See the Quarto render section for more details. Afterwards, git add and git commit any changes to the updated _freeze directory before continuing on to the next step.\nLocal re-rendering is necessary in cases where code is changed because the workflow used to make this site assumes that all code has been pre-executed. Read more about this at freeze section at the Quarto docs.\n\n\n\n\nPush changes\nWhen you’re ready to push changes you’ve made in your branch, you’ll first need to connect it to github.com by pushing it “upstream” to the “origin repository” (-u below is short for --set-upstream):\ngit push -u origin branch-name  # connect your branch to github.com and push\nThe above is a one-time command to connect your local branch back to github.com. After you’ve pushed successfully the first time, then as you continue to commit, you’ll be able to push as normal:\ngit push\n\n\nDelete your local changes\nThere are several ways to delete your local changes if you were playing around and want to reset. Here are a few:\nBurn it all down - delete the whole repo that you have locally, and then reclone.\ncd 2021-Cloud-Hackathon\nrm -rf 2021-Cloud-Hackathon \nUndo changes you’ve maybe saved or committed, but not pushed. This is less time and internet intensive (no new clone/download).\nIf you’ve got changes saved, but not yet staged, committed, or pushed, you’ll delete unstaged changes in the working directory with clean:\ncd 2021-Cloud-Hackathon\ngit clean -df\ngit checkout -- .\nHere is a whole blog on how to do this, with conceptual diagrams, command line code, and screenshots from RStudio. https://ohi-science.org/news/github-going-back-in-time\n\n\nUpdate local branch with remote main branch\nIf while you’re working you would like to update your local your-branch with the most recent updates on the main branch on GitHub.com, there are several ways to do this.\n\ncheckouts and merge main\nGit Update Local Branch with remote Master\ngit checkout main\ngit pull\ngit checkout your-branch\ngit merge main\n\n\nfetch and merge origin/main\ngit checkout your-branch\ngit fetch\ngit merge origin/main\n\n\n\nPull Request\nNow you’ve synced your work to github.com. It is currently online, in a separate branch from the main branch. Go to https://github.com/nasa-openscapes/earthdata-cloud-cookbook, find your branch, and do a pull request.\nTODO: Let’s discuss our review process:\n\nTag someone to review, (including you if it’s a quick fix?)\nTimeline\nMerging\n\nWhen the pull request is merged, delete the branch on github.com. GitHub will prompt you with a button at the end of the merge.\n\n\nDelete Branch\nOnce your pull request is merged and you’ve deleted the branch from github.com, then come back to your local setup and delete the branch locally:\ngit checkout main         # switch to the main branch\ngit pull                  # pull merged updates from github.com\ngit branch -d branch-name # delete old local  branch"
  },
  {
    "objectID": "contributing/workflow.html#quarto-workflow",
    "href": "contributing/workflow.html#quarto-workflow",
    "title": "Workflow",
    "section": "Quarto Workflow",
    "text": "Now the fun part! Our overall workflow will be to serve the book at the beginning, develop/edit chapters as simple text files (.md/.qmd/.Rmd) or executable notebooks (.ipynb) that will all render into the book.\nQuarto lets us easily convert between file types, so depending on how you prefer to work and how you’d like to interact with different audiences, we can go between formats as we wish. For example, we can converting an existing .ipynb to .qmd to collaborate during development, and then convert back to .ipynb files for our workshops. See quarto convert help for details.\nAs you work, you’ll follow our GitHub workflow above, committing regularly. Remember to quarto render individual notebooks you’re working on so that your changes will be be included in the whole Cookbook before pushing to github.com.\nThe following is to run Quarto from the command line; see quarto.org to see equivalents in R.\n\nQuarto serve\nThe thing to do first is to “serve” the Cookbook so that we can see what it looks like as we develop the chapters (it’s called “serve” because it’s really a website that looks like a book).\nRun the following from your branch in your earthdata-cloud-cookbook directory from the command line:\nquarto serve\nAnd after it’s is served, you can click from the console (or paste the url into your browser) to see the development version of the Cookbook.\n\n\n\n\n\n\nThis command line instance is now being used to serve Quarto\n\n\n\nYou can open another instance to continue working from the command line, including running other shell commands and rendering (see next). Launching your command line shell of choice will open a new instance.\n\n\n\n\nDevelop Cookbook Content\nYou can develop Cookbook chapters in the text editor, IDE, or notebook editor of your choice (i.e. see JupyterLab with Quarto).\n\nRStudio IDE & Visual Editor\nYou can also use the RStudio IDE. It can be used as a simple text editor, but it can also interactively execute code in .qmd and .Rmd files too.\nThe RStudio IDE Visual Editor makes this experience feel like a cross between an interactive notebook and a Google Doc:\n\n\n\nThe RStudio IDE Visual Editor with an interactive .qmd file\n\n\nAbove shows the Visual Editor in the top left pane with an interactive .qmd file. Learn more about the RStudio Visual Editor.\nAnother benefit of the RStudio IDE is that it has a docked command line (Terminal, bottom left pane), file navigation (bottom right pane) and GitHub interface (top right pane). The IDE helps keep things organized as you work, and provides a visual way to review your git commits and do common commands (see this RStudio-GitHub walk through from R for Excel Users). Note too that the image shows the second instance of the Terminal command line; the first is being used to serve Quarto.\n\n\n\nQuarto render\nAs you develop book chapters and edit files, any .md files will automatically refresh in the browser (so long as quarto serve is running)!\nTo refresh files with executable code, you’ll need to render them individually. You can do the following to render .ipynb/.qmd/.Rmd files so that they show up refreshed in the served Cookbook.\nquarto render my-document.ipynb      ## render a notebook\nquarto render my-work.qmd            ## render a Quarto file\nquarto render my-contribution.Rmd    ## render a RMarkdown file\nFrom the RStudio IDE, you can also press the Render button to render .qmd and .Rmd files.\nAnd you can also render the whole book:\nquarto render\nLearn more about rendering with Quarto. From J.J. Allaire:\n\nThe reason Quarto doesn’t render .Rmd and .qmd on save is that render could (potentially) be very long running and that cost shouldn’t be imposed on you whenever you save. Here we are talking about the age old debate of whether computational markdown should be rendered on save when running a development server. Quarto currently doesn’t do this to give the user a choice between an expensive render and a cheap save.\n\n\n\nIncludes (re-use markdown)\nWe are setup with an includes filter (details at Quarto.org) that lets us re-use markdown content within a project! (You can think of this like “knit child” in R Markdown and lets you source text like you source scripts from each other). This means that we can write text more modularly and re-use it in multiple places so that it’s always up to date.\nAn example of this in action is in our 2021-Cloud-Hackathon Quarto book, where each day of the schedule is saved in a separate file:\n\n\nThis is then called within a book chapter, with a relative filepath:\nThe Clinic will occur in 2 halves, with a 5 minute break in-between:\n\n{.include}\n../logistics/_schedule-clinic.md\n…to finally look like so:\n\nIncludes - things to note\n\nprefix files to include with an underscore.\n\nFrom quarto.org: You should always use an underscore prefix with included files so that they are automatically ignored (i.e. not treated as standalone files) by a quarto render of your project."
  },
  {
    "objectID": "contributing/workflow.html#i2c-workflow",
    "href": "contributing/workflow.html#i2c-workflow",
    "title": "Workflow",
    "section": "2i2c Workflow",
    "text": "We use 2i2c to run notebooks.\n\nLog into 2i2c\n\nGo to the openscapes 2i2c. You should see the openscapes 2i2c instance.\nClick on the orange “Log in to continue” button. You should see the Openscapes-Prod page.\nClick the “Sign in with Github” button and log in. If you aren’t already logged into Github, you should see the login prompt. Otherwise, you will be logged in automatically.\n\n\n\nStart a 2i2c session\nAt this point, what you see depends on whether or not you have an active session. If your session is active, JupyterLab will load and you can start your work. If not, you’ll need to start a new session:\n\nSelect a server based on the size of your job. You should see a progress window showing you what 2i2c is doing to get your session started. It may take several minutes, but you’ll eventually see a JupyterLab instance.\n\n\n\nCreate a Jupyter kernel to run notebooks\nThe default jupyter kernel may not have all the libraries you need to run a notebook. Fortunately, you can make a new kernel on the fly to use with the notebook.\n\nOpen a terminal in JupyterLab.\n\nClick on the large blue “+” button on the upper left. You should get a new Laucher tab.\nIn the Launcher tab, click on “Terminal” under “Other.” You should get a tab with a command line prompt.\n\nCreate a conda environment using your favorite method.\nActivate the new environment with conda activate YOUR_ENVIRONMENT_HERE. You should see the name of the new environment in your command line prompt.\nCreate a new kernel by calling ipython kernel install --name YOUR_ENVIRONMENT_HERE --user. You should get a response saying saying the kernel has been installed.\n\nTo use this new kernel,\n\nOpen the notebook you want to run. You should see the notebook in a tab.\nClick on the current kernel on the upper right. The default kernel is called Python 3. You should see a kernel selection widget.\nSelect the new kernel you created and click the “Select” button. The kernel on the upper right should now give the name of your custom kernel."
  },
  {
    "objectID": "contributing/workflow.html#virtual-environments",
    "href": "contributing/workflow.html#virtual-environments",
    "title": "Workflow",
    "section": "Virtual Environments",
    "text": "If you are working on a chapter that loads any Python or R packages, to make your work reproducible you’ll need to create and then update the environments.txt file. Do this use the pip freeze command:\npip freeze > requirements.txt\nThis will overwrite/update the requirements.txt file. Depending on where you are working, you might also want to create a new subfolder to store the requirements.txt. See the next section on Cookbook Structure.\nYou you will then commit and push along with your other edits back to github.com.\nTODO: info about conda…"
  },
  {
    "objectID": "contributing/workflow.html#cookbook-structure",
    "href": "contributing/workflow.html#cookbook-structure",
    "title": "Workflow",
    "section": "Cookbook Structure",
    "text": "Each chapter in our Cookbook is a separate file (.md/ .ipynb/.qmd/.Rmd). These are stored in our files directory, organized by sub-directory.\nThe Cookbook structure (i.e. the order of sections and chapters) is determined in the _quarto.yml file in the root directory. We can shuffle chapter order by editing the _quarto.yml file, and and add new chapters by adding to the _quarto.yml and creating a new file in the appropriate sub-directory that is indicated in _quarto.yml.\n\n\n\nComparing `_quarto.yml` file to served project in the browser\n\n\nPlease experiment, add new chapters and sections; we can shuffle chapter order and subsections as we continue to develop the Cookbook, nothing is set in stone."
  },
  {
    "objectID": "contributing/workflow.html#cookbook-practices",
    "href": "contributing/workflow.html#cookbook-practices",
    "title": "Workflow",
    "section": "Cookbook Practices",
    "text": "These are shared practices that we have for co-developing the Cookbook. This will be developed further as we go!\n\nMarkdown formatting with Quarto\nQuarto expects a certain “flavor of Markdown”, which means there are a few things we should be aware of with any Markdown in notebooks:\nAvoid --- as Markdown breaks because it is confused with yaml headers and will stop displaying your notebook.\nPut a carriage return before lists\nThis will avoid the following, when ipynb can display the list correctly but quarto does not:\n\n\n\nmarkdown-nospace-ipynb\n\n\n\n\n\nmarkdown-nospace-quarto\n\n\n\n\nExecuting notebooks\nAs you develop files with executable code ( .qmd, .Rmd, and .ipynb), you can decide if you don’t want the notebook to execute. By adding YAML as a raw text cell at the top of an .ipynb file, you can control whether it is executed or not. Adding execute: false to the YAML at the top of the file basically means that Quarto never runs the code, but the user of course still can interactively in Jupyter.\nUsing .qmd there are also ways to control execution cell-by-cell via # | syntax within a code chunk; see https://quarto.org/docs/computations/execution-options.html\n\n\nIncluding remote notebooks\nWe can include remote notebooks in the Cookbook by explicitly importing them with the following steps. This will create a local copy of them that have additional preamble inserted that includes the original urls and attribution for the notebook.\n\nNavigate to the _import directory.\nOpen assets.json in a text editor. Copy an existing example and use the same structure to indicate the remote notebook you’d like to include. You can write Markdown in the preamble.\n\ntitle: this will be the new title of the notebook\npreamble: this text will be inserted into the notebook below the new title. It should include any description and attribution of the notebook. The preamble is followed by two URL fields (next):\nsource: the url landing page. This should be more general than the specific notebook (i.e. this could be the root GitHub repository).\nurl: the url of the notebook. (i.e. a url that ends with .ipynb)\ntarget: the local filename to give the notebook. The notebook will be saved in the external folder in the root directory.\nprocess: true or false: whether or not to include the entire entry when running the quarto_import.py script\n\n\nAfter these updates to _import/assets.json, to the following in the terminal:\ncd _import\nconda env update -f environment.yml\nconda activate quarto-import\npython quarto_import.py -f assets.json\nThis will return a confirmation of the file that has been processed.\nThen update _quarto.yml by adding your file (external/<target) to the appropriate location in the Cookbook.\n\n\nCreate a .qmd from external .ipynb\nThis is a bit of a hacky way that we can formalize further if it’s useful:\n\nGo to an existing notebook, e.g. https://github.com/podaac/AGU-2020/blob/main/Part-II/01_sst_shpfile/AGU_tutorial1_shp_search.ipynb\nClick on Raw, e.g. https://raw.githubusercontent.com/podaac/AGU-2020/main/Part-II/01_sst_shpfile/AGU_tutorial1_shp_search.ipynb\nCopy text\nIn your local setup (e.g. RStudio IDE): New File > Text File > Paste > Save (with the same name if appropriate) and .ipynb extension, e.g.: discovery/search-by-shapefile.ipynb\nIn the command line:\n\n#| eval: false\nquarto convert help # reminder of commands!\nquarto convert discovery/search-by-shapefile.ipynb # convert to .qmd\nrm discovery/search-by-shapefile.ipynb # delete .ipynb"
  },
  {
    "objectID": "contributing/workflow.html#notebook-review",
    "href": "contributing/workflow.html#notebook-review",
    "title": "Workflow",
    "section": "Notebook Review",
    "text": "Preamble\nGitHub displays differences in code, so a nice way to review other people’s work (or your own) is to edit files directly and view the commit differences at GitHub. We’ll do this by first creating a branch and pull request so that that there is a chance for the original author to be notified of the suggestions and review them before merging back to the main branch (at which point the quarto book/site will be rebuilt since we have a GitHub action setup to do so).\nGitHub can have trouble with .ipynb notebooks; their differences include file-formatting changes and are often too big to display. See examples: CMR notebook with file-formatting diffs and the CMR-STAC API notebook review with diffs too big to display. This means that while you can still edit it directly, it’s hard to alert the author to what changes you made. Some emerging conventions to help with this:\n\nwrite TODO for “to do’s”, but this as one word all-caps makes it easier to search\n\n\n\nReview with .qmds\nSince quarto notebook files (.qmds) are text files, GitHub can display their differences nicely, for example: CMR-STAC API qmd.\nHere is a workflow to leverage .qmds in our review process:\n\nClone/pull the repo, create a branch following our GitHub Workflow above\n\n## git clone and branch\ngit clone https://github.com/NASA-Openscapes/2021-Cloud-Hackathon/\ngit checkout -b my-new-branch\n\nIn the terminal, use quarto convert to convert a notebook from .ipynb to .qmd. You’ll have to either cd into the folder or you can convert by specifying the filepath. Typing quarto convert help will describe more options. The following will create tutorials/Data_Discovery__CMR-STAC_API.qmd.\n\n## quarto convert\nquarto convert tutorials/Data_Discovery__CMR-STAC_API.ipynb\n\nCommit this file now so that the whole file is tracked and any changes you make will be viewed as diffs.\nNow, make edits/review the .qmd in your editor of choice, which will likely also have spell-check and markdown support. At the moment, this is best with making edits to the .qmd locally while ALSO running the .ipynb notebook in 2i2c. This will iterate as we get quarto set up in 2i2c (and as we work more with .qmds).\nWhen you’re done with your review, you’ll want to copy all your edits from the .qmd back to the .ipynb. This way, the notebook author can both easily see the diffs made to the .qmd, and run the .ipynb as they normally would in 2i2c. The following code creates tutorials/Data_Discovery__CMR-STAC_API.ipynb, and over-write the original .ipynb with your changes.\n\nquarto convert tutorials/Data_Discovery__CMR-STAC_API.qmd \n\nQuarto will consider the .qmd book chapter and try to render it. To avoid this, by hand you can prefix the filename with an underscore: before you push: _Data_Discovery__CMR-STAC_API.qmd (do this by hand for now, we’ll iterate further)\nFinally, commit and push your review!"
  },
  {
    "objectID": "contributing/workflow.html#code-review",
    "href": "contributing/workflow.html#code-review",
    "title": "Workflow",
    "section": "Code Review",
    "text": "Upcoming."
  },
  {
    "objectID": "contributing/workflow.html#troubleshooting",
    "href": "contributing/workflow.html#troubleshooting",
    "title": "Workflow",
    "section": "Troubleshooting",
    "text": "Error: AddrInUse\nERROR: AddrInUse: Address already in use (os error 48)\nThis error is because you had more than one instance of quarto serve going in your session. So close other command line instances that are running and try again. (If you use the R package and do quarto_serve() it will automatically make sure you only ever have 1 instance.)\n\n\nLeave/exit a virtual environment\nIn your Command Line Shell, if you want to leave your virtual environment, the command is:\ndeactivate\nThe way you tell that you are in a virtual environment: it’s named in parentheses at the beginning of your prompt:\n(.venv) (base) JLos-Macbook-Pro:earthdata-cloud-cookbook lowndes$ deactivate\n(base) JLos-Macbook-Pro:earthdata-cloud-cookbook lowndes$"
  },
  {
    "objectID": "contributing/setup.html#overview",
    "href": "contributing/setup.html#overview",
    "title": "Initial Setup",
    "section": "Overview",
    "text": "This is the setup required to contribute to our Cookbook. Instructions are for the command line; see quarto.org for equivalents in R and for the most up-to-date and more detailed information."
  },
  {
    "objectID": "contributing/setup.html#install-quarto",
    "href": "contributing/setup.html#install-quarto",
    "title": "Initial Setup",
    "section": "Install Quarto",
    "text": "Option 1: Install Quarto locally on your machine\nFirst, in your internet browser, go to the very latest version of the Quarto command line interface (CLI):\nhttps://github.com/quarto-dev/quarto-cli/releases/latest\nDownload Quarto by clicking on the file appropriate for your operating system:\n\nLinux: amd64.deb\nMac: macos.pkg\nWindows: win.msi\n\nOnce download is complete, follow the installation prompts on your computer like you do for other software.\nNote for Mac users: If you do not have administrative privileges, please select “Install for me only” during the Destination Selection installation step (you will first click on “Change Install Location” at the Installation Type step).\n\n\nCheck install\nFinally, check to make sure Quarto installed properly. Open a command line terminal and type:\nquarto check install\n\n\n\n\n\n\nAdditional checks\n\n\n\n\n\nYou can also run:\n\nquarto check knitr to locate R, verify we have the rmarkdown package, and do a basic render\nquarto check jupyter to locate Python, verify we have Jupyter, and do a basic render\nquarto check to run all of these checks together\n\n\n\n\nFrom quarto.org: If you need to install a version more recent than the latest release, see the documentation on installing the Development Version.\n\n\nOption 2: Install Quarto in a docker container\nIf installing quarto locally is not an option, you can install it inside a container using the following Dockerfile:\n\n##############################\n# This Dockerfile installs quarto and then runs quarto serve against the\n# internal /home/quarto/to_serve.\n#\n# BUILD\n# -----\n# To build this container, run\n#\n#     docker build -t quarto_serve .\n#\n# Add the --no-cache option to force docker to build fresh and get the most\n# recent version of quarto.\n#\n#\n# RUN\n# ---\n# 1. Find the directory you want quarto to serve. Let's call this /PATH/TO/earthdata-cloud-cookbook.\n# 2. Run docker:\n#\n#     docker run --rm -it -p 4848:4848 -v /PATH/TO/earthdata-cloud-cookbook:/home/quarto/to_serve quarto_serve\n#\n# 3. Open your browser and go to http://127.0.0.1:4848/\n#\n##############################\n\nFROM ubuntu:hirsute\n\n######\n# Install some command line tools we'll need\n######\nRUN apt-get update\nRUN apt-get -y install wget\nRUN apt-get -y install gdebi-core\nRUN apt-get -y install git\n\n\n######\n# Install quarto (https://quarto.org/)\n######\n\n# This is a quick and dirty way of getting the newest version number from\n# https://github.com/quarto-dev/quarto-cli/releases/latest. What's happening is\n# we're pulling the version number out of the redirect URL. This will end up\n# with QVER set to something like 0.2.11.\nRUN QVER=`wget --max-redirect 0 https://github.com/quarto-dev/quarto-cli/releases/latest 2>&1 | grep \"Location\" | sed 's/L.*tag\\/v//' | sed 's/ .*//'` \\\n    && wget -O quarto.deb \"https://github.com/quarto-dev/quarto-cli/releases/download/v$QVER/quarto-$QVER-amd64.deb\"\nRUN gdebi -n quarto.deb\n\n# Run this to make sure quarto installed correctly\nRUN quarto check install\n\n\n######\n# Create a non-root user called quarto\n######\nRUN useradd -ms /bin/bash quarto\nUSER quarto\nRUN mkdir /home/quarto/to_serve\nWORKDIR /home/quarto/to_serve\n\n\n######\n# Start quarto serve\n######\n\nCMD quarto serve --no-browse --host 0.0.0.0 --port 4848"
  },
  {
    "objectID": "contributing/setup.html#clone-cookbook-from-github",
    "href": "contributing/setup.html#clone-cookbook-from-github",
    "title": "Initial Setup",
    "section": "Clone Cookbook from GitHub",
    "text": "Now clone our Cookbook and make it your current directory.\ngit clone https://github.com/NASA-Openscapes/earthdata-cloud-cookbook\ncd earthdata-cloud-cookbook"
  },
  {
    "objectID": "contributing/setup.html#build-cookbook",
    "href": "contributing/setup.html#build-cookbook",
    "title": "Initial Setup",
    "section": "Build Cookbook!",
    "text": "You should now be able to serve our Cookbook from the earthdata-cloud-cookbook directory.\nquarto serve\nThis will open the Cookbook as a new tab in your browser. Now you’re all set to contribute to the Cookbook! Read about how in the next chapter."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Earthdata Cloud Cookbook",
    "section": "Welcome",
    "text": "Welcome to the NASA Openscapes EarthData Cloud Cookbook!\nThis Cookbook is under active development but is an open resource as we iterate. Its purpose is to learning-oriented to support scientific researchers using NASA Earthdata from Distributed Active Archive Centers (DAACs) as they migrate their workflows to the cloud."
  },
  {
    "objectID": "index.html#the-new-cloud-paradigm",
    "href": "index.html#the-new-cloud-paradigm",
    "title": "Earthdata Cloud Cookbook",
    "section": "The new cloud paradigm",
    "text": "NASA Distributed Active Archive Centers (DAACs) are in the process of moving their data holdings to the cloud. In the new paradigm, data storage (orange in the illustration) will migrate to the cloud (green) and DAAC-provided tools and services built on top of the data are co-located in the Earthdata Cloud.\n\n\n\nIllustration by Catalina Oaida, PO.DAAC\n\n\nAs this data migration occurs, DAACs will have more information about how users can access data. For example, the Cloud Data page at PO.DAAC offers access to resources to help guide data users in discovering, accessing, and utilizing cloud data. During this transition, some data will continue to be available from the traditional on premise archive, while some data will also be available from and within the Earthdata Cloud.\nTo learn more about NASA’s EarthData Cloud Migration:\n\nHow we work - Katie Baynes, Keynote at FedGeoDay2020 (video). Nice overview of EOSDIS work and emphasis on open science\nCloud Data Egress: How EOSDIS Supports User Needs - Emily Cassidy (blog). Describes supporting users’ needs during cloud migration by providing analysis-ready data"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Earthdata Cloud Cookbook",
    "section": "About",
    "text": "This Earthdata Cloud Cookbook is being developed as a cross-DAAC collaboration by the NASA-Openscapes team. Learn more at nasa-openscapes.github.io."
  }
]